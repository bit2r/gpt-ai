{
  "hash": "2f3b380d69ac6655fd45d88d4adcc952",
  "result": {
    "engine": "knitr",
    "markdown": "# 허깅페이스\n\n[허깅 페이스(Hugging Face)](https://huggingface.co/)는 자연어 처리(NLP) 및 기계 학습 분야에 특화된 오픈소스 플랫폼이다. 이 플랫폼은 사전 학습된 모델, 데이터셋, 그리고 다양한 NLP 도구를 제공한다. [캐글(Kaggle)](https://www.kaggle.com/)이 주로 데이터 과학 경진대회와 다양한 도메인의 데이터셋에 중점을 두는 반면, Hugging Face는 NLP 기술의 개발과 공유에 초점을 맞춘다. Hugging Face의 'Model Hub'는 연구자와 개발자들이 자신의 모델을 쉽게 공유하고 다른 사람의 모델을 사용할 수 있게 해주며, 'Transformers' 라이브러리를 통해 최신 NLP 모델을 쉽게 구현하고 사용할 수 있다. \n\n\n\n\n```{mermaid}\ngraph LR\n    A[허깅페이스]\n\n    subgraph HF[모형과 데이터셋]\n        B1[모형]\n        B2[데이터셋]\n        B3[파이프라인]\n    end\n\n    subgraph text[텍스트]\n        C1[텍스트 <br> 감성 분류]\n        C2[요약]\n        C3[질의 응답]\n    end\n\n    subgraph image_audio[이미지와 오디오]\n        D1[이미지<br>오디오 분류]\n        D2[자동 음성 인식<br>STT]\n    end\n\n    subgraph advanced[고급 주제]\n        E1[파인튜닝]\n        E2[텍스트 생성<br>GPT]\n        E3[임베딩<br>시맨틱 검색]\n    end\n\n    A --> HF\n    A --> text\n    A --> image_audio\n    A --> advanced\n```\n\n\n\n\n## 설치\n\n허깅페이스에서 모형(transformers)과 데이터셋(datasets)를 설치하면 다양한 모형과 데이터를 다운로드 받을 수 있다. 목적이 다양한 기계학습 모형을 개발하는 것이기 때문에 텍스트는 `torch`, 이미지 영상은 `torchvision` 오디오는 `torchaudio` 플레임워크를 설치한다.\n`huggingface_hub`을 설치하면 CLI 방식으로 프로그래밍 방식으로 모형을 검색하고 설치에 도움을 받을 수 있다.\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# 허깅 페이스\npip install transformers datasets\n# ML 플레임워크\npip install torch torchvision torchaudio\n## HF 허브\npip install huggingface_hub\n```\n:::\n\n\n\n\n### 모형\n\n`huggingface_hub`으로 작업(task)에 맞는 모형을 `HfAPi` `list_models()` 메쏘드를 통해 찾아낼 수 있다.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom huggingface_hub import HfApi\nfrom pprint import pprint\n\napi = HfApi()\n\nmodels = api.list_models(\n    task=\"table-question-answering\", \n    sort=\"downloads\", \n    direction=-1, \n    limit=1\n)\n\ndef format_model_info(model):\n    return {\n        \"id\": model.id,\n        \"downloads\": model.downloads,\n        \"likes\": model.likes,\n        \"task\": model.pipeline_tag,\n        \"last_modified\": (\n            model.last_modified.strftime(\"%Y-%m-%d %H:%M:%S\")\n            if model.last_modified\n            else \"N/A\"\n        ),\n    }\n\nformatted_models = [format_model_info(model) for model in models]\n\nprint(\"Table Question Answering Models:\")\npprint(formatted_models, width=100, sort_dicts=False)\n```\n:::\n\n\n\n\n\n가장 많은 조회수와 다운로드를 자랑하는 텍스트 분류 작업(text-classificatin)에서 \n[distilbert/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) 모형을 찾았으면 로컬 컴퓨터에 저장하여 다음 후속작업을 이어나갈 수 있다. \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Import AutoModel\nfrom transformers import AutoModel\n\nmodelId = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# Download model using the modelId\nmodel = AutoModel.from_pretrained(modelId)\n# Save the model to a local directory\nmodel.save_pretrained(save_directory=f\"d:/llms/{modelId}\")\n```\n:::\n\n\n\n\n`tokenizer_config.json`, `config.json`, `model.safetensors` 파일이 모두 다운로드 되었는지 확인한다.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\nimport warnings\n\n# 경고 필터 설정\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint\")\n\n# 모델 ID와 저장 경로\nmodel_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel_path = f\"d:/llms/{model_id}\"\n\n# 모델 로드\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"<string>\", line 3, in <module>\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 3716, in from_pretrained\n    state_dict = load_state_dict(resolved_archive_file)\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 556, in load_state_dict\n    return safe_load_file(checkpoint_file)\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\torch.py\", line 313, in load_file\n    result[k] = f.get_tensor(k)\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\storage.py\", line 321, in __getitem__\n    return super().__getitem__(*args, **kwargs)\nC:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\storage.py:321: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n  return super().__getitem__(*args, **kwargs)\n```\n\n\n:::\n\n```{.python .cell-code}\n# 토크나이저 로드 (온라인에서 직접 다운로드)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# 분석할 텍스트\ntext = \"오늘 정말 기분이 좋다.\"\n\n# 텍스트를 토큰화하고 모델 입력으로 준비\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# 모델 추론\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# 결과 해석\nprobabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\npositive_probability = probabilities[0][1].item()\n\nprint(f\"입력 텍스트: '{text}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n입력 텍스트: '오늘 정말 기분이 좋다.'\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"긍정적인 감성일 확률: {positive_probability:.2%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n긍정적인 감성일 확률: 53.07%\n```\n\n\n:::\n\n```{.python .cell-code}\nif positive_probability > 0.5:\n    print(\"결과: 긍정적인 감성입니다.\")\nelse:\n    print(\"결과: 부정적인 감성입니다.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n결과: 긍정적인 감성입니다.\n```\n\n\n:::\n:::\n\n\n\n\n## 데이터셋\n\n허깅페이스 데이터셋은 `3D`, `Audio`, `Geospatial`, `Image`, `Tabular`, `Text`, `Time-series`, `Video` 범주로 나누어져 제공되고 있다.\n정형데이터(Tabular) 중에서 [lavita/medical-qa-shared-task-v1-toy](https://huggingface.co/datasets/lavita/medical-qa-shared-task-v1-toy) 데이터셋이 가장 많은 다운로드를 기록하고 있다.[^hf-dataset]\n정형데이터를 판다스 데이터프레임으로 변환한 후 후속 작업을 진행한다.\n\n[^hf-dataset]: [Working with Hugging Face Datasets](https://medium.com/towards-data-science/working-with-hugging-face-datasets-bba14dd8da68)\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"lavita/medical-qa-shared-task-v1-toy\", split=\"train\")\n\ndataset.to_pandas().head()[[\"id\", \"ending0\", \"GCC\"]]\n```\n:::\n\n\n\n\n## 파이프라인\n\n**Auto 클래스**는 모델, 토크나이저, 설정 등을 유연하게 사용할 수 있는 일반 클래스로, ML 작업에 대한 더 많은 제어와 유연성을 제공한다. \n**AutoModels**는 특정 작업에 맞는 사전 훈련된 모델을 직접 다운로드하고 초기화하는 기능을 제공하여 복잡한 모델 아키텍처를 직접 구현할 필요 없이 바로 사용할 수 있다.\n**AutoTokenizers**는 텍스트 입력 데이터를 모델이 이해할 수 있는 형식을 담당한다. 모델과 쌍을 이루는 토크나이저를 사용하는 것이 권장되며, 이를 통해 텍스트 전처리 과정을 자동화할 수 있다.\n\n파이프라인 모듈은 특정 작업에 필요한 모든 단계를 포함하는 고수준 인터페이스로 모델 로딩, 전처리, 추론 등의 과정을 자동화하여 빠르게 작업을 수행하고 시작하기에 적합하다.\n\n\n\n\n\n```{mermaid}\ngraph LR\n    subgraph Auto클래스\n        B[Auto 클래스]\n        B --> C[AutoModels]\n        B --> D[AutoTokenizers]\n        B --> E[Auto 컴포넌트<br>* AutoConfig<br>* AutoProcessor<br>* AutoFeatureExtractor]\n        \n    end\n\n    subgraph 파이프라인모듈\n        F[파이프라인 모듈]\n        F --> F2[모델 로딩]\n        F --> F3[전처리]\n        F --> F4[추론]\n    end\n\n    Auto클래스 --> 파이프라인모듈\n```\n\n\n\n\n\n::: panel-tabset\n\n### 감성분석\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom transformers import pipeline\n\n# 다국어 BERT 모델을 사용한 감성 분석 파이프라인 생성\nclassifier = pipeline(\n    task=\"sentiment-analysis\",\n    model=\"bert-base-multilingual-cased\",\n    return_all_scores=True,\n)\n\n# 분석할 텍스트 리스트\ntexts = [\n    \"이 영화는 정말 재미있었어요. 추천합니다!\",\n    \"서비스가 너무 별로였어요. 다시는 안 갈 것 같아요.\",\n    \"그저 그랬어요. 특별히 좋지도 나쁘지도 않았습니다.\",\n    \"This movie was really great. I recommend it!\",\n    \"The service was terrible. I won't go there again.\",\n]\n\n# 각 텍스트에 대해 감성 분석 수행\nfor text in texts:\n    result = classifier(text)[0]\n\n    # 긍정, 부정 점수 추출\n    positive_score = next(score for score in result if score[\"label\"] == \"LABEL_1\")[\n        \"score\"\n    ]\n    negative_score = next(score for score in result if score[\"label\"] == \"LABEL_0\")[\n        \"score\"\n    ]\n\n    print(f\"\\n텍스트: {text}\")\n    print(f\"긍정 점수: {positive_score:.2%}\")\n    print(f\"부정 점수: {negative_score:.2%}\")\n\n    # 감성 판단\n    if positive_score > negative_score:\n        sentiment = \"긍정\"\n        score = positive_score\n    else:\n        sentiment = \"부정\"\n        score = negative_score\n\n    print(f\"판단된 감성: {sentiment}\")\n\n    # 확신도에 따른 해석\n    if score > 0.75:\n        interpretation = \"매우 강한\"\n    elif score > 0.6:\n        interpretation = \"강한\"\n    elif score > 0.4:\n        interpretation = \"약한\"\n    else:\n        interpretation = \"중립적인\"\n\n    print(f\"해석: {interpretation} {sentiment}적 의견입니다.\")\n```\n:::\n\n\n\n\n### 텍스트 분류\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom transformers import pipeline\n\n# 파이프라인 생성\nclassifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n# 예제 1: 영화 리뷰 분류\ntext1 = \"이 영화는 흥미진진한 플롯과 멋진 특수효과로 가득했습니다. 배우들의 연기도 훌륭했어요.\"\ncandidate_labels1 = [\"긍정적\", \"부정적\", \"중립적\"]\noutput1 = classifier(text1, candidate_labels1)\nprint(\"영화 리뷰 분류:\")\nprint(f\"가장 높은 확률의 레이블: {output1['labels'][0]}\")\nprint(f\"확률: {output1['scores'][0]:.4f}\")\n\n# 예제 2: 뉴스 기사 주제 분류\ntext2 = \"최근 연구에 따르면 규칙적인 운동이 스트레스 감소와 수면의 질 향상에 도움이 된다고 합니다.\"\ncandidate_labels2 = [\"건강\", \"스포츠\", \"기술\", \"정치\"]\noutput2 = classifier(text2, candidate_labels2)\nprint(\"\\n뉴스 기사 주제 분류:\")\nprint(f\"가장 높은 확률의 레이블: {output2['labels'][0]}\")\nprint(f\"확률: {output2['scores'][0]:.4f}\")\n\n# 예제 3: 상품 리뷰 감성 분석\ntext3 = \"이 제품은 가격은 비싸지만 품질이 정말 뛰어나고 오래 사용할 수 있을 것 같아요.\"\ncandidate_labels3 = [\"만족\", \"불만족\", \"중립\"]\noutput3 = classifier(text3, candidate_labels3)\nprint(\"\\n상품 리뷰 감성 분석:\")\nprint(f\"가장 높은 확률의 레이블: {output3['labels'][0]}\")\nprint(f\"확률: {output3['scores'][0]:.4f}\")\n```\n:::\n\n\n\n\n### STT\n\nASR(Automatic Speech Recognition)은 음성을 인식하여 텍스트로 출력하는 기계학습 모형으로 \nOpenAI 위스퍼, [페이스북 Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)이 유명하다.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom transformers import pipeline\n\n# 한국어 음성 인식을 위한 모델 로드\ntranscriber = pipeline(\n    task=\"automatic-speech-recognition\", model=\"kresnik/wav2vec2-large-xlsr-korean\"\n)\n\n# 오디오 파일 경로\naudio_file = \"data/한국어_영어번역.mp3\"\n\n# 음성 인식 실행\nresult = transcriber(audio_file)\n\nprint(result[\"text\"])\n```\n:::\n\n\n\n\n```\n정교는 국미럽 얼는 여던 레번째 사미지를 맞아 나라 리 기생하고 신하신 애국 습예들개 단았는 감사와 경위를 표합니또립 유공자와 유가적 여러분에게도 존경과 감사에 말씀을 쓰립니다\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(embedr)\n\nembed_audio(\"data/한국어_영어번역.mp3\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<audio controls> <source src='data/한국어_영어번역.mp3' type='audio/mpeg'> Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp </audio>\n```\n\n:::\n:::\n\n\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}