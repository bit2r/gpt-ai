# 올라마

[올라마(ollama)](https://ollama.com/download) 웹사이트에서 운영체제에 맞춰 다운로드 받아 설치한다. 만약 별도 설정하지 않게 되면 `C:\Users\<사용자명>\.ollama` 디렉토리에 LLM 이 저장된다.

```{bash}
#| eval: false
setx OLLAMA_MODELS D:\llms
```

```{bash}
#| eval: false
ollama run gemma2
```

```{python}
#| eval: false
from langchain_community.llms import Ollama
llm = Ollama(model="gemma2")
llm.invoke("축구를 가장 잘하는 나라는 어디야?")
```

## 세가지 방법

LLaMA 모델을 사용하는 세 가지 주요 방법은 허깅페이스 Transformers, Ollama, llama-cpp-python이다. 허깅페이스 Transformers는 클라우드 호스팅 모델을 사용하여 쉽게 접근할 수 있다. Ollama는 로컬 서버를 통해 최적화된 모델을 실행한다. llama-cpp-python은 C++로 구현된 LLaMA의 Python 바인딩을 사용하여 로컬에서 효율적으로 모델을 실행한다.

| 방법 | 장점 | 단점 | 사용 사례 |
|:---------:|----------------|----------------|-------------------------|
| Hugging Face Transformers | \- 쉬운 사용<br>- 다양한 모델 지원<br>- 클라우드 리소스 활용 | \- 인터넷 연결 필요<br>- 대용량 데이터 전송 시 비용 발생 가능 | \- 빠른 프로토타이핑<br>- 다양한 모델 실험<br>- 클라우드 기반 애플리케이션 |
| Ollama | \- 로컬 실행으로 빠른 응답<br>- API를 통한 간편한 사용<br>- 커스텀 모델 지원 | \- 초기 설정 필요<br>- 로컬 리소스 사용 | \- 개인 프로젝트<br>- 로컬 애플리케이션 개발<br>- 데이터 프라이버시 중요 사례 |
| llama-cpp-python | \- 매우 효율적인 로컬 실행<br>- 낮은 메모리 사용량<br>- 커스텀 모델 지원 | \- 모델 파일 관리 필요<br>- 초기 설정 복잡할 수 있음 | \- 리소스 제한적인 환경<br>- 임베디드 시스템<br>- 고성능 요구 애플리케이션 |

```{mermaid}
graph TD
    A["사용자 입력<>프롬프트"] --> B{로컬 LLM <br> 방법 선택}
    
    B --> |1| C[허깅페이스 Transformers]
    C --> D[클라우드 호스팅 모델]
    D --> E[토크나이저]
    E --> F[모델 추론]
    F --> G[출력]

    B --> |2| H[Ollama]
    H --> I[로컬 Ollama 서버]
    I --> J[HTTP API 요청]
    J --> K[모델 추론]
    K --> L[출력]

    B --> |3| M[llama-cpp-python]
    M --> N[로컬 GGUF 모델 파일]
    N --> O[Llama 클래스 인스턴스]
    O --> P[모델 추론]
    P --> Q[출력]

    G --> R[결과]
    L --> R
    Q --> R
```

## 라마3

라마(Llama)는 메타(Meta)에서 2023년 2월에 처음 공개한 대규모 언어 모델이며, 라마2는 2023년 7월에 출시된 라마의 개선 버전으로, 오픈 소스로 공개되어 연구 및 상업적 용도로 사용할 수 있다. 라마3는 2024년 6월 출시되었으며 128,256 토큰을 보유한 새로운 토크나이저를 장착했으며, 8,192 컨텍스트 길이를 갖고 15조개(15T)토큰 학습 데이터를 사용하여 성능을 크게 높였고, Supervised fine-tuning (SFT) Rejection Sampling, Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO) 알고리즘을 적용하였다.

```{mermaid}
graph BT
    subgraph Llama_Model_and_CPP["Llama 모델 및 llama.cpp"]
        A[Llama 모델] -->|제공| B((모델 가중치))
        A -->|정의| C((모델 아키텍처))
        subgraph llama_cpp[llama.cpp]
            D[가중치 로딩]
            E[아키텍처 구현]
            F[추론 수행]
            G[최적화된 연산]
        end
        B --> D
        C --> E
    end

    subgraph Upper_Layers["AI 응용프로그램"]
        H[llama-cpp-python]
        I[Llama 클래스]
        J[사용자 코드]
    end

    llama_cpp --> H
    H --> I
    I --> J

```

**Llama 모델**은 GGUF(GPT-Generated Unified Format) 파일 형태로 제공되며, 모델 가중치(학습된 파라미터 값)와 모델 아키텍처(신경망의 구조와 레이어)를 포함하고 있다. **llama.cpp**는 C++로 작성된 Llama 모델의 추론 엔진으로 Llama 모델의 아키텍처를 C++ 코드로 구현되어 있으며 GGUF 파일에서 가중치를 읽어 메모리에 로드하고, 입력 텍스트에 대해 모델의 추론을 실행한다. 즉, llama.cpp는 Llama 모델을 "실행"하는 엔진으로 모델 파일(GGUF)은 "무엇을" 계산할지를 정의하고, llama.cpp는 "어떻게" 계산할지를 구현한다.

## 헬로월드

[CMake](https://cmake.org/download/)를 설치하고 `llama-cpp-python` 패키지를 설치한다.

```{bash}
#| eval: false
pip install llama-cpp-python --prefer-binary
```

허깅페이스 [QuantFactory/Meta-Llama-3-8B-GGUF](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/tree/main) 에서 `Meta-Llama-3-8B.Q8_0.gguf` 파일을 다운로드 받아 사용한다.

```{python}
from llama_cpp import Llama

path_to_model = "data/Meta-Llama-3-8B.Q8_0.gguf"
llm = Llama(model_path=path_to_model)
output = llm(
    "Why is the sky blue?",
)
print(output["choices"][0]["text"])
```

## 라마3.1


### 허깅페이스

허깅페이스에서 라마3 모형을 다운로드 받기 위해서 먼저 `huggingface-cli`를 설치한다.
다음으로 쉘에서 `huggingface-cli login`을 통해 토큰 <https://huggingface.co/settings/tokens>을 생성한 후 로그인하여 인증한다.
[meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)에서 라이선스 동의 절차를 밟는다.
라이선스 승인이 완료되면 `huggingface-cli download` 명령어로 허깅페이스 클라우드에서 라마3 모형을 다운로드 받아 로컬 디렉토리에 설치하게 된다.

```{bash}
#| eval: false
pip install -U "huggingface_hub[cli]"

huggingface-cli login

huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include "original/*" --local-dir D:\llms\Meta-Llama-3.1-8B-Instruct
```

`pip install --upgrade transformers` 를 최신 버전으로 업그레이드한다.

```{bash}
#| eval: false
pip install --upgrade transformers
pip install accelerate  # Using `low_cpu_mem_usage=True` or a `device_map`         
```

```{python}
#| eval: false
from accelerate import Accelerator
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer and model
model_path = "D:/llms/Meta-Llama-3.1-8B-Instruct/original"
# model_name = "meta-llama/Llama-2-7b-chat-hf"  # 또는 다른 LLaMA 모델 이름
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# 간단한 프롬프트 생성
prompt = "안녕하세요, LLaMA입니다. 오늘의 날씨는"
inputs = tokenizer(prompt, return_tensors="pt")

# 모델 추론
outputs = model.generate(**inputs, max_length=50)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)
```

### `llama-cpp-python`

[라마 3.1](https://llama.meta.com/)을 405B, 70B, 8B 중 하나를 용도에 맞춰 다운로드한다. 

```{python}
#| eval: false
# llama-cpp-python 
from llama_cpp import Llama

def load_llama_cpp():
    # 모델 파일 경로 (실제 경로로 변경 필요)
    model_path = "path/to/your/gguf/model.gguf"

    # LLaMA 모델 로드
    llm = Llama(model_path=model_path)

    # 프롬프트 생성 및 추론
    prompt = "안녕하세요, LLaMA입니다. 오늘의 날씨는"
    output = llm(prompt, max_tokens=50)

    print(output["choices"][0]["text"])


if __name__ == "__main__":
    print("\nUsing llama-cpp-python:")
    load_llama_cpp()
```

### 올라마

`ollama run llama3.1:8b` 명령어로 라마3 모형을 설치한 후 올라마를 사용해서 로컬에서 오픈소스 버전 LLM을 사용할 수 있다.

```{bash}
#| eval: false
ollama run llama3.1:8b
```




```{python}
# | eval: false
import requests
import json


def generate_text(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "llama3.1:8b", "prompt": prompt},
        stream=True,
    )

    text_response = ""
    for line in response.iter_lines():
        if line:
            json_response = json.loads(line)
            if "response" in json_response:
                text_response += json_response["response"]
            if json_response.get("done", False):
                break

    return text_response.strip()


# Usage example
prompt = "한글로 답을 해. 축구를 가장 잘하는 나라는 어디야?"
response = generate_text(prompt)
print("Prompt:", prompt)
print("Response:", response)
```

```{python}
# | eval: false
from langchain_community.llms import Ollama

llm = Ollama(model="llama3.1:8b")
llm.invoke("한글로 답을 해. 축구를 가장 잘하는 나라는 어디야?")
```

