---
editor: 
  markdown: 
    wrap: 72
---

# R 패키지

## `chattr`


![](images/chattr-diagram.png){fig-align="center"}

```{r}
library(tidyverse)
```

```{r}
#| eval: false
> chattr::chattr_app()

── chattr - Available models 
Select the number of the model you would like to use:

1: OpenAI - GitHub Copilot Chat -  (copilot) 

2: OpenAI - Chat Completions - gpt-3.5-turbo (gpt35) 

3: OpenAI - Chat Completions - gpt-4 (gpt4) 

4: OpenAI - Chat Completions - gpt-4o (gpt4o) 


선택: 
```

![](images/chattr_gpt35.png){fig-align="center" width="261"}

![](images/chattr_prompt.png){fig-align="center" width="302"}

::: callout-note
### 시스템 프롬프트

> Use the 'Tidy Modeling with R' (https://www.tmwr.org/) book as main
> reference Use the 'R for Data Science' (https://r4ds.had.co.nz/) book
> as main reference Use tidyverse packages: readr, ggplot2, dplyr, tidyr
> For models, use tidymodels packages: recipes, parsnip, yardstick,
> workflows, broom Avoid explanations unless requested by user,
> expecting code only For code output, use RMarkdown code chunks Avoid
> all code chunk options
:::

## 키보드 단축키



![](images/chattr_shortcut.png)

```{r}
#| eval: false
library(chattr)
chattr_use(provider = "LlamaGPT", path_url = "~/LlamaGPTJ-chat/build/bin/chat", model = "D:/llms/ggml-gpt4all-j.bin")

```


```bash
> chattr_defaults(path = "D:\\llms\\ggml-gpt4all-j.bin", model = "LlamaGPT")

── chattr ────────────────────────────────────────────────────────────────────────

── Defaults for: Default ──

── Prompt: 
• Use the R language, the tidyverse, and tidymodels

── Model 
• Provider: LlamaGPT
• Path/URL: D:\llms\ggml-gpt4all-j.bin
• Model: LlamaGPT
• Label: GPT4ALL 1.3 (LlamaGPT)

── Model Arguments: 
• threads: 4
• temp: 0.01
• n_predict: 1000

── Context: 
Max Data Files: 0
Max Data Frames: 0
✖ Chat History
✖ Document contents
```

## `ollamar`

[`ollamar`](https://github.com/hauselin/ollama-r) 패키지는 오픈소스 LLM을 R에서 직접 사용할 수 있도록 하는 패키지다. 올라마를 설치하고 LLM 모형을 다운로드 받은 후에 `ollamar` 패키지 함수를 사용해서 활용할 수 있다.

```{r}
library(ollamar)
test_connection() # 연결확인
list_models()     # LLM 모형 
```

라마 3.1 모형을 기반모형으로 헬로월드 프롬프트를 통해 시범운전해보자.
``` {r}
generate("llama3.1:8b", "하늘은 왜 파란색이야?", output = "text")
```

뉴스기사를 요약하는 코드를 다음과 같이 작성할 수 있다.
한국 R 사용자회에서 뉴스토마토 기사를 R 데이터 패키지로 제작한 [bitTomato](https://github.com/bit2r/bitTomato) 패키지에서 일부 뉴스를 추출해서 라마3.1 LLM에 요약작업을 수행한다.

```{r}
library(tidyverse)
library(ollamar)
library(bitTomato) # devtools::install_github("bit2r/bitTomato")

summarize_news <- function(news) {
  generate(
    "llama3.1:8b",
    str_glue(
      "뉴스 기사내용을 글머리표 3개로 요약한다.
      제목은 개조식으로 작성한다.
      {news}"
    ),
    output = "text"
  )
}

news_tbl <- bitTomato::cherry_tomato |>
  slice_sample(n = 3) |>
  mutate(요약 = map(contents, summarize_news))

news_tbl |>
  mutate(요약글 = map(요약, select, "response") |> unlist()) |> 
  select(원기사=contents, 요약글) |> 
  pander::pander()

```