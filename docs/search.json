[
  {
    "objectID": "openai_apps.html",
    "href": "openai_apps.html",
    "title": "",
    "section": "",
    "text": "사례OpenAI 응용프로그램 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#openai-패키지",
    "href": "openai_apps.html#openai-패키지",
    "title": "",
    "section": "\nopenai 패키지",
    "text": "openai 패키지\nGitHub 저장소 OpenAI Python Library의 openai 파이썬 패키지를 설치한 후 버전을 확인한다.\n\n코드! pip show openai\n\nName: openai\nVersion: 0.27.2\nSummary: Python client library for the OpenAI API\nHome-page: https://github.com/openai/openai-python\nAuthor: OpenAI\nAuthor-email: support@openai.com\nLicense: \nLocation: c:\\miniconda\\envs\\r-reticulate\\lib\\site-packages\nRequires: aiohttp, requests, tqdm\nRequired-by:",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#gpt-4-모델",
    "href": "openai_apps.html#gpt-4-모델",
    "title": "",
    "section": "\ngpt-4 모델",
    "text": "gpt-4 모델\ngpt-3.5-turbo 모형은 속도가 빠르고 API 호출 당 가격이 저렴하지만, 성능이 gpt-4 보다 낮은 것으로 알려져 있다. max_tokens 크기를 달리하여 API 반환길이를 조절할 수 있고, temperature 값을 달리하여 사실에 보다 가까운 값을 얻고자 할 경우 0으로 그렇지 않고 다양한 창의적인 응답을 원할 경우 0 보다 큰 값을 지정한다.\n\n코드import openai\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\nresponse = ChatCompletion.create(\n  model       = \"gpt-4\",\n  messages    = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"파이썬하고 R 하고 싸우면 누가 이겨?\"}\n  ],\n  max_tokens   = 100,\n  temperature  = 0\n)\n\nprint(response[\"choices\"][0]['message']['content'])\n\n\n파이썬과 R은 프로그래밍 언어로, 싸우는 것이 아니라 각각의 특성과 용도에 따라 사용됩니다. \n\n파이썬은 범용 프로그래밍 언어로, 웹 개발, 데이터 분석, 인공 지능 등 다양한 분야에서 사용됩니다.\n\n\n\n\n\n\nCompletion과 ChatCompletion\n\n\n\nopenai.Completion.create() 함수를 사용하여 text-davinci-003, text-davinci-002, ada, babbage, curie, davinci 등 텍스트 완성 기능을 사용할 수 있다. 하지만, openai.ChatCompletion.create() 함수가 대세로 떠올랐으며 gpt-4를 비롯한 고급 모형과 AI 고급기능을 사용할 경우 비용이 높아지는 단점도 있다. 가성비가 뛰어난 LLM 모형을 선택하여 해당 기능을 구현하고 문제를 풀어가는 지혜가 필요하다.",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#openai-모형",
    "href": "openai_apps.html#openai-모형",
    "title": "",
    "section": "\nopenai 모형",
    "text": "openai 모형\nopenai API를 사용할 때 다양한 모형에 대한 확인이 필요하다. 이를 위해서 API 호출을 통해 이용가능한 모형을 확인한다. 특히, 수시로 OpenAI에서 제공되는 모형이 생겨나고 없어지기 때문에 API를 사용한 AI 모형을 개발할 때 특히 주의한다.\n\n코드import openai\ndef list_all_models():\n    model_list = openai.Model.list()[\"data\"]\n    model_ids = [x[\"id\"] for x in model_list]\n    model_ids.sort()\n    pprint.pprint(model_ids)\n\nif __name__ == \"__main__\":\n    list_all_models()\n\n\n['ada',\n 'ada-code-search-code',\n 'ada-code-search-text',\n 'ada-search-document',\n 'ada-search-query',\n 'ada-similarity',\n 'babbage',\n 'babbage-code-search-code',\n 'babbage-code-search-text',\n 'babbage-search-document',\n 'babbage-search-query',\n 'babbage-similarity',\n 'code-davinci-edit-001',\n 'code-search-ada-code-001',\n 'code-search-ada-text-001',\n 'code-search-babbage-code-001',\n 'code-search-babbage-text-001',\n 'curie',\n 'curie-instruct-beta',\n 'curie-search-document',\n 'curie-search-query',\n 'curie-similarity',\n 'davinci',\n 'davinci-instruct-beta',\n 'davinci-search-document',\n 'davinci-search-query',\n 'davinci-similarity',\n 'gpt-3.5-turbo',\n 'gpt-3.5-turbo-0301',\n 'gpt-3.5-turbo-0613',\n 'gpt-3.5-turbo-16k',\n 'gpt-3.5-turbo-16k-0613',\n 'gpt-4',\n 'gpt-4-0314',\n 'gpt-4-0613',\n 'text-ada-001',\n 'text-babbage-001',\n 'text-curie-001',\n 'text-davinci-001',\n 'text-davinci-002',\n 'text-davinci-003',\n 'text-davinci-edit-001',\n 'text-embedding-ada-002',\n 'text-search-ada-doc-001',\n 'text-search-ada-query-001',\n 'text-search-babbage-doc-001',\n 'text-search-babbage-query-001',\n 'text-search-curie-doc-001',\n 'text-search-curie-query-001',\n 'text-search-davinci-doc-001',\n 'text-search-davinci-query-001',\n 'text-similarity-ada-001',\n 'text-similarity-babbage-001',\n 'text-similarity-curie-001',\n 'text-similarity-davinci-001',\n 'whisper-1']",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#중재moderation",
    "href": "openai_apps.html#중재moderation",
    "title": "",
    "section": "중재(Moderation)",
    "text": "중재(Moderation)\nAI LLM 모형에 입력값으로 전달되는 다양한 텍스트에 부적절한 내용이 담겨있을 수 있다. AI 제품을 제작할 때 이를 사전에 감지하여 적절한 조치를 취하는 것이 필요하다.\n\n코드import openai\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\nresponse = openai.Moderation.create(\n  model = \"text-moderation-latest\",\n  input = \"정신병자 천국이구나\"\n)\n\nprint(response)\n\n\n#&gt; {\n#&gt;   \"id\": \"modr-7dwWS1RCU6qbmrmJNQOZrxElV6Qsa\",\n#&gt;   \"model\": \"text-moderation-005\",\n#&gt;   \"results\": [\n#&gt;     {\n#&gt;       \"flagged\": false,\n#&gt;       \"categories\": {\n#&gt;         \"sexual\": false,\n#&gt;         \"hate\": false,\n#&gt;         \"harassment\": false,\n#&gt;         \"self-harm\": false,\n#&gt;         \"sexual/minors\": false,\n#&gt;         \"hate/threatening\": false,\n#&gt;         \"violence/graphic\": false,\n#&gt;         \"self-harm/intent\": false,\n#&gt;         \"self-harm/instructions\": false,\n#&gt;         \"harassment/threatening\": false,\n#&gt;         \"violence\": false\n#&gt;       },\n#&gt;       \"category_scores\": {\n#&gt;         \"sexual\": 4.693797e-05,\n#&gt;         \"hate\": 0.000676339,\n#&gt;         \"harassment\": 0.0005268965,\n#&gt;         \"self-harm\": 9.536401e-06,\n#&gt;         \"sexual/minors\": 8.569186e-07,\n#&gt;         \"hate/threatening\": 0.0012578131,\n#&gt;         \"violence/graphic\": 9.558387e-07,\n#&gt;         \"self-harm/intent\": 2.2572526e-06,\n#&gt;         \"self-harm/instructions\": 2.0226207e-06,\n#&gt;         \"harassment/threatening\": 0.00021643011,\n#&gt;         \"violence\": 1.2982889e-05\n#&gt;       }\n#&gt;     }\n#&gt;   ]\n#&gt; }\n욕설 감지 데이터셋에서 욕설로 라벨링된 텍스트를 하나 가져와서 살펴본다. 결과는 욕설 뿐만 아니라 다양한 측면에서 해당 프롬프트 텍스트에 대한 평가결과를 제시하고 있다.\n\n코드library(reticulate)\nlibrary(tidyverse)\n\nmoderation_part_1 &lt;- py$response$results[[1]]$categories |&gt; \n  enframe(name = \"구분\") |&gt; \n  mutate(판정 = map_lgl(value, 1)) |&gt; \n  select(구분, 판정)\n\n\nmoderation_part_2 &lt;- py$response$results[[1]]$category_scores |&gt; \n  enframe(name = \"구분\") |&gt; \n  mutate(점수 = map_dbl(value, 1)) |&gt; \n  select(구분, 점수)\n\nmoderation_tbl &lt;- left_join(moderation_part_1, moderation_part_2)\n\nmoderation_tbl |&gt; \n  arrange(desc(점수))\n\n\n#&gt; # A tibble: 11 × 3\n#&gt;    구분                   판정         점수\n#&gt;    &lt;chr&gt;                  &lt;lgl&gt;       &lt;dbl&gt;\n#&gt;  1 hate/threatening       FALSE 0.00126    \n#&gt;  2 hate                   FALSE 0.000676   \n#&gt;  3 harassment             FALSE 0.000527   \n#&gt;  4 harassment/threatening FALSE 0.000216   \n#&gt;  5 sexual                 FALSE 0.0000469  \n#&gt;  6 violence               FALSE 0.0000130  \n#&gt;  7 self-harm              FALSE 0.00000954 \n#&gt;  8 self-harm/intent       FALSE 0.00000226 \n#&gt;  9 self-harm/instructions FALSE 0.00000202 \n#&gt; 10 violence/graphic       FALSE 0.000000956\n#&gt; 11 sexual/minors          FALSE 0.000000857",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#음성",
    "href": "openai_apps.html#음성",
    "title": "",
    "section": "음성",
    "text": "음성\n행정안전부 대통령기록관에 연설기록 중 연설음성에서 제84주년 3.1절 기념식 노무현 대통령 음성원고를 대상으로 OpenAI Whisper API를 사용해서 텍스트를 추출해보자.\n연설문 낭독\n‘제84주년 3.1절’ 기념행사에서 노무현 대통령이 낭독한 연설문을 요약하면 다음과 같다.\n\n3.1절을 맞아 일제에 항거한 애국선열들께 감사와 경의 표함. 국민통합과 개혁으로 평화와 번영의 동북아시대를 열어가고 자랑스런 대한민국을 우리 후손에게 물려줄 것을 연설했다.\n\nAudacity 프로그램에서 불필요한 부분 삭제하고 해당 연설문만 추출하여 .mp3 파일로 준비한다.\n\n코드library(av)\nlibrary(embedr)\n\nembedr::embed_audio(\"data/제84주년_31절_기념사_노무현.mp3\")\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n.mp3 → 텍스트\nOpenAI Whisper API를 사용하여 음성을 TXT로 변환하는 작업을 수행한다. .mp3 파일을 준비하고,Audio.transcribe() 함수를 사용하여 .txt 텍스트 파일로 저장한다.\n\n코드import openai\nfrom dotenv import load_dotenv\nimport os\n\n# API KEY ----------------------\nload_dotenv()\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# STT --------------------------\n\nspeech_file = open(\"data/제84주년_31절_기념사_노무현.mp3\", \"rb\")\n\nresponse = openai.Audio.transcribe(\"whisper-1\", speech_file)\n\n\nwith open(\"data/stt_audio.txt\", \"w\") as file:\n    file.write(response[\"text\"])\n\n\n후처리\nSTT를 통해 나온 텍스트는 사람이 읽기에는 가독성이 무척 떨어지는 텍스트에 불과하다. 이를 읽을 수 있도록 후처리를 한다.\n\n\n음성원고 원문\nWhisper 텍스트\n챗 GPT 후가공\nWhisper 텍스트\n\n\n\n존경하는 국민 여러분,\n오늘 여든 네 번째 3, 1절을 맞아 나라를 위해 희생하고 헌신하신 애국선열들께 한없는 감사와 경의를 표합니다. 독립유공자와 유가족 여러분에게도 존경과 감사의 말씀을 드립니다.\n기미년 오늘, 우리는 일제의 총칼에 맞서 맨주먹으로 분연히 일어섰습니다. 대한독립 만세 소리가 전국 방방곡곡을 뒤덮었고, 우리는 자주독립 의지를 세계만방에 알렸습니다. 3, 1운동을 계기로 국내외의 독립투쟁은 더욱 힘차게 전개되었습니다. 상해에 대한민국 임시정부가 세워졌고, 우리는 마침내 빼앗긴 국권을 되찾았습니다.\n3, 1정신은 끊임없는 도전을 슬기롭게 극복해 온 우리 민족의 자랑입니다. 우리는 이러한 빛나는 정신을 계승하여 전쟁의 폐허를 딛고 세계 12위의 경제강국으로 발돋움했습니다. 4, 19 혁명과 광주민주화운동, 6월 민주항쟁을 거쳐 민주주의와 인권을 쟁취해 냈습니다. 오늘의 참여정부는 바로 그 위대한 역사의 연장선 위에 서 있습니다.\n참여정부의 출범으로 이제 아픔의 근, 현대사는 막을 내리게 되었습니다. 우리의 지난날은 선열들의 고귀한 희생에도 불구하고 좌절과 굴절을 겪어야 했습니다. 정의는 패배했고 기회주의가 득세했습니다.\n그러나 이제 비로소 역사적 전환점이 마련되었습니다. 국민이 진정 주인으로 대접받는 시대가 열린 것입니다.\n참여정부에서는 권력에 아부하는 사람들이 더 이상 설 땅이 없을 것입니다. 오로지 성실하게 일하고 정정당당하게 승부하는 사람들이 성공하는 시대가 열릴 것입니다. 그것이 바로 선열들의 희생에 보답하는 길이자 저와 참여정부에게 주어진 역사적 소명입니다.\n국민 여러분,\n지금 우리는 세계사의 새로운 흐름과 마주하고 있습니다. 동북아 시대의 도래가 바로 그것입니다. 동북아시아는 근대 이후 세계의 변방으로만 머물러 왔습니다. 그러나 이제 유럽연합, 북미지역과 함께 세계경제의 3대 축으로 부상하고 있습니다. 앞으로 20년 후에는 세계경제의 3분의 1을 차지하게 된다는 전망도 있습니다. 민족웅비의 크나큰 기회가 우리에게 다가오고 있는 것입니다.\n우리는 동북아시대의 중심국가로 도약할 수 있는 충분한 조건을 갖추고 있습니다. 우선, 지리적으로 중심에 자리잡고 있습니다. 서울에서 3시간의 비행거리 안에 인구 100만 이상의 도시가 마흔 세 개나 됩니다. 중국과 러시아의 인력과 자원, 그리고 일본의 기술을 접목할 수 있는 유리한 위치입니다. 대륙과 해양을 잇는 지정학적 이점도 가지고 있습니다. 하늘과 바다와 땅에 걸친 물류와, 세계 일류의 정보화 기반과 역량을 두루 갖추고 있습니다.\n한반도는 더 이상 세계의 변방이 아닙니다. 남북 철도가 연결되고 철의 실크로드가 열리면 광활한 대륙을 향해 나아갈 수 있습니다. 그 곳에는 중국대륙이라는 새로운 기회가 기다리고 있습니다. 시베리아와 중앙아시아의 무한한 자원도 있습니다.\n한반도가 대륙과 해양을 잇는 물류와 금융과 생산 거점으로 거듭나게 됩니다. 이것이 바로 우리 앞에 있는 미래입니다. 우리에게는 이를 현실로 만들어야 하는 책무가 주어져 있습니다.\n존경하는 국민 여러분,\n그러나 동북아 중심국가로 나아가기 위해서는 반드시 해야 할 일이 있습니다. 한반도에 평화를 정착시키는 일입니다. 남북이 대립하며 한반도에 긴장이 고조되는 한, 동북아 중심국가의 꿈은 실현될 수 없습니다. 동북아의 평화와 번영도 기대하기 어렵습니다.\n그동안 우리는 한반도에 평화를 정착시키기 위해 많은 노력을 기울여 왔습니다. 남북간에 대화와 교류가 빈번해졌고 이산가족이 만나고 있습니다. 최근에는 육로도 열렸습니다.\n그러나 아직 풀어야 할 숙제가 많습니다. 특히 북핵 문제는 시급히 해결해야 할 과제입니다.\n저는 북한의 핵 개발에 단호히 반대합니다. 그러나 이 문제는 반드시 평화적으로 해결되어야 합니다. 어떠한 이유로든 한반도의 평화가 깨어진다면, 우리는 그 엄청난 재앙을 감당할 수 없습니다. 한반도의 평화와 국민의 안전을 지키는 것은 대통령의 가장 큰 책무입니다.\n앞으로 남북관계는 국민 여러분께 소상히 보고 드리고, 국민적 합의를 바탕으로 추진해 나가겠습니다. 야당의 협력도 적극적으로 구해나갈 것입니다. 미국과 일본, 중국, 러시아 등 주변국과, EU를 비롯한 국제사회와도 능동적으로 협력해나갈 것입니다.\n국민 여러분,\n한반도에 평화를 정착시키는 일 못지 않게 중요한 것은 국민의 힘을 하나로 모으는 일입니다. 84년 전 오늘, 우리의 선열들은 한마음 한뜻으로 독립운동에 나섰습니다. 빈부와 귀천, 남녀와 노소, 지역과 종교의 차이는 없었습니다. 나라의 독립과 민족의 자존심을 되찾는 데 하나가 되었습니다.\n오늘을 사는 우리도 지역과 계층과 세대를 넘어 하나가 되어야 합니다. 내부에 분열과 반목이 있으면 세계경쟁에서 뒤쳐질 수밖에 없습니다. 국권까지 상실했던 100년 전의 실패가 되풀이될 수도 있습니다. 지금이야말로 3, 1정신을 되돌아보며 역사의 교훈을 되새겨야 할 때입니다.\n마음속에 지역갈등의 응어리가 있다면 가슴을 열고 풀어야 합니다. 어른은 젊은이의 목소리에 귀기울이고 젊은이는 어른의 경험을 구해야 합니다. 차별 받고 소외되어 온 사람들에게 더 많은 관심과 노력을 기울여야 합니다. 국민 모두가 참된 주인으로서 국정에 참여하고, 온 국민의 힘을 하나로 모으는 국민참여시대를 힘차게 열어가야겠습니다.\n개혁 또한 멈출 수 없는 우리 시대의 과제입니다. 무엇보다 정치와 행정이 바뀌어야 합니다. 이른바 몇몇 ’권력기관’은 그동안 정권을 위해 봉사해 왔던 것이 사실입니다. 그래서 내부의 질서가 무너지고 국민의 신뢰를 잃었습니다. 이제 이들 ’권력기관’은 국민을 위한 기관으로 거듭나야 합니다. 참여정부는 더 이상 ’권력기관’에 의존하지 않을 것입니다. 언제나 정정당당한 정부로서 국민 앞에 설 것입니다.\n참여정부는 공정하고 투명한 시장질서, 노사화합, 기술혁신, 지역 균형발전 속에 정직하고 성실하게 사는 사람들이 성공하는 나라를 만들어갈 것입니다. 이를 위해 원칙과 신뢰, 공정과 투명, 대화와 타협, 분권과 자율의 문화를 사회 곳곳에 뿌리내릴 것입니다.\n존경하는 국민 여러분!\n우리에게는 선열들이 보여준 자주독립의 기상과 대동단결의 지혜가 있습니다. 오늘 3, 1절을 맞아 일제의 총칼에 항거하며 이루고자 했던 선열들의 뜻을 다시 한번 가슴에 새깁시다. 국민통합과 개혁으로 평화와 번영의 동북아시대를 열어갑시다. 자랑스런 대한민국을 우리 후손들에게 물려줍시다.\n\n\n\n코드library(tidyverse)\n\nstt_txt &lt;- read_lines(\"data/stt_audio.txt\")\n\nstt_txt\n\n\n\n\n\n\n\n\n\n\n프롬프트: 다음 텍스트에 구두점을 넣고 문단별로 나눠 가독성을 높혀주세요.  “존경하는 국민여러분 오늘 84번째 ..&lt;중략&gt;.. 물려줍시다 감사합니다”\n\n\n\n“존경하는 국민여러분, 오늘 84번째 3.1절을 맞아, 나라를 위해 희생하고 헌신하신 애국선열들께 한없는 감사와 경의를 표합니다. 독립유공자와 유가족 여러분에게도 존경과 감사의 말씀을 드립니다.\n3.1 1922년 오늘, 우리는 일제의 총칼에 맞서 맨주먹으로 분연히 일어섰습니다. 대한독립만세 소리가 전국 방방곡곡을 뒤덮었고, 우리는 자주독립의지를 세계 만방에 알렸습니다. 3.1운동을 계기로 국내외의 독립투쟁은 더욱 힘차게 전개되었습니다. 상해 대한민국 임시정부가 세워졌고, 우리는 마침내 빼앗긴 국권을 되찾았습니다.\n3.1정신은 끊임없는 도전을 슬기롭게 극복해온 우리 민족의 자랑입니다. 우리는 이러한 빛나는 정신을 계승하여 전쟁의 폐허를 딛고 세계 12위의 경제강국으로 발돋움했습니다. 4.19혁명과 광주민주화운동, 6월 민주항쟁을 거쳐서 민주주의와 인권을 쟁취해냈습니다. 오늘의 참여정부는 바로 그 위대한 역사의 연장선 위에 서 있습니다.\n참여정부의 출범으로, 이제 아픔의 근현대사는 막을 내리게 되었습니다. 우리의 지난 날은 선열들의 고귀한 희생에도 불구하고 좌절과 굴절을 겪어야 했습니다. 정의는 패배했고 기회주의가 덕세했습니다. 그러나 이제 비로소 역사적 전환점이 마련됐습니다. 국민이 진정 주인으로 대접받는 시대가 열릴 것입니다. 참여정부에서는 권력의 아부하는 사람들이 더 이상 설 땅이 없을 것입니다. 오로지 성실하게 일하고 정정당당하게 선고하는 사람들이 성공하는 시대가 열릴 것입니다. 그것이 바로 선열들의 희생에 보답하는 길이자, 저와 참여정부에게 주어진 역사의 소명이라고 생각합니다.\n존경하는 국민 여러분, 지금 우리는 세계사의 새로운 흐름과 마주하고 있습니다. 동북아 시대의 도래가 바로 그것입니다. 동북아시아는 근대 이후 세계의 변방으로만 머물러 왔습니다. 그러나 이제 유럽연합, 북미 지역과 함께 세계 경제의 3대 축으로 부상하고 있습니다. 앞으로 20년 후에는 세계 경제의 3분의 1을 차지하게 된다는 전망도 있습니다.\n민족웅비의 크나큰 기회가 우리에게 다가오고 있는 것입니다. 우리는 동북아 시대의 중심국가로 돌아갈 수 있는 충분한 조건들의 꿈을 생각해 보겠습니다. 이 땅에 건설한 자유와 평화, 독립과 민주주의의 신성한 꿈입니다. 우리는 그 꿈을 계승하고 이어가야 합니다.\n참여정부는 모든 일을 국민과 함께 할 것입니다. 어려운 일에도 함께 참여하고, 희망을 나누는 일에도 함께 참여하겠습니다. 정의를 위해 함께 서고, 기회를 위해 함께 투쟁하겠습니다. 바로 그것이 참여정부의 이념이고 실천의 방향입니다.\n모든 국민이 참여하는 민주주의, 그것이 바로 참여정부의 민주주의입니다. 대통령으로서 제게 주어진 역사의 소명을 다할 것입니다. 국민이 주인이 되는 참여정부, 그것을 만들어 가겠습니다. 국민 여러분의 도움과 협력이 절실히 필요합니다.\n국민 여러분, 오늘 3.1절을 맞아 이 자리에 함께 해주신 데 대해 깊은 감사의 말씀을 드립니다. 우리 모두가 동행하며, 선열들의 꿈을 이루어 나가는 큰 한 걸음을 내딛을 수 있기를 기원합니다. 감사합니다.\n\n\n\n\ngpt-4, 특히 gpt-4-0314 모형을 사용하여 API로 앞서 Whisper로 STT하여 받아쓰기한 사항을 윤문하여 연설문을 보기 좋게 다시 작성한다.\n\n코드import openai\nfrom dotenv import load_dotenv\nimport os\n\n# API KEY ----------------------\nload_dotenv()\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# 텍스트 불러오기 --------------------------\nwith open(\"data/stt_audio.txt\", \"r\") as file:\n    content = file.read()\n\nprint(content)\n\n# 텍스트 윤문 --------------------------\n\nprompt = f'다음 텍스트에 구두점을 넣고 문단별로 나눠 가독성을 높혀주세요. \\n\\n{content}'\n\nspeech_response = openai.ChatCompletion.create(\n  model       = \"gpt-4-0314\",\n  messages    = [\n    {\"role\": \"system\", \"content\": \"You are a Korean language expert.\"},\n    {\"role\": \"user\", \"content\": prompt}\n  ],\n  max_tokens   = 5000,\n  temperature  = 0\n)\n\nprint(speech_response[\"choices\"][0]['message']['content'])\n\nwith open(\"data/stt_audio_gpt4.txt\", \"w\") as file:\n    file.write(speech_response[\"choices\"][0]['message']['content'])\n\n\n\n코드gpt4_txt &lt;- read_lines(\"data/stt_audio_gpt4.txt\")\n\nglue::glue(\"{gpt4_txt}\")",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#번역",
    "href": "openai_apps.html#번역",
    "title": "",
    "section": "번역",
    "text": "번역\nWhisper API를 사용하면 다국어를 번역하는 것도 가능하다. 먼저, 연설문 앞쪽부분을 영어로 실시간 번역해보자. Whisper는 영어가 아닌 다양한 언어음성을 영어로 번역하는 기능도 갖추고 있다.\n\n\n연설문 초반부\n연설 원음\n한국어 음성 → 영문\n\n\n\n존경하는 국민 여러분,\n오늘 여든 네 번째 3, 1절을 맞아 나라를 위해 희생하고 헌신하신 애국선열들께 한없는 감사와 경의를 표합니다. 독립유공자와 유가족 여러분에게도 존경과 감사의 말씀을 드립니다.\n\n\n\n코드library(av)\nlibrary(embedr)\n\nembedr::embed_audio(\"data/한국어_영어번역.mp3\")\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n특히, 프롬프트에 맥락 정보를 넣어 전달하면 더욱 정확한 번역 결과를 기대할 수 있다.\n\n코드import openai\nfrom dotenv import load_dotenv\nimport os\n\n# API KEY ----------------------\nload_dotenv()\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# 번역 ----------------------\nstart_audio_file = open(\"data/한국어_영어번역.mp3\", \"rb\")\nstart_prompt = \"한국 대통령이 3.1절 기념식 연설문 중 앞부분입니다.\"\nstart_resp = openai.Audio.translate(\"whisper-1\",\n                                    start_audio_file,\n                                    prompt = start_prompt)\nprint(start_resp[\"text\"])\n\n\nHonorable citizens, Today, on the 84th anniversary of 3.1, I would like to express my deepest gratitude and respect to the patriotic soldiers who sacrificed for the country. I would also like to express my respect and gratitude to the independence activists and their families.",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#뉴스기사-제작",
    "href": "openai_apps.html#뉴스기사-제작",
    "title": "",
    "section": "뉴스기사 제작",
    "text": "뉴스기사 제작\n오디오나 사진, 동영상 등 다양한 형태의 데이터를 텍스트 문자로 변환시키면 다음 단계로 다양한 후속 작업을 수행할 수 있다. 앞서 노무현 대통령의 3.1절 연설문을 거의 실시간으로 녹취를 떠서 이를 텍스트로 추출한 후 속보 형태 기사로 송고할 수 있다.\n\n\n코드import openai\nfrom dotenv import load_dotenv\nimport os\n\n# API KEY ----------------------\nload_dotenv()\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n\n# 연설문 불러오기 --------------------------\nwith open(\"data/stt_audio_gpt4.txt\", \"r\") as file:\n    speech_content = file.read()\n\nprompt = f'다음 텍스트를 뉴스 속보 형태 기사로 엄중한 문체로 100자 내외로 짧게 작성해주세요.. \\n\\n{speech_content}'\n\n# API KEY ----------------------\nbreaking_news_response = openai.ChatCompletion.create(\n  model       = \"gpt-3.5-turbo\",\n  messages    = [\n    {\"role\": \"system\", \"content\": \"You are a Korean reporter.\"},\n    {\"role\": \"user\", \"content\": prompt}\n  ],\n  max_tokens   = 800,\n  temperature  = 0\n)\n\nprint(breaking_news_response[\"choices\"][0]['message']['content'])\n\n\n[속보] 3.1절 맞아 국민에게 경의 표한\n오늘 84번째 3.1절을 맞아 애국선열들에게 감사와 경의를 표합니다. 독립유공자와 유가족에게도 존경과 감사의 말씀을 전합니다. 102년 전 오늘, 우리는 일제의 총칼에 맞서 독립을 위해 일어섰습니다. 3.1운동을 계기로 국내외의 독립투쟁은 더욱 힘차게 전개되었습니다. 오늘의 참여정부는 역사의 연장선 위에 서 있으며, 한반도의 평화를 정착시키는 일이 중요합니다. 국민 모두가 참된 주인으로서 국정에 참여하고 개혁을 추진해야 합니다.",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#ai-이미지",
    "href": "openai_apps.html#ai-이미지",
    "title": "",
    "section": "AI 이미지",
    "text": "AI 이미지\nAI 이미지 생성 프롬프트를 작성하고, DALL·E 2 - OpenAI openai.Image.create() 함수를 사용하면 AI 이미지를 생성할 수 있다.\n서울 강남 거리를 AI 이미지로 생성해보자.\n\n프롬프트: streets of Gangnam Seoul Korea, summertime, bright and beautiful, cinematic landscape\n\n\n코드import openai\nfrom dotenv import load_dotenv\nimport os\n\n# API KEY ----------------------\nload_dotenv()\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# 프롬프트 ----------------------\nprompt = \"streets of Gangnam Seoul Korea, summertime, bright and beautiful, cinematic landscape\"\n\ngangnam_response = openai.Image.create(\n    prompt=prompt,\n    model=\"image-alpha-001\",\n    size=\"1024x1024\",\n    response_format=\"url\"\n)\n\nprint(gangnam_response[\"data\"][0][\"url\"])\n\n\nAI 생성이미지 바로가기: https://oaidalleapiprodscus.blob.core.windows.net/private/org-GpPkNlGHcRh9i7pQIlhT18p7/user-Qkv0ntrn5tQoUu6pocAidY5V/img-BHSmfeMCmONIkrl3nSHIZIR7.png?st=2023-07-21T05%3A08%3A01Z&se=2023-07-21T07%3A08%3A01Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-07-20T20%3A06%3A58Z&ske=2023-07-21T20%3A06%3A58Z&sks=b&skv=2021-08-06&sig=8G4gBoBYYHoklBNVR1ggHiURZulcxJ62y0YjJ/OJfkM%3D\nAI 이미지 후처리\n웹에 이미지가 걸려있어 이를 로컬 파일로 다운로드 받아 후속 작업에 활용할 수 있도록 코드를 작성한다.\n\n코드import requests\nfrom PIL import Image\nimport io\n\n# 이미지 URL\nurl = \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-GpPkNlGHcRh9i7pQIlhT18p7/user-Qkv0ntrn5tQoUu6pocAidY5V/img-BHSmfeMCmONIkrl3nSHIZIR7.png?st=2023-07-21T05%3A08%3A01Z&se=2023-07-21T07%3A08%3A01Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-07-20T20%3A06%3A58Z&ske=2023-07-21T20%3A06%3A58Z&sks=b&skv=2021-08-06&sig=8G4gBoBYYHoklBNVR1ggHiURZulcxJ62y0YjJ/OJfkM%3D\"\n\n# GET 요청\nresponse = requests.get(url)\n\n# 이미지 가져오기\nimage = Image.open(io.BytesIO(response.content))\n\n# 이미지 JPEG 변환\nimage.save('images/dalle_image.jpeg', 'JPEG')",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "openai_apps.html#임베딩",
    "href": "openai_apps.html#임베딩",
    "title": "",
    "section": "임베딩",
    "text": "임베딩\n임베딩(Embedding) 일반적으로 실수 벡터의 형태로 텍스트 데이터를 수치로 표현한 것으로, 이를 통해서 표현된 벡터는 연속적인 벡터 공간에서 텍스트 데이터에 내재된 의미(semantic)를 포착한다.\n실용적인 관점에서 임베딩은 실제 객체와 관계를 벡터로 표현하는 방식이다. 동일한 벡터 공간을 사용하여 두 사물이 얼마나 유사한지 측정하는 데 사용한다.\n텍스트 벡터 표현\ntext-embedding-ada-002 모델은 빠르고 가성비가 뛰어난 임베딩 모델이다. “대한민국 수도는 서울입니다.” 이라는 문서를 벡터로 표현하면 다음과 같다. 즉, 1,536 차원을 갖는 공간에 하나의 점으로 표현될 수 있다.\n\n코드import os\nimport openai\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nseoul_response = openai.Embedding.create(\n  model=\"text-embedding-ada-002\",\n  input=\"대한민국 수도는 서울입니다.\",\n)\n\nseoul_embedding = seoul_response[\"data\"][0]['embedding']\n\nprint(f'벡터길이: {len(seoul_embedding)}')\n#&gt; 벡터길이: 1536\nprint(f'벡터 일부: {seoul_embedding[:10]}')\n#&gt; 벡터 일부: [0.014582998119294643, -0.018063032999634743, 0.004872684367001057, -0.013805408962070942, -0.031180081889033318, 0.025176068767905235, -0.034519895911216736, 0.011357911862432957, -0.007960736751556396, -0.0020682618487626314]\n\n\n마찬가지로 일본의 수도 도쿄도 벡터로 표현할 수 있다.\n\n코드tokyo_response = openai.Embedding.create(\n  model=\"text-embedding-ada-002\",\n  input=\"일본 수도는 동경입니다.\",\n)\n\ntokyo_embedding = tokyo_response[\"data\"][0]['embedding']\nprint(f'벡터길이: {len(tokyo_embedding)}')\n#&gt; 벡터길이: 1536\nprint(f'벡터 일부: {tokyo_embedding[:10]}')\n#&gt; 벡터 일부: [0.010957648046314716, -0.013234060257673264, 0.009729413315653801, -0.011890077032148838, -0.03179261088371277, 0.03436483070254326, -0.029786281287670135, 0.008629790507256985, 0.01711810939013958, -0.0014733985299244523]",
    "crumbs": [
      "사례",
      "OpenAI 응용프로그램"
    ]
  },
  {
    "objectID": "langchain.html",
    "href": "langchain.html",
    "title": "랭체인",
    "section": "",
    "text": "랭체인(langcahin)은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 소프트웨어 개발 프레임워크로, LLM을 다양한 애플리케이션과 통합하는 것을 용이하기 쉽기 때문에 인기를 얻고 있다. 랭체인은 LLM과 인터페이스, 다양한 구성 요소 연결, 메모리 관리 등이 수월하기 때문에 특히 개발자 사이에서 인기가 높다. 랭체인의 주요 목적은 LLM 기반 애플리케이션 개발을 단순화하고 가속화하는 것으로 데이터 처리, 프롬프트 관리, 모델 통합 등 LLM 애플리케이션 개발의 여러 측면을 쉽게 다룰 수 있도록 도구와 추상화를 제공한다.",
    "crumbs": [
      "AI 코딩",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#api",
    "href": "langchain.html#api",
    "title": "랭체인",
    "section": "API",
    "text": "API\n\n허깅페이스\n파이썬과 R을 사용해 Hugging Face Hub의 대형 언어 모델(Large Language Model, LLM)을 활용한다. 파이썬에서는 필요한 라이브러리를 설치하고, R에서는 reticulate 라이브러리를 통해 파이썬 환경을 사용한다. 파이썬 코드에서 Hugging Face Hub에 접근하기 위한 API 토큰을 로드하고, HuggingFaceHub 클래스를 사용하여 특정 모델(‘tiiuae/falcon-7b-instruct’)에 질문을 하고, 모델의 답변을 출력한다.\n\npip install langchain_community, pip install dotenv, pip install langchain-huggingface: 이 세 명령어는 파이썬 환경에서 필요한 패키지들을 설치한다. langchain_community는 언어 체인 커뮤니티 라이브러리, dotenv는 환경 변수를 관리하는 라이브러리, huggingface_hub는 Hugging Face Hub와 연동하는 데 사용되는 라이브러리다.\nR 코드 부분에서 library(reticulate)를 사용해 파이썬과 R 사이의 상호작용을 가능하게 하는 reticulate 라이브러리를 로드한다. use_condaenv(\"langchain\", required = TRUE)는 langchain이라는 이름의 Conda 환경을 사용하도록 지시한다. 이는 파이썬 코드를 R 환경에서 실행하기 위한 준비 단계다.\n파이썬 코드에서는 먼저 langchain_community.llms에서 HuggingFaceHub 클래스를, dotenv에서 load_dotenv 함수를 가져온다. 이후 os 모듈을 임포트한다. load_dotenv()를 호출하여 환경 변수를 로드한다. 이는 .env 파일에 저장된 환경 변수를 사용할 수 있게 한다.\nhuggingfacehub_api_token = os.getenv('HF_TOKEN')는 환경 변수에서 ’HF_TOKEN’을 찾아 해당 토큰을 변수에 저장한다. 이 토큰은 Hugging Face Hub에 접근할 때 인증을 위해 사용된다.\nHuggingFaceHub 클래스의 인스턴스를 생성한다. 이 때 repo_id에는 사용할 Hugging Face 모델의 저장소 ID를, huggingfacehub_api_token에는 위에서 얻은 API 토큰을 넣는다.\n대형 언어 모델에 질문을 하기 위해 question 변수에 질문을 저장하고, llm.invoke(question)을 호출하여 모델에 질문을 전달하고 결과를 받는다.\n마지막으로 print(output)을 통해 얻은 결과를 출력한다. 이 코드는 Hugging Face Hub의 특정 모델을 사용하여 질문에 대한 답변을 얻는 과정을 보여준다.\n\n\npip install langchain_community\npip install dotenv\npip install -U langchain-huggingface\n\n다양한 한국어가 지원되는 언어모형을 실험해봤지만 언어모형 크기가 큰 경우 실행이 불가능하다.\n\n\n코드\nfrom langchain_huggingface import HuggingFaceEndpoint\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nhuggingfacehub_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\nhuggingfacehub_api_token\n\nllm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', \n                     huggingfacehub_api_token = huggingfacehub_api_token)\n\nquestion = 'What is LLM in AI?'\noutput = llm.invoke(question)\n\nprint(output)\n\n\n\n\nOpenAI\npip install --upgrade langchain openai 명령어로 openai 패키지를 설치하고 pip install langchain-openai 명령어로 랭체인 인터페이스를 사용해서 LLM 을 활용한다.\n\n\n코드\nfrom langchain_openai import OpenAI\n\nopenai_api_key = os.getenv('OPENAI_API_KEY')\n\nllm = OpenAI(\n    model_name=\"gpt-3.5-turbo-instruct\",   \n    openai_api_key=openai_api_key\n)\nquestion = '인공지능 대규모 언어모형 LLM이 뭐야'\noutput = llm.invoke(question)\nprint(output)\n\n\nLLM은 'Large Language Model'의 약자로, 인공지능 기술 중 하나인 자연어 처리(Natural Language Processing) 분야에서 사용되는 대규모 언어모형을 말합니다. LLM은 수많은 문장과 단어를 학습하고 이를 바탕으로 텍스트를 생성하고 이해하는 기술을 갖춘 인공지능 모델을 의미합니다. 이를 통해 LLM은 인간과 유사한 수준의 언어 이해 및 생성 능력을 가지고 있습니다. LLM은 다양한 분야에서 활용되고 있으며, 텍스트 생성, 기계 번역, 자연어 이해 등 다양한 응용 분야에서 성능을 발휘하고 있습니다.\n\n\n클로드\npip install -U langchain-anthropic 앤트로픽을 설치한 후 동일하게 실행한다.\n\n\n코드\nfrom langchain_anthropic import ChatAnthropic\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nanthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n\nllm = ChatAnthropic(\n    model=\"claude-3-opus-20240229\",\n    anthropic_api_key=anthropic_api_key\n)\n\nquestion = '인공지능 대규모 언어모형 LLM이 뭐야?'\noutput = llm.invoke(question)\nprint(output.content)\n\n\n'인공지능 대규모 언어모형(Large Language Model, LLM)은 방대한 양의 텍스트 데이터를 학습하여 만들어진 거대한 신경망 모델입니다. LLM은 다음과 같은 특징을 가지고 있습니다:\\n\\n1. 데이터 크기: 수백 기가바이트에서 수 테라바이트에 이르는 방대한 텍스트 데이터로 학습합니다.\\n\\n2. 모델 크기: 수십억에서 수조 개의 매개변수를 가진 거대한 신경망 구조를 가집니다. \\n\\n3. 자연어 이해 및 생성: 문맥을 이해하고 자연스러운 언어를 생성할 수 있습니다.\\n\\n4. 다양한 태스크 수행: 질의응답, 요약, 번역, 창작 등 다양한 자연어 처리 태스크를 수행할 수 있습니다.\\n\\n5. 사전 학습과 전이 학습: 대량의 데이터로 사전 학습된 후, 특정 태스크를 위해 추가 학습(전이 학습)될 수 있습니다.\\n\\n대표적인 LLM으로는 GPT-3, BERT, XLNet, T5 등이 있습니다. 이러한 모델들은 자연어 처리 분야에서 혁신을 가져왔으며, 다양한 응용 분야에서 활용되고 있습니다. 그러나 막대한 컴퓨팅 자원이 필요하고, 편향성 문제 등 한계점도 존재합니다. LLM 기술은 계속 발전하고 있으며, 앞으로도 자연어 인공지능 분야를 선도할 것으로 예상됩니다.'",
    "crumbs": [
      "AI 코딩",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#프롬프트-템플릿",
    "href": "langchain.html#프롬프트-템플릿",
    "title": "랭체인",
    "section": "프롬프트 템플릿",
    "text": "프롬프트 템플릿\n프롬프트 템플릿(Prompt Template)은 대규모 언어 모델(LLM)에 입력할 프롬프트의 구조를 정의하는 틀(template)로 일관된 형식의 프롬프트를 생성하고, 동적으로 내용을 채워 넣을 수 있게 해주는 도구다.\n\n재사용성: 동일한 구조의 프롬프트를 여러 번 사용할 수 있다.\n일관성: 프롬프트의 형식을 일정하게 유지할 수 있다.\n동적 내용: 변수를 사용하여 프롬프트의 특정 부분을 동적으로 변경할 수 있다.\n구조화: 컨텍스트, 질문, 응답 형식 등을 체계적으로 구성할 수 있다.\n\n프롬프트 템플릿은 주로 세 가지 요소(컨텍스트, 질문, 응답 형식)로 이루어진다.\n컨텍스트는 모델에게 배경 정보를 제공하고, 질문은 모델에게 요구하는 구체적인 작업을 명시한다. 응답 형식은 모델이 어떤 방식으로 답변해야 하는지 지시한다.\n프롬프트 템플릿에는 변수를 포함시킬 수 있다. 변수는 프롬프트를 생성할 때 동적으로 값이 채워지는 부분으로 동일한 구조의 프롬프트를 다양한 상황에 맞춰 재사용할 수 있다.\n프롬프트 템플릿은 이러한 요소들을 조합해 최종 프롬프트를 만들어내는 과정에서 변수에 실제 값이 할당되고, 전체 프롬프트의 구조가 완성된다. 완성된 최종 프롬프트는 대규모 언어 모델(LLM)에 입력된다. LLM은 이 프롬프트를 받아 처리하고, 요청된 작업에 따라 적절한 응답을 생성해 출력한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n코드\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# .env 파일에서 환경 변수 로드\nload_dotenv()\n\n# 프롬프트 템플릿 정의\ntemplate = \"\"\"\n컨텍스트: {context}\n\n질문: {question}\n\n응답 형식: {response_format}\n\n위의 컨텍스트를 바탕으로 질문에 답하세요. 응답은 제시된 형식을 따라주세요.\n\"\"\"\n\nprompt_template = PromptTemplate(\n    template=template,\n    input_variables=[\"context\", \"question\", \"response_format\"]\n)\n\n# OpenAI LLM 초기화 (API 키는 .env 파일에서 가져옴)\nllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\",   \n             temperature=0.7, \n             openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# 프롬프트 생성\nprompt = prompt_template.format(\n    context=\"랭체인은 대규모 언어 모델을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\",\n    question=\"랭체인의 주요 특징 세 가지는 무엇인가요?\",\n    response_format=\"1. 첫 번째 특징\\n2. 두 번째 특징\\n3. 세 번째 특징\"\n)\n\n# LLM에 프롬프트 전달 및 응답 생성\nresponse = llm.invoke(prompt)\n\nprint(\"생성된 프롬프트:\")\nprint(prompt)\nprint(\"\\nLLM 응답:\")\nprint(response)\n\n\n생성된 프롬프트:\n\n컨텍스트: 랭체인은 대규모 언어 모델을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n\n질문: 랭체인의 주요 특징 세 가지는 무엇인가요?\n\n응답 형식: 1. 첫 번째 특징\n2. 두 번째 특징\n3. 세 번째 특징\n\n위의 컨텍스트를 바탕으로 질문에 답하세요. 응답은 제시된 형식을 따라주세요.\n\n\nLLM 응답:\n\n1. 랭체인은 대규모 언어 모델을 활용하여 자연어 처리 작업을 수행할 수 있습니다.\n2. 랭체인은 다양한 애플리케이션 개발을 지원하기 위한 다양한 API를 제공합니다.\n3. 랭체인은 사용자가 직접 학습한 데이터를 추가하여 모델을 개선할 수 있는 기능을 제공합니다.",
    "crumbs": [
      "AI 코딩",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#lcel",
    "href": "langchain.html#lcel",
    "title": "랭체인",
    "section": "LCEL",
    "text": "LCEL\n랭체인 표현언어(LangChain Expression Language, LCEL)는 LangChain 컴포넌트들을 연결하고 조합하기 위한 선언적 방식의 인터페이스로 복잡한 AI 애플리케이션 구축을 단순화하고 가독성을 높이는 데 도움을 준다. 체인(Chain)은 LCEL의 핵심 개념 중 하나로, 여러 컴포넌트들을 연결하여 하나의 작업 흐름을 만드는 것을 말한다.\n유닉스 파이프(|)를 이해하고 있다면 prompt | model와 같이 프롬프트 템플릿과 LLM 모델을 연결하는 간단한 체인을 생성한다. 프롬프트 | LLM | 출력파서와 같은 패턴이 일반적이다. 프롬프트 템플릿으로 주제를 받아 프롬프트를 완성하고 LLM 모형에 전달하고 출력파서를 통해 원하는 결과물을 출력시킨다.\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom dotenv import load_dotenv\nimport os\n\n# .env 파일에서 환경 변수 로드\nload_dotenv()\n\n# 출력 파서 정의\noutput_parser = CommaSeparatedListOutputParser()\n\n# 프롬프트 템플릿 정의\nprompt_template = ChatPromptTemplate.from_template(\"\"\"\n다음 주제에 관련된 키워드를 5개 나열해주세요: {topic}\n\n당신의 응답은 반드시 쉼표로 구분된 단일 단어 목록이어야 합니다.\n\"\"\")\n\n# 모델 정의 (API 키는 .env 파일에서 가져옴)\nmodel = OpenAI(\n    model_name=\"gpt-3.5-turbo-instruct\",   \n    temperature=0.7, \n    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n\n# 체인 구성\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt_template \n    | model \n    | output_parser\n)\n\n# 체인 실행\nresult = chain.invoke(\"인공지능\")\n\nprint(\"인공지능 관련 키워드:\")\nfor keyword in result:\n    print(f\"- {keyword}\")\n\n\n인공지능 관련 키워드:\n- 기계학습\n- 딥러닝\n- 자연어처리\n- 로봇\n- 빅데이터\nLCEL과 Chain을 사용하면 복잡한 AI 로직을 더 쉽게 구조화하고 관리할 수 있으며, 코드의 가독성과 유지보수성을 크게 향상시킬 수 있다.",
    "crumbs": [
      "AI 코딩",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#랭그래프",
    "href": "langchain.html#랭그래프",
    "title": "랭체인",
    "section": "랭그래프",
    "text": "랭그래프\n랭그래프(LangGraph)는 랭체인(LangChain)의 일부로, 에이전트(Agent) 시스템을 설계하기 위한 도구로 복잡한 AI 워크플로우를 구축하는 데 사용된다. 랭그래프는 에이전트의 상태를 추적하고 관리할 수 있는 상태 관리 기능을 제공한다. 이를 통해 장기적인 대화나 복잡한 작업의 진행 상황을 효과적으로 추적할 수 있고, 조건부 로직과 반복을 포함한 복잡한 워크플로우를 정의할 수 있는 유연성을 제공한다. 또한, 다양한 외부 도구와 API를 쉽게 통합할 수 있는 기능도 랭그래프의 중요한 특징으로 AI 시스템의 능력을 확장하고 실제 세계의 다양한 작업을 수행할 수 있게 된다. 특히, 랭그래프는 재사용 가능한 컴포넌트를 만들어 복잡한 시스템을 구축할 수 있는 모듈성을 제공하여 개발 효율성을 높이고 일관성 있는 시스템 구축을 가능하게 한다.\npip install langgraph 명령어로 설치한 후 프롬프트 템플릿에 사칙연산 관련 문제풀이로 LLM을 활용한다. LLMMathChain은 numexpr 패키지도 필요하니 pip install numexpr 명령어로 설치한다.\n\n코드실행결과\n\n\n\n\n코드\nfrom langchain.agents import initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Get OpenAI API key\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\nllm = OpenAI(\n    model_name=\"gpt-3.5-turbo-instruct\",  \n    temperature=0.0, \n    openai_api_key=openai_api_key\n)\n\n# Load the necessary tools\ntools = load_tools([\"llm-math\"], llm=llm)\n\n# Define the prompt for the agent\nprompt = \"\"\"\nYou are a helpful assistant that can perform various mathematical calculations and provide accurate results.\n\"\"\"\n\n# Create the ReAct agent with the prompt\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent_type=\"react\",\n    verbose=True\n)\n\n# List of questions to ask the agent\nquestions = [\n    \"15 더하기 27은 얼마인가요?\",\n    \"144의 제곱근은 얼마인가요?\",\n    \"250의 30%는 얼마인가요?\",\n    \"5 곱하기 8에서 12를 뺀 값은 얼마인가요?\",\n]\n\n# Ask each question to the agent and print the response\nfor question in questions:\n    response = agent.invoke({\"input\": question})\n    print(f\"질문: {question}\")\n    print(f\"답변: {response['output']}\\n\")\n\n\n\n\n&gt; Entering new AgentExecutor chain...\n I should use a calculator to add 15 and 27\nAction: Calculator\n\nObservation: Answer: 42\nThought: I now know the final answer\nFinal Answer: 42\n\n&gt; Finished chain.\n질문: 15 더하기 27은 얼마인가요?\n답변: 42\n\n\n\n&gt; Entering new AgentExecutor chain...\n I should use a calculator to find the square root of 144\nAction: Calculator\nAction Input: 144\nObservation: Answer: 144\n I now know the final answer\nFinal Answer: 12\n\n&gt; Finished chain.\n질문: 144의 제곱근은 얼마인가요?\n답변: 12\n\n\n\n&gt; Entering new AgentExecutor chain...\n We need to find the percentage of 250.\nAction: Calculator\n\nObservation: Answer: 75.0\nThought: We have found the percentage.\nFinal Answer: 75.0\n\n&gt; Finished chain.\n질문: 250의 30%는 얼마인가요?\n답변: 75.0\n\n\n\n&gt; Entering new AgentExecutor chain...\n I should use a calculator to solve this problem.\nAction: Calculator\nAction Input: 5 * 8 - 12\nObservation: Answer: 28\n I now know the final answer.\nFinal Answer: 28\n\n&gt; Finished chain.\n질문: 5 곱하기 8에서 12를 뺀 값은 얼마인가요?\n답변: 28",
    "crumbs": [
      "AI 코딩",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#rag",
    "href": "langchain.html#rag",
    "title": "랭체인",
    "section": "RAG",
    "text": "RAG\n검색 증강 생성(Retrieval Augmented Generation, RAG)는 대규모 언어 모델(LLM)의 성능을 향상시키기 위해 외부 지식을 활용하는 기술로 사용자의 질문이나 프롬프트에 대해 관련성 높은 정보를 검색하고, 이를 원래의 프롬프트와 결합하여 LLM에 제공한다. RAG의 핵심 아이디어는 LLM의 생성 능력과 외부 데이터베이스의 최신 정보를 결합하는 것으로 임베딩(Embedding)이 중요한 역할을 한다. 사용자의 질문과 데이터베이스 내 문서들은 벡터 형태로 변환되며, 이를 통해 의미적 유사성을 기반으로 관련 정보를 빠르게 검색할 수 있다.\nRAG의 작동 과정은 다음과 같다. 먼저, 사용자가 질문을 입력하면 이 질문은 벡터로 변환된다. 그 다음, 이 벡터를 사용해 미리 준비된 벡터 데이터베이스에서 가장 유사한 문서나 정보를 검색한다. 검색된 정보는 원래의 질문과 함께 새로운 프롬프트를 구성하는 데 사용된다. 이렇게 증강된 프롬프트가 LLM에 입력되어 최종 응답을 생성한다.\nLLM은 학습 시점의 데이터에 기반하므로 최신 정보를 반영하지 못할 수 있지만, RAG를 통해 외부 데이터베이스의 최신 정보를 활용할 수 있어 LLM의 환각(hallucination) 문제를 줄이고 더 정확하고 신뢰할 수 있는 응답을 생성하는 데 도움을 준다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy 버전 오류\n\n\n\nAttributeError: np.float_ was removed in the NumPy 2.0 release. Use np.float64 instead.\nCell In[53], line 26\n     24 # 임베딩 모델 및 벡터 저장소 설정\n     25 embeddings = OpenAIEmbeddings()\n---&gt; 26 vectorstore = Chroma.from_documents(texts, embeddings)\n     28 # RAG 체인 설정\n     29 llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nShow Traceback\n\n\n\n\n코드\nfrom langchain.document_loaders import WikipediaLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# 환경 변수 로드\nload_dotenv()\n\n# OpenAI API 키 설정\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n# 위키백과에서 \"인공지능\" 문서 로드\nloader = WikipediaLoader(\"인공지능\", load_max_docs=1)\ndocuments = loader.load()\n\n# 문서 분할\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_documents(documents)\n\n# 임베딩 모델 및 벡터 저장소 설정\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(texts, embeddings)\n\n# RAG 체인 설정\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever()\n)\n\n# 질문-답변 루프\nwhile True:\n    query = input(\"\\n질문을 입력하세요 (종료하려면 'q' 입력): \")\n    if query.lower() == 'q':\n        break\n    result = qa_chain.invoke(query)\n    print(f\"\\n답변: {result['result']}\")",
    "crumbs": [
      "AI 코딩",
      "랭체인"
    ]
  },
  {
    "objectID": "API.html",
    "href": "API.html",
    "title": "",
    "section": "",
    "text": "기본기API 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "API.html#restful-api",
    "href": "API.html#restful-api",
    "title": "",
    "section": "RESTful API",
    "text": "RESTful API\nRESTful API가 사실상의 표준이 된 이유는 여러 가지가 있다. 우선, HTTP 프로토콜을 기반으로 하여 이해하기 쉽고 사용이 간편한 단순성과 직관성을 갖추고 있다. 또한, 클라이언트-서버 분리를 통해 각 부분을 독립적으로 확장할 수 있는 확장성을 제공한다. 다양한 플랫폼에서 사용 가능하며 언어에 구애받지 않는 플랫폼 독립성도 갖추고 있다. 기존 웹 인프라를 그대로 활용할 수 있어 웹 표준을 준수하며, JSON, XML 등 다양한 데이터 형식을 지원하는 유연성도 갖추고 있다. 마지막으로, 캐싱을 통해 효율적인 리소스 사용이 가능한 성능 측면의 이점도 있다.\nRESTful API는 REST(Representational State Transfer) 아키텍처 원칙을 따르는 웹 서비스 API로 2000년 로이 필딩(Roy Fielding)이 그의 박사 논문에서 제안한 분산 시스템을 위한 아키텍처 스타일이다. RESTful API와 HTTP는 밀접한 관계를 갖는데, RESTful API는 HTTP 프로토콜의 특성을 최대한 활용하여 설계되었다. 이로 인해 기존 웹 인프라를 그대로 사용할 수 있으며, 웹 개발자들에게 친숙한 방식으로 API를 구현하고 사용할 수 있게 되었다.\nRESTful API는 몇 가지 주요 특징을 가지고 있다. 첫째, 자원 기반 구조를 채택하여 모든 것을 “자원”으로 표현하며, 각 자원은 고유한 URI를 갖는다. 둘째, 상태가 없는 통신 방식을 사용하여 각 요청이 독립적으로 처리되며, 서버는 클라이언트의 상태를 저장하지 않는다. 셋째, HTTP 메서드를 사용하여 자원을 조작하는 균일한 인터페이스를 제공한다. 넷째, 클라이언트와 서버의 관심사를 분리하여 독립적인 발전을 가능하게 하는 클라이언트-서버 구조를 채택한다. 다섯째, 중간 서버를 통해 로드 밸런싱, 공유 캐시 등을 구현할 수 있는 계층화 시스템을 지원한다. 마지막으로, 응답을 캐시할 수 있어 성능 향상이 가능한 캐시 가능성을 제공한다.\n\n기반 프로토콜: RESTful API는 HTTP를 기본 프로토콜로 사용한다. HTTP의 메서드, 상태 코드, 헤더 등을 활용하여 통신한다.\n메서드 활용: RESTful API는 HTTP 메서드(GET, POST, PUT, DELETE 등)를 사용하여 CRUD(Create, Read, Update, Delete) 작업을 수행한다.\n상태 코드: HTTP 상태 코드를 사용하여 요청의 결과를 표현한다. 예를 들어, 200(OK), 201(Created), 404(Not Found) 등이다.\n헤더 활용: HTTP 헤더를 사용하여 메타데이터를 전송하고, 컨텐츠 협상, 인증 등을 처리한다.\nURL 구조: RESTful API는 HTTP URL을 사용하여 리소스를 식별하고 접근한다.\n무상태성: HTTP의 무상태성 특성을 그대로 따르며, 각 요청은 독립적으로 처리된다.\n\n\n\n표 1: HTTP 상태 코드표\n\n\n\n\n\n\n\n\n\n대분류\n코드\n상태\n설명\n\n\n\n정보\n100\nContinue\n서버가 요청의 일부를 받았으며 클라이언트가 나머지를 계속 보내야 함\n\n\n성공\n200\nOK\n요청이 성공적으로 처리됨\n\n\n성공\n201\nCreated\n새 리소스가 성공적으로 생성됨\n\n\n성공\n204\nNo Content\n요청은 성공했지만 응답 본문이 없음\n\n\n리다이렉션\n301\nMoved Permanently\n요청한 리소스의 URI가 변경됨\n\n\n클라이언트 오류\n400\nBad Request\n잘못된 요청으로 서버가 처리할 수 없음\n\n\n클라이언트 오류\n401\nUnauthorized\n인증이 필요한 리소스에 대한 인증 실패\n\n\n클라이언트 오류\n403\nForbidden\n서버가 요청을 거부함\n\n\n클라이언트 오류\n404\nNot Found\n요청한 리소스를 찾을 수 없음\n\n\n서버 오류\n500\nInternal Server Error\n서버 내부 오류 발생\n\n\n서버 오류\n503\nService Unavailable\n서버가 일시적으로 서비스를 제공할 수 없음\n\n\n네트워크\n504\nGateway Timeout\n게이트웨이나 프록시 역할하는 서버에서 시간 초과 발생",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "API.html#리소스",
    "href": "API.html#리소스",
    "title": "",
    "section": "리소스",
    "text": "리소스\nRESTful API는 리소스 중심으로 설계되어, 리소스를 통해 실제 세계의 개체나 개념을 표현하므로 API 구조를 이해하기 쉽다. 예를 들어, ‘/users’는 사용자 리소스를,’/products’는 제품 리소스를 나타내어 API의 목적과 기능을 명확히 전달한다. 또한, 리소스 중심 설계는 일관성을 제공한다. 모든 리소스에 대해 동일한 인터페이스(HTTP 메서드)를 사용하므로 API의 사용 방법이 일관된다. GET, POST, PUT, DELETE 등의 메서드로 모든 리소스를 조작할 수 있어 개발자들이 쉽게 API를 익히고 사용할 수 있다. 특히, 리소스 중심 설계의 가장 큰 장점은 플랫폼 독립성이다. 특정 프로그래밍 언어나 기술에 종속되지 않아 다양한 클라이언트와의 호환성을 보장되어 다양한 환경에서 API를 활용할 수 있다.\n리소스 예로는 사용자, 제품, 주문 등이 있다. 사용자 리소스의 경우 ‘/users/{user_id}’와 같은 URI로 표현되며, GET, POST, PUT, DELETE 등의 HTTP 메서드를 통해 사용자 정보의 조회, 생성, 수정, 삭제 등의 조작이 가능하다. 제품 리소스는’/products/{product_id}‘로 표현될 수 있으며, 제품 목록 조회, 새 제품 추가, 제품 정보 업데이트 등의 작업을 수행할 수 있다. 주문 리소스의 경우’/orders/{order_id}’로 표현되어 특정 사용자의 주문 목록 조회, 새 주문 생성, 주문 상태 변경 등의 기능을 제공할 수 있다.\n리소스 https://www.example.com:443/products/electronics?category=laptops&price=1000 를 해부학적으로 살펴보자.\n\n\n\n\n\n\n\n\n구성 요소\n설명\n예시\n비유\n\n\n\n프로토콜 (Protocol)\n데이터 전송 방식\nhttp://, https://\n교통 수단 (자동차, 기차)\n\n\n도메인 (Domain)\n웹 서버의 주소\nwww.example.com\n건물의 도로명 주소\n\n\n포트 (Port)\n서비스 접근 위한 논리적 연결 지점\n:80, :443\n건물의 출입구 또는 문\n\n\n경로 (Path)\n서버 내 리소스 구체적인 위치\n/products/electronics/\n건물 내 특정 사무실 유닛\n\n\n쿼리 (Query)\n리소스에 전달되는 추가 매개변수\n?category=laptops&price=1000\n추가 지시사항 또는 세부 요청",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "API.html#urllib과-requests",
    "href": "API.html#urllib과-requests",
    "title": "",
    "section": "\nurllib과 requests\n",
    "text": "urllib과 requests\n\nrequests는 Python에서 HTTP 요청을 보내는 작업을 단순화하고, 사용자 친화적으로 만든 패키지다. GET, POST, DELETE, PUT 등의 다양한 HTTP 메소드를 사용하여 웹 서버와 통신할 수 있고, requests는 JSON 응답을 쉽게 파싱하고, 쿠키를 유지하며, 파일 업로드, HTTP 헤더 조작, SSL 인증 등의 복잡한 작업을 쉽게 처리할 수 있다.\nurllib는 Python의 표준 라이브러리 중 하나지만, 사용법이 다소 복잡하고 번거로운데 반해, requests는 인터페이스가 간결하고 직관적이어서 같은 작업을 더 쉽게 수행할 수 있다.\nurllib과 requests는 파이썬에서 HTTP 통신을 위해 사용되는 라이브러리로 urllib은 파이썬 표준 라이브러리에 포함된 모듈이다. URL 처리와 HTTP 요청을 위한 기본적인 기능을 제공한다. 웹 페이지 열기, 데이터 다운로드, 서버에 데이터 전송 등의 작업을 수행할 수 있지만,사용법이 다소 복잡하고 저수준의 API를 제공한다.\nrequests는 써드파티 라이브러리로, HTTP 요청을 보다 쉽고 효율적으로 처리할 수 있게 해준다. urllib에 비해 더 직관적이고 사용하기 쉬운 인터페이스를 제공한다. GET, POST, PUT, DELETE 등 다양한 HTTP 메소드를 쉽게 사용할 수 있으며, 세션 관리, 쿠키 처리, 파일 업로드 등 고급 기능도 간단하게 구현할 수 있다.\nrequests 라이브러리가 대부분의 작업에서 더 간단하고 직관적인 인터페이스를 제공함하며, 특히 JSON 파싱, 인증, 세션 관리 등에서 requests가 더 편리하다. 그러나 두 라이브러리 모두 BeautifulSoup과 같은 추가 라이브러리와 함께 사용되어 HTML 파싱 등의 작업을 수행하고 병렬 처리 작업을 위해서 requests 생태계의 grequests 라이브러리를 사용하면 더 쉽게 구현할 수 있다.\n\n\n\n\n\n\n\n\n범주\n기능\nurllib\nrequests\n\n\n\n기본 요청\nGET 요청\nurllib.request.urlopen('http://example.com')\nrequests.get('http://example.com')\n\n\n\nPOST 요청\nurllib.request.urlopen('http://example.com', data=bytes(urlencode(data), encoding='utf-8'))\nrequests.post('http://example.com', data=data)\n\n\n헤더 처리\n사용자 정의 헤더\nurllib.request.Request('http://example.com', headers={'User-Agent': 'Mozilla/5.0'})\nrequests.get('http://example.com', headers={'User-Agent': 'Mozilla/5.0'})\n\n\n데이터 파싱\nHTML 파싱\n\nBeautifulSoup(html, 'html.parser') (별도 라이브러리 필요)\n\nBeautifulSoup(response.content, 'html.parser') (별도 라이브러리 필요)\n\n\n\nJSON 파싱\njson.loads(response.read().decode('utf-8'))\nresponse.json()\n\n\n인증\n기본 인증\nurllib.request.HTTPBasicAuthHandler()\nrequests.get('http://example.com', auth=('user', 'pass'))\n\n\n세션 관리\n쿠키 처리\nurllib.request.HTTPCookieProcessor()\nsession = requests.Session()\n\n\n에러 처리\n상태 코드 확인\n\ntry-except 블록으로 HTTPError 처리\nresponse.raise_for_status()\n\n\n병렬 처리\n동시 요청\n\nconcurrent.futures 모듈 사용 (복잡)\n\ngrequests 라이브러리 사용 (간단)\n\n\n\nurllib과 requests 패키지로 두가지 구현 코드를 비교해보자. 파머 펭귄 데이터셋 웹주소에서 데이터를 마치 로컬파일 데이터셋을 가져오듯이 동일한 방식으로 가져온다.\n\nurllib\n\n\n\n코드from urllib.request import urlretrieve\nimport pandas as pd\n\nurl = 'https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv'\n\nurlretrieve(url, 'data/penguins.csv')\n\n# Read file into a DataFrame and print its head\npenguins_df = pd.read_csv('data/penguins.csv', sep=',')\nprint(penguins_df.head())\n\n\n\n코드import requests\nimport pandas as pd\n\nurl = \"https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv\"\n\n# GET 요청을 보내 데이터를 받아옴\nresponse = requests.get(url)\n\n# 받아온 데이터를 파일로 저장\nwith open(\"data/penguins.csv\", \"wb\") as file:\n    file.write(response.content)\n\n# 파일을 DataFrame으로 읽어들임\npenguins_df = pd.read_csv(\"data/penguins.csv\", sep=\",\")\n\n# DataFrame의 처음 5행을 출력\nprint(penguins_df.head())",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "API.html#beautifulsoup",
    "href": "API.html#beautifulsoup",
    "title": "",
    "section": "BeautifulSoup",
    "text": "BeautifulSoup\nBeautifulSoup은 Python에서 HTML과 XML 문서를 파싱하기 위한 라이브러리다. 웹 스크래핑 작업에서 특히 유용하며, 복잡하고 잘 정돈되지 않은 웹 페이지에서 데이터를 쉽게 추출할 수 있다.\nBeautifulSoup은 문서를 트리 구조로 파싱하고, 이 트리를 통해 원하는 정보를 찾고, 수정하고, 네비게이트 할 수 있다. 예를 들어, HTML 문서에서 특정 태그를 찾거나, 특정 클래스 또는 ID를 가진 요소를 검색할 수 있다.\nrequests와 BeautifulSoup는 웹 스크래핑 작업에서 함께 사용되는 경우가 많다. 일반적으로 requests를 사용하여 웹페이지의 HTML을 가져오고, 그런 다음 BeautifulSoup을 사용하여 가져온 HTML을 파싱하고 원하는 정보를 추출하는 방식이다.\n\n\n웹사이트 제목\n링크\n\n\n\n\n코드import requests\nfrom bs4 import BeautifulSoup\n\nurl = 'https://r2bit.com'\n\nr = requests.get(url)\n\nhtml_doc = r.text\n\nsoup = BeautifulSoup(html_doc)\n\n# pretty_soup = soup.prettify()\nr2bit_title = soup.title\n\nprint(r2bit_title)\n\n&lt;title&gt;한국 R 사용자회&lt;/title&gt;\n\n\n\n\n\n코드a_tags = soup.find_all('a')\n\nfor link in a_tags:\n    print(link.get('href'))\n\n./index.html\n./index.html\n./packages.html\n./book.html\n./data.html\n./blog.html\n#\nhttps://www.sharingkorea.net/gboard/\nhttps://r2bit.com/sen\nhttps://choonghyunryu.github.io/workshop_lecture/\nhttps://r2bit.com/bitSlide/\nhttps://r2bit.com/chatGPT/\n#\nhttps://use-r.kr/\nhttps://www.meetup.com/seoul-r-meetup\nhttps://tidyverse-korea.github.io/seoul-R/\nhttps://r2bit.com/seoul-r/\nhttps://quarto.org/\n./contest.html\n./swc.html\n./learnr.html\nhttps://discord.gg/2etGFvfk\nhttps://github.com/bit2r\nhttps://www.facebook.com/groups/tidyverse\nhttps://www.youtube.com/channel/UCW-epmIvjBEhhVXw_F0Nqbw\nNone\nNone\nNone\n./about.html\n./coc.html\n./donation.html\n./records.html\nhttps://www.msit.go.kr/\n#챗gpt-출판\n#서울-r-미트업\n#공익법인-후원\n#회원가입-및-기부금-신청\nNone\nNone\nNone\nNone\nNone\nNone\nfig/chatGPT_series3.png\nfig/cover_ds.jpg\nfig/ds_logo.png\nhttps://bit.ly/4cVDxxL\nhttps://bit.ly/4dccQ8n\nhttps://bit.ly/4909pOB\nhttps://bit.ly/3wXugES\nfig/cover_coding.jpg\nfig/coding_logo.png\nhttps://bit.ly/4a8v1JS\nhttps://bit.ly/4auasHU\nhttps://bit.ly/48V8u1T\nhttps://bit.ly/48RqMB9\nfig/cover_quarto.jpg\nfig/quarto_logo.png\nhttps://bit.ly/3wElKuk\nhttps://bit.ly/3IkWE6f\nhttps://bit.ly/3P7PVjX\nhttps://bit.ly/3InK46m\nfig/sql_cover.jpg\nfig/sql_logo.png\nhttps://bit.ly/3OJmMeT\nhttps://bit.ly/48fujZD\nhttps://bit.ly/48gYn7d\nhttps://bit.ly/3wrgeuP\nfig/챗GPT_유닉스쉘.jpg\nfig/hex_chatGPT_unix_shell_02.png\nhttps://bit.ly/434h5y9\nhttps://bit.ly/3KzUdOU\nhttps://bit.ly/3I4FpWS\nhttps://bit.ly/49p0I1f\nhttps://omn.kr/25f0i\nhttps://www.meetup.com/seoul-r-meetup\nhttps://r-community.org/usergroups/\nhttps://r2bit.com/\nhttps://use-r.kr/\nhttps://r2bit.com/seoul-r\nhttps://www.youtube.com/channel/UCW-epmIvjBEhhVXw_F0Nqbw\nhttps://www.facebook.com/groups/tidyverse\nhttps://korea-r-user-group.slack.com\nfig/hex-stickers.png\nhttps://discord.gg/4APU9Y8j\nhttps://forms.gle/1gF8QezVPa8ur7617\nhttps://forms.gle/MEwLjasSaYj4a72k6\nmailto:admin@r2bit.com\nhttps://quarto.org/",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "API.html#api-사례",
    "href": "API.html#api-사례",
    "title": "",
    "section": "API 사례",
    "text": "API 사례\nAPI(“Application Programming Interface)는 서로 다른 소프트웨어 시스템 간에 상호작용하는 방법을 정의한 것으로 API를 통해 함수, 클래스, 프로토콜, 데이터 구조 등을 포함할 수 있다.\nAPI의 주요 목적은 소프트웨어 개발을 단순화하고 표준화하는 것으로, API를 사용하면 개발자가 특정 기능을 처음부터 새롭게 개발할 필요 없이 이미 개발된 기능을 재사용할 수 있어, API를 통해 서로 다른 시스템 간의 상호작용을 가능하게 하므로, 시스템 간의 통합을 단순화시킬 수 있다.\n웹 API는 일반적으로 HTTP를 통해 통신하며, 이를 통해 웹 서버에서 데이터를 요청하거나 웹 서버에 데이터를 보낼 수 있다. 이러한 웹 API를 통해 개발자는 웹 사이트, 애플리케이션, 서비스 등에서 다른 서비스의 기능이나 데이터를 활용할 수 있다. 예를 들어, 소셜 미디어 사이트의 API를 사용하면 개발자는 사용자가 해당 사이트에서 직접 게시물을 공유하거나 댓글을 작성하는 등의 기능을 자신의 애플리케이션에 통합할 수 있다. 이렇게 API를 사용하면 개발자는 기존의 플랫폼과 서비스에서 제공하는 기능과 데이터를 활용하여 새로운 애플리케이션을 더 빠르고 효과적으로 구축할 수 있다.\nOMDb API - The Open Movie Database에서 영화 API에 대한 다양한 정보를 얻을 수 있다.\n\n\nAPI 사양서\nAPI 시험\n직접 호출\nR API 호출\n파이썬 API 호출\n보안강화\n.env\n\n\n\nOMDb API - The Open Movie Database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n코드library(httr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n코드squid_game_req &lt;- glue::glue(\"http://www.omdbapi.com/\",\n           \"?i={Sys.getenv('OMDB_API_ID')}\",\n           \"&apikey={Sys.getenv('OMDB_API_KEY')}\",\n           \"&t=squid+game\")\n\nsquid_resp &lt;- GET(squid_game_req)\n\njsonlite::fromJSON(content(squid_resp, \"text\")) %&gt;% \n  enframe() %&gt;% \n  mutate(value = map_chr(value, 1))\n\n# A tibble: 22 × 2\n   name     value                                                               \n   &lt;chr&gt;    &lt;chr&gt;                                                               \n 1 Title    Squid Game                                                          \n 2 Year     2021–                                                               \n 3 Rated    TV-MA                                                               \n 4 Released 17 Sep 2021                                                         \n 5 Runtime  N/A                                                                 \n 6 Genre    Action, Drama, Mystery                                              \n 7 Director N/A                                                                 \n 8 Writer   N/A                                                                 \n 9 Actors   Lee Jung-jae, Park Hae-soo, Yasushi Iwaki                           \n10 Plot     Hundreds of cash-strapped players accept a strange invitation to co…\n# ℹ 12 more rows\n\n\n\n\nimport requests\n\nurl = 'http://www.omdbapi.com/?i=ttxxxxx&apikey=47xxxx&t=squid+game'\n\nr = requests.get(url)\nprint(r.text)\n\n{\"Title\":\"Squid Game\",\"Year\":\"2021–\",\"Rated\":\"TV-MA\",\"Released\":\"17 Sep 2021\",\"Runtime\":\"N/A\",\"Genre\":\"Action, Drama, Mystery\",\"Director\":\"N/A\",\"Writer\":\"N/A\",\"Actors\":\"Lee Jung-jae, Park Hae-soo, Yasushi Iwaki\",\"Plot\":\"Hundreds of cash-strapped players accept a strange invitation to compete in children's games. Inside, a tempting prize awaits with deadly high stakes. A survival game that has a whopping 45.6 billion-won prize at stake.\",\"Language\":\"Korean, English, Urdu\",\"Country\":\"South Korea\",\"Awards\":\"Won 6 Primetime Emmys. 46 wins & 80 nominations total\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BYWE3MDVkN2EtNjQ5MS00ZDQ4LTliNzYtMjc2YWMzMDEwMTA3XkEyXkFqcGdeQXVyMTEzMTI1Mjk3._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"8.0/10\"}],\"Metascore\":\"N/A\",\"imdbRating\":\"8.0\",\"imdbVotes\":\"482,476\",\"imdbID\":\"tt10919420\",\"Type\":\"series\",\"totalSeasons\":\"1\",\"Response\":\"True\"}\njson_data = r.json()\n\n# Print each key-value pair in json_data\nfor k in json_data.keys():\n    print(k + ': ', json_data[k])\n    \nTitle:  Squid Game\nYear:  2021–\nRated:  TV-MA\nReleased:  17 Sep 2021\nRuntime:  N/A\nGenre:  Action, Drama, Mystery\nDirector:  N/A\nWriter:  N/A\nActors:  Lee Jung-jae, Park Hae-soo, Yasushi Iwaki\nPlot:  Hundreds of cash-strapped players accept a strange invitation to compete in children's games. Inside, a tempting prize awaits with deadly high stakes. A survival game that has a whopping 45.6 billion-won prize at stake.\nLanguage:  Korean, English, Urdu\nCountry:  South Korea\nAwards:  Won 6 Primetime Emmys. 46 wins & 80 nominations total\nPoster:  https://m.media-amazon.com/images/M/MV5BYWE3MDVkN2EtNjQ5MS00ZDQ4LTliNzYtMjc2YWMzMDEwMTA3XkEyXkFqcGdeQXVyMTEzMTI1Mjk3._V1_SX300.jpg\nRatings:  [{'Source': 'Internet Movie Database', 'Value': '8.0/10'}]\nMetascore:  N/A\nimdbRating:  8.0\nimdbVotes:  482,476\nimdbID:  tt10919420\nType:  series\ntotalSeasons:  1\nResponse:  True\n\n\n\n코드import requests\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nTrue\n\n코드api_key = os.getenv('OMDB_API_KEY')\ni = os.getenv('OMDB_API_ID')\nurl = f'http://www.omdbapi.com/?i={i}&apikey={api_key}&t=squid+game'\n\nr = requests.get(url)\nprint(r.text)\n\n{\"Title\":\"Squid Game\",\"Year\":\"2021–\",\"Rated\":\"TV-MA\",\"Released\":\"17 Sep 2021\",\"Runtime\":\"N/A\",\"Genre\":\"Action, Drama, Mystery\",\"Director\":\"N/A\",\"Writer\":\"N/A\",\"Actors\":\"Lee Jung-jae, Park Hae-soo, Yasushi Iwaki\",\"Plot\":\"Hundreds of cash-strapped players accept a strange invitation to compete in children's games. Inside, a tempting prize awaits with deadly high stakes: a survival game that has a whopping 45.6 billion-won prize at stake.\",\"Language\":\"Korean, English, Urdu\",\"Country\":\"South Korea\",\"Awards\":\"Won 6 Primetime Emmys. 43 wins & 85 nominations total\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BYWE3MDVkN2EtNjQ5MS00ZDQ4LTliNzYtMjc2YWMzMDEwMTA3XkEyXkFqcGdeQXVyMTEzMTI1Mjk3._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"8.0/10\"}],\"Metascore\":\"N/A\",\"imdbRating\":\"8.0\",\"imdbVotes\":\"551,221\",\"imdbID\":\"tt10919420\",\"Type\":\"series\",\"totalSeasons\":\"2\",\"Response\":\"True\"}\n\n\n\n\nOMDB_API_KEY=47xxxx\nOMDB_API_ID=ttxxxxx",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "API.html#json-파일",
    "href": "API.html#json-파일",
    "title": "",
    "section": "JSON 파일",
    "text": "JSON 파일\nJSON(Javascript Object Notation, 자바스크립트 객체 표기법)은 가장 흔한 일반적인 웹 데이터 형식으로 데이터를 저장하거나 전송할 때 많이 사용되는 가벼운 자료형으로, 사람이 읽고 쓰기 쉬우며, 기계가 파싱하고 생성하기도 쉽다.\n\n언어 독립성: JSON은 JavaScript에서 파생되었지만, 거의 모든 프로그래밍 언어는 JSON 데이터를 파싱하는 라이브러리를 가지고 있고, 다양한 언어로 작성된 시스템 간에 데이터를 교환하는 데 아주 유용하게 작용한다.\n데이터 구조: JSON은 데이터를 구조화하는 방법도 제공하여 Nested JSON을 통해 중첩된 키-값 쌍을 통해 복잡한 데이터 구조를 표현할 수 있다.\n웹 표준: JSON은 웹 애플리케이션에서 서버와 클라이언트 사이의 데이터를 교환하는 표준으로 AJAX(Asynchronous JavaScript And XML)와 같은 기술이 이 형식을 기반으로 동작한다.\n읽기 쉬움: JSON 형식은 사람이 읽기 쉽기 때문에 디버깅을 도울 뿐만 아니라, 데이터를 보다 쉽게 이해하고 수정할 수 있다는 장점이 있다.\n\n\n코드library(tidyverse)\n\ndataframe_df &lt;- tribble(~\"Name\", ~\"Age\",\n                        \"Alice\", 25,\n                        \"Bob\", 30,\n                        \"Charlie\", 35)\n\ngt::gt(dataframe_df)\n\n\n\n\n\nName\nAge\n\n\n\nAlice\n25\n\n\nBob\n30\n\n\nCharlie\n35\n\n\n\n\n\n\n6가지 JSON 파일 표현방법\n\nrecord: 각 레코드가 사전인 목록.\ncolumn: 키가 열 레이블인 중첩 딕셔너리.\nsplit: 인덱스, 열, 데이터가 포함된 딕셔너리.\nindex: 키가 인덱스 레이블인 중첩 딕셔너리.\nvalues: 그냥 값 배열.\ntable: 스키마와 데이터를 포함하는 중첩된 딕셔너리(Apache Arrow 형식과 상호 운용성을 목표로 하는 더 복잡한 형식).\n\n\n\nrecord\ncolumn\nsplit\nindex\nvalues\ntable\n\n\n\n[\n    {\"Name\": \"Alice\", \"Age\": 25},\n    {\"Name\": \"Bob\", \"Age\": 30},\n    {\"Name\": \"Charlie\", \"Age\": 35}\n]\n\n\n{\n    \"Name\": {\"0\": \"Alice\", \"1\": \"Bob\", \"2\": \"Charlie\"},\n    \"Age\": {\"0\": 25, \"1\": 30, \"2\": 35}\n}\n\n\n{\n    \"columns\": [\"Name\", \"Age\"],\n    \"index\": [0, 1, 2],\n    \"data\": [[\"Alice\", 25], [\"Bob\", 30], [\"Charlie\", 35]]\n}\n\n\n{\n    \"0\": {\"Name\": \"Alice\", \"Age\": 25},\n    \"1\": {\"Name\": \"Bob\", \"Age\": 30},\n    \"2\": {\"Name\": \"Charlie\", \"Age\": 35}\n}\n\n\n[\n    [\"Alice\", 25],\n    [\"Bob\", 30],\n    [\"Charlie\", 35]\n]\n\n\n{\n    \"schema\": {\n        \"fields\": [\n            {\"name\": \"index\", \"type\": \"integer\"},\n            {\"name\": \"Name\", \"type\": \"string\"},\n            {\"name\": \"Age\", \"type\": \"integer\"}\n        ],\n        \"primaryKey\": [\"index\"],\n        \"pandas_version\": \"0.20.0\"\n    },\n    \"data\": [\n        {\"index\": 0, \"Name\": \"Alice\", \"Age\": 25},\n        {\"index\": 1, \"Name\": \"Bob\", \"Age\": 30},\n        {\"index\": 2, \"Name\": \"Charlie\", \"Age\": 35}\n    ]\n}\n\n\n\nJSON 불러오기\n다양한 형식으로 데이터프레임 파일을 저장했다면 다음 단계로 이를 파이썬에서 불러오는 방식은 json 패키지를 사용한다.\n\n코드import json\n\nwith open(\"data/json.json\") as json_file:\n    json_data = json.load(json_file)\n\nfor item in json_data:\n    if isinstance(item, dict):  # Check if the item is a dictionary\n        for k in item.keys():\n            print(k + ': ', item[k])\n\nName:  Alice\nAge:  25\nName:  Bob\nAge:  30\nName:  Charlie\nAge:  35\n\n\npandas 불러오기\n\n코드import pandas as pd\n\njson_pd = pd.read_json(\"data/json.json\")\n\nprint(json_pd.describe())\n\n        Age\ncount   3.0\nmean   30.0\nstd     5.0\nmin    25.0\n25%    27.5\n50%    30.0\n75%    32.5\nmax    35.0",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "API.html#인증",
    "href": "API.html#인증",
    "title": "",
    "section": "인증",
    "text": "인증\nAPI 인증 방법이 필요한 이유는 우선, 보안을 강화하여 허가받지 않은 접근을 차단하고, 사용자를 정확히 식별하여 맞춤 서비스를 제공한다. 또한 리소스 사용을 제어하고 모니터링하여 적절한 과금과 사용량 제한을 적용할 수 있으며, 데이터의 무결성을 보장한다. 법규 준수를 위해서도 중요하며, API 버전 관리와 지원에도 도움이 된다.\n주요 AI LLM 서비스들의 API 인증 방식을 살펴보면, OpenAI의 GPT 모델들과 Google Cloud의 Vertex AI는 API 키/토큰 방식을 사용하며, Hugging Face의 Inference API는 JWT를 사용하고, Microsoft의 Azure OpenAI Service는 Azure 생태계의 일부로 OAuth 2.0을 사용하고 있다.\n\n\n\n\n\n\n\n\n인증 방법\n장점\n단점\n대표적인 적용 사례\n\n\n\n기본 인증\n• 구현이 간단하고 빠름• 대부분의 HTTP 클라이언트에서 지원\n• 보안성이 낮음 (암호화되지 않은 채로 전송)• 자격 증명을 매 요청마다 전송해야 함• 세션 관리 기능 없음\n• Git 저장소 접근 (GitHub, GitLab)• 간단한 내부 API 시스템\n\n\nAPI 키/토큰\n• 구현이 비교적 간단• 사용자별 접근 제어 가능• 키 폐기 및 재발급이 용이\n• 키가 노출될 경우 보안 위험• 키 관리의 부담• 상세한 권한 제어가 어려움\n• Google Maps API• OpenWeatherMap API• Stripe 결제 API• OpenAI API (GPT-3, GPT-4)• Google Cloud Vertex AI API\n\n\nJWT\n• 서버 측 세션 저장소가 필요 없음• 다양한 정보를 토큰에 포함 가능• 마이크로서비스 아키텍처에 적합\n• 토큰 크기가 커질 수 있음• 한번 발급된 토큰의 정보 변경 불가• 토큰 탈취 시 보안 위험\n• Auth0 인증 서비스• Firebase Authentication• 많은 SPA(Single Page Application)• Hugging Face Inference API\n\n\nOAuth 2.0\n• 높은 보안성• 세부적인 권한 제어 가능• 사용자 데이터에 대한 접근 권한 위임 가능\n• 구현이 복잡함• 초기 설정에 시간이 소요됨• 작은 규모의 API에는 과도할 수 있음\n• Google API• Facebook 로그인• Twitter API• Microsoft Graph API• Azure OpenAI Service\n\n\n\n\n\n기본 인증(Basic Authentication): 사용자 이름과 비밀번호를 사용하는 가장 단순한 형태의 인증 방식으로, requests 라이브러리에서 다음과 같이 구현된다.\nrequests.get('http://api.example.com', auth=('username', 'password'))\n\n\nAPI 키/토큰 인증: 서버에서 발급한 고유한 키나 토큰을 사용하여 인증하는데 주로 두 가지 방식으로 구현된다.\n\n쿼리 파라미터로 전송 방식\n\nparams = {'access_token': 'your_api_key'}\nrequests.get('http://api.example.com', params=params)\n\nAuthorization 헤더로 전송 전송\n\nheaders = {'Authorization': 'Bearer your_api_key'}\nrequests.get('http://api.example.com', headers=headers)\n\nJWT(JSON Web Token) 인증: 클레임 정보를 JSON 객체로 안전하게 전송하는 방식으로 주로 Authorization 헤더에 포함되어 전송된다.\nOAuth 2.0: 사용자 동의를 바탕으로 제3자 애플리케이션의 리소스 접근을 관리하는 프로토콜로 구현 방식이 복잡하여 별도의 라이브러리를 사용하는 경우가 많다.",
    "crumbs": [
      "기본기",
      "API"
    ]
  },
  {
    "objectID": "positron.html",
    "href": "positron.html",
    "title": "",
    "section": "",
    "text": "기본기데이터 과학 편집기 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#인텔리센스",
    "href": "positron.html#인텔리센스",
    "title": "",
    "section": "인텔리센스",
    "text": "인텔리센스\n지능형 코딩 지원 기능인 인텔리센스(IntelliSense)는 VS Code 기능을 기반으로 데이터 과학 작업에 특화된 코드 작성을 돕는 기능이 포함되어 있으며 포지트론에서 지원되는 중요한 기능은 다음과 같다.\n\n코드 자동 완성\n\n변수, 함수, 클래스 이름 등을 입력하기 시작하면 관련 제안을 표시한다.\n라이브러리나 패키지의 함수들도 자동으로 제안한다.\n\n\n문법 강조\n\n코드의 구문에 따라 다양한 색상으로 강조 표시하여 가독성을 높인다.\n\n\n실시간 오류 검출\n\n코드를 입력하는 동안 문법 오류나 잠재적인 문제를 실시간으로 표시한다.\n\n\n함수 시그니처 도움말\n\n함수를 입력할 때 해당 함수의 매개변수 정보를 보여준다.\n\n\n정의로 이동\n\n변수나 함수의 정의된 위치로 쉽게 이동할 수 있다.\n\n\n리팩토링 도구\n\n변수 이름 변경, 코드 구조 개선 등의 리팩토링 작업을 지원한다.\n\n\n다양한 언어 지원\n\nR, 파이썬, SQL 등 다양한 언어에 대해 인텔리센스 기능을 제공한다.\n\n\n\n\n\n\n\n\n그림 2: 포지트론 인텔리센스",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#단축키",
    "href": "positron.html#단축키",
    "title": "",
    "section": "단축키",
    "text": "단축키\nR 코드 개발을 진행할 때  %&gt;% ,  ←  두가지 기능이 가장 많이 사용되는 단축키로 RStudio에서는 기본으로 지원되었다. 포지트론에서 자주 사용되는 단축키를  CTRL  +  Shift  +  m ,  Alt  +  -  를 적용시키는 방법을 살펴보자.\nPositron의 키보드 단축키는 몇 가지 예외를 제외하고 Visual Studio Code에서 사용되는 키보드 단축키를 그대로 사용할 수 있다. 다음 표는 Positron에 특별히 추가된 단축키를 번역했고 Positron 위키 Keyboard Shortcuts에서 원문을 확인할 수 있다.\n전역 단축키\n\n\n\n\n\n\n단축키\n설명\n\n\n\n\nCmd/Ctrl+Enter\n\n편집기에서 선택한 코드를 실행한다. 선택된 코드가 없으면 현재 문장을 실행한다.\n\n\n\nCmd/Ctrl+Shift+0\n\n콘솔에서 현재 열려있는 인터프리터를 재시작한다.\n\n\n\nCmd/Ctrl+Shift+Enter\n\n편집기에서 열린 파일을 실행한다(예: source() 또는 %run 사용).\n\n\nF1\n커서 아래의 주제에 대한 상황별 도움말을 표시한다.\n\n\n\nCmd/Ctrl+K, Cmd/Ctrl+R\n\n커서 아래의 주제에 대한 상황별 도움말을 표시한다(대체 바인딩).\n\n\n\nCmd/Ctrl+K, F\n\n콘솔에 초점을 맞춘다.\n\n\n\nCtrl+L\n\n콘솔을 지운다.\n\n\nR 단축키\n\n\n\n\n\n\n단축키\n설명\n\n\n\n\nCmd/Ctrl+Shift+M\n\n파이프 연산자(|&gt; 또는 %&gt;%)를 삽입한다.\n\n\n\nAlt+-\n\n할당 연산자(&lt;-)를 삽입한다.\n\n\n\nCmd/Ctrl+Shift+L\n\n현재 R 패키지가 있다면 로드한다.\n\n\n\nCmd/Ctrl+Shift+B\n\n현재 R 패키지가 있다면 빌드하고 설치한다.\n\n\n\nCmd/Ctrl+Shift+T\n\n현재 R 패키지가 있다면 테스트한다.\n\n\n\nCmd/Ctrl+Shift+E\n\n현재 R 패키지가 있다면 검사한다.\n\n\n\nCmd/Ctrl+Shift+D\n\n현재 R 패키지가 있다면 문서화한다.\n\n\nRStudio 키맵\nRStudio 키 바인딩을 사용하려면 다음 단계를 수행하면 다음 RStudio 키 매핑이 활성화된다.\n\nPositron 설정을 연다(Cmd+, 또는 Ctrl+,).\n“keymap”을 검색하거나 Extensions &gt; RStudio Keymap으로 이동한다.\n“Enable RStudio key mappings for Positron” 체크박스를 선택한다.\n\n\n\n\n\n\n\n단축키\n설명\n\n\n\n\nCtrl+1\n\n소스에 초점을 맞춘다.\n\n\n\nCtrl+2\n\n콘솔에 초점을 맞춘다.\n\n\n\nCmd/Ctrl+.\n\n심벌로 이동한다.\n\n\n\nCmd/Ctrl+Shift+C\n\n한 줄을 주석 처리하거나 주석 해제한다.\n\n\n\nCmd/Ctrl+Shift+N\n\n새 R 파일을 생성한다.\n\n\nF2\n정의로 이동한다.\n\n\n\nCmd/Ctrl+I\n\n선택 영역을 다시 들여쓰기한다.\n\n\n\nCmd/Ctrl+Shift+A\n\n선택 영역을 재포맷한다.\n\n\n\nCmd/Ctrl+Shift+S\n\n현재 R 스크립트를 소스로 실행한다.\n\n\n\nCmd/Ctrl+Alt+Shift+M\n\n이름을 변경한다.\n\n\n\nCmd/Ctrl+Alt+I\n\n새 Quarto/R Markdown 셀을 삽입한다.\n\n\n\nCmd/Ctrl+Alt+M\n\n버전 관리 창을 연다.\n\n\n\nCmd/Ctrl+Alt+Left\n\n이전 탭으로 이동한다.\n\n\n\nCmd/Ctrl+Alt+Right\n\n다음 탭으로 이동한다.\n\n\n\nCmd/Ctrl+D\n\n현재 줄을 삭제한다.\n\n\n\nCmd/Ctrl+Shift+M\n\n파이프 연산자를 삽입한다.\n\n\n\nCmd/Ctrl+Shift+R\n\n섹션을 삽입한다.\n\n\n\nAlt+Shift+K\n\n전역 키 바인딩 목록을 연다.\n\n\n\nAlt+-\n\n왼쪽 할당 연산자 &lt;-를 삽입한다.",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#위지윅-편집기",
    "href": "positron.html#위지윅-편집기",
    "title": "",
    "section": "위지윅 편집기",
    "text": "위지윅 편집기\nRStudio에서 쿼토, R마크다운, 마크다운 문서를 편집할 때 위지윅(WYSIWYG) 기능을 비주얼 편집기(Visual Editor) 기능을 통해서 제공하였다. 동일한 기능은 쿼토(Quart) 확장 기능을 설치하게 되면 포함되어 있다. 수식이나 도형 등 기본 미리보기 기능은 내장되어 제공되지만 위지윅 편집 기능은 Cmd/Ctrl+Shift+F4 단축키를 눌러 위지윅 모드, 텍스트 편집 모드 사이를 자유로이 오갈 수 있다.\n위지윅 편집기능을 통해서 문서에 적정한 이미지 크기 조절 및 정렬을 비롯하여 참고문헌 등 다양한 용도로 활용하여 문서제작 생산성을 획기적으로 높일 수 있다.\n\n\n\n\n\n포지트론 위지윅 편집기능",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#코딩-글꼴",
    "href": "positron.html#코딩-글꼴",
    "title": "",
    "section": "코딩 글꼴",
    "text": "코딩 글꼴\n다른 언어와 마찬가지로 R 코드로 데이터 과학 제품을 개발할 경우 글꼴도 코딩에 적합한 한글 글꼴을 설정한다. 먼저 D2 Coding 글꼴을 다운로드하여 운영체제에 설치한다.\n포지트론/VS코드 좌측 하단 톱니바퀴  Settings  설정을 클릭 혹은 메뉴에서 “File” → “Preferences” → “Settings”를 통해 편집기 (Text Editor)로 들어가 운영체제에 설치한 코딩 폰트를 지정한다. Font Ligatures 도 true로 설정한다. 이를 통해 &lt; - 표시가 &larra; 로 화면에 표현된다.\n\n\n\n\n\nD2코딩 글꼴 장착\n\nsettings.json 설정파일에 Font Family, Font Size, Font Ligature를 설정하는 방식도 있다.\n{\n    \"workbench.colorTheme\": \"Default Dark Modern\",\n    \"editor.fontFamily\": \"'D2Coding ligature', D2Coding, monospace\",\n    \"editor.fontSize\": 15,\n    \"editor.fontLigatures\": true\n}",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#테마",
    "href": "positron.html#테마",
    "title": "",
    "section": "테마",
    "text": "테마\nVS Code Themes 웹사이트에서 Visual Studio Code의 테마를 검색하고 발견할 수 있는 웹사이트로 다양한 테마를 설치할 수 있고, 평점 등으로 정렬하여 쉽게 찾을 수 있도록 돕고 있으며, 어두운 테마와 밝은 테마 등 여러 카테고리로 나뉘어져 있다.\nCommand Palette (CTRL + SHIFT + P) → CTRL + K CTRL + T 단축키를 통해 테마를 변경할 수 있다.\nOpen VSX 레지스트리는 Visual Studio Code 확장을 위한 오픈 소스 마켓플레이스 플랫폼으로 커뮤니티 주도로 운영되며, 개발자들이 확장을 게시하고 발견하며 설치할 수 있게 한다. 독립적으로 운영되어 독점적인 확장 마켓플레이스에 대한 대안으로 각광받고 있다.\n\n\n\n\n\nPositron 테마 변경과정",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#프로그래밍-엔진",
    "href": "positron.html#프로그래밍-엔진",
    "title": "",
    "section": "프로그래밍 엔진",
    "text": "프로그래밍 엔진\nR도 동일하지만, 특히 파이썬 버전을 달리하여 설치하고 아나콘다, 가상환경 등 다양한 파이썬 버전을 설치할 경우 경우에 포지트론에서 잡히지 않는 경우가 있다. 이와 같은 문제가 있는 경우 Cmd/Ctrl+Shift+P “명령 팔레트” (Command Palette)를 실행하고 나서 Devloper: Reload Window 명령어를 실행하게 되면 일렉트론(electron)으로 제작된 윈도우가 재실행되어 설치된 파이썬 버전을 찾을 수 있다.\n\n\n\n\n\n그림 3: 파이썬 버전 활성화",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#맞춤법-검사",
    "href": "positron.html#맞춤법-검사",
    "title": "",
    "section": "맞춤법 검사",
    "text": "맞춤법 검사\n비주얼 스튜디오 코드 한스펠은 개인이나 학생에게는 무료인 맞춤법 VS 코드 맞춤법 기능을 제공하는 확장프로그램이다. 아마도 당분간 포지트론에서 확장기능을 제공하지 않을 것으로 보인다. 따라서, vscode-hanspell 확장기능을 포지트론에서 사용하기 위해서 .vsix 파일을 소스코드를 컴파일해서 Extenstions → ... → Install from VSIX...을 통해 로컬 파일형태로 설치한다. 소스코드를 컴파일하지 않고 확장프로그램만 사용하실 분은 다음 .vsix 파일을 설치해서 사용하면 된다.\n .vsix 확장프로그램 \n\n\n\n\n\n\n\n\n\n\n\n그림 4: vscode-hanspell 확장프로그램\n\n\n\n\n\n\n\n\n\n그림 5: 맞춤법 선택\n\n\n\n\n\n\n\n\n\n그림 6: VS 코드 맞춤법 검사\n\n\n\n\n\n\n.vsix 컴파일 과정\n윈도우 환경에서 GitHub 저장소에 소스 코드만 있는 경우, 확장 프로그램을 직접 빌드하고 설치해야 한다.\n\n먼저, Node.js가 컴퓨터에 설치되어 있어야 하고, 설치되어 있지 않다면 Node.js 웹사이트에서 다운로드하여 설치한다.\n터미널(명령 프롬프트)을 연다.\n\nGitHub 저장소를 클론한다.\ngit clone https://github.com/9beach/vscode-hanspell.git\n\n\n클론한 디렉토리로 이동한다.\ncd vscode-hanspell\n\n\n필요한 의존성을 설치한다.\nnpm install\n\nvsce 를 확인한다. npm config get prefix     /c/Users/YourUsername/AppData/Roaming/npm/vsce.cmd --version\n\n확장 프로그램을 빌드한다.\n/c/Users/YourUsername/AppData/Roaming/npm/vsce.cmd package  \nvsce.cmd package 명령어는 .vsix 파일을 생성한다.\n\n포짓트론을 실행한다.\n포짓트론에서 확장 마켓플레이스(Ctrl+Shift+X)를 열고, 상단의 “…” 메뉴를 클릭한다.\n“Install from VSIX…”를 선택하고 방금 생성한 .vsix 파일을 선택한다.\n설치가 완료되면 포지트론을 재시작한다.\n사용자 정의 설정\n~/.hanspell-bad-expressions.json, ~/.hanspell-ignore을 적용한다. 자세한 사항은 비주얼 스튜디오 코드 한스펠 설정을 참고한다.",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#문서-제작",
    "href": "positron.html#문서-제작",
    "title": "",
    "section": "문서 제작",
    "text": "문서 제작\n쿼토 문서\n쿼토(Quarto) 확장프로그램을 설치하게 되면 New File...에서 Quarto Document와 Quarto Project를 선택하여 쿼토 문서를 제작할 수 있다.\n\n\n\n\n\nPositron 쿼토 문서 작성\n\nPDF 보기\n포지트론에서 PDF 파일을 직접 열어보기 위해서는 vscode-pdf 확장프로그램을 설치하면 별도 프로그램(아도브 애크로뱃 등) 없이 PDF 파일을 바로 열 수 있다.\n\n\n\n\n\n그림 7: PDF 확장프로그램 - vscode-pdf\n\n\nR/파이썬\nquarto-webr, pyodide 쿼토 확장프로그램을 설치하면 R, 파이썬 콘솔을 쿼토 문서에서 직접 프로그래밍 할 수 있다.\nquarto add coatless/quarto-webr\nquarto add coatless-quarto/pyodide\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n샤이니 앱\nshiny 확장프로그램을 설치하면 포지트론에서 웹앱을 제작할 수 있다.\nPosit(이전의 RStudio)에서 Shiny 앱을 실행하는 단축키는 다음과 같다.\nCtrl+Shift+Enter (Windows/Linux) 또는 Cmd+Shift+Enter (Mac)\n추가로 알아두면 좋은 관련 단축키들은 다음과 같다.\n\n앱 중지: Esc 키\n앱 새로고침 (코드 변경 후): Ctrl+Enter (Windows/Linux) 또는 Cmd+Enter (Mac)\nRun App 버튼 클릭: Ctrl+Shift+Enter (Windows/Linux) 또는 Cmd+Shift+Enter (Mac)\n\n\n\n\n\n\n\nshiny app을 포지트론에서 실행할 때 오류\n\n\n\n\nCould not find R. Is R installed on your system?If R is installed, please make sure your PATH environment variable is configured correctly.\n\n환경설정에서 설치된 R 경로를 추가한다.\n\n\n\n\n\n\n\n\n\n\n\nshiny 확장 프로그램\n\n\n\n\n\nshiny 앱 실행\n\n\n\n\n\n그림 8\n\n\nshinylive\nshinylive 패키지를 r-shinylive에서 설치하고 쿼토 확장프로그램도 설치하면 개발한 shiny 앱을 문서를 포함한 다양한 곳에 삽입할 수 있다.\nquarto add quarto-ext/shinylive\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Load the Old Faithful dataset\ndata(faithful)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bins\",\n                  \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    ggplot(faithful, aes(x = waiting)) +\n      geom_histogram(bins = input$bins, fill = \"steelblue\", color = \"white\") +\n      labs(title = \"Histogram of waiting times\",\n           x = \"Waiting time to next eruption (in mins)\",\n           y = \"Frequency\") +\n      theme_minimal() +\n      theme(plot.title = element_text(hjust = 0.5))\n  })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#r-패키지-저장소",
    "href": "positron.html#r-패키지-저장소",
    "title": "",
    "section": "R 패키지 저장소",
    "text": "R 패키지 저장소\n‘C:/Program Files/R/R-4.4.1/library’ 디렉터리에 패키지를 설치할 수 있도록 환경을 설정하는 방법은 다음과 같다. 시스템 관리자 권한이 필요하며, 윈도즈 시스템 설정을 변경해야 한다.\n\n먼저 R을 관리자 권한으로 실행한다.\n\nR 아이콘에 우클릭하고 “관리자 권한으로 실행”을 선택한다.\n\n\nR 콘솔에서 현재 라이브러리 경로를 확인한다.\n\n\n코드.libPaths()\n\n[1] \"C:/Users/statkclee/AppData/Local/R/win-library/4.4\"\n[2] \"C:/Program Files/R/R-4.4.1/library\"                \n\n\n\n\n시스템 환경 변수를 설정한다.\n\nWindows 검색창에 “시스템 환경 변수 편집”을 입력하고 실행한다.\n“환경 변수” 버튼을 클릭한다.\n“시스템 변수” 섹션에서 “새로 만들기”를 클릭한다.\n변수 이름에 “R_LIBS”를 입력한다.\n변수 값에 “C:/Program Files/R/R-4.4.1/library”를 입력한다.\n“확인”을 눌러 저장한다.\n\n\nR을 재시작한다.\n다시 R을 관리자 권한으로 실행하고, 라이브러리 경로를 확인한다.\n\n\n코드.libPaths()\n\n[1] \"C:/Users/statkclee/AppData/Local/R/win-library/4.4\"\n[2] \"C:/Program Files/R/R-4.4.1/library\"                \n\n\n\n이제 패키지 설치를 시도한다.\n\ninstall.packages(\"remotes\")\n그럼에도 불구하고 패키지를 시스템 환경에 저장할 수 없는 경우, R_LIBS_USER을 사용해서 패키지 저장소를 설정한다.\n\n\n\n\n\n\n사용자 R 패키지 저장소\n\n\n\nR_LIBS_USER 사용자 R 패키지 저장소를 .libPaths() 설정하고 패키지 저장소로 활용한다.\n\n코드# 사용자 라이브러리 디렉토리 생성\ndir.create(path = Sys.getenv(\"R_LIBS_USER\"), showWarnings = FALSE, recursive = TRUE)\n\n# .libPaths()에 사용자 라이브러리 추가\n.libPaths(Sys.getenv(\"R_LIBS_USER\"))\n\n# 패키지 설치 시도\ninstall.packages(\"remotes\")",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#프로젝트-관리자",
    "href": "positron.html#프로젝트-관리자",
    "title": "",
    "section": "프로젝트 관리자",
    "text": "프로젝트 관리자\n프로젝트를 넘나들며 작업하기 위해서 Project Manager 확장 프로그램을 설치한다.",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#데이터-탐색기",
    "href": "positron.html#데이터-탐색기",
    "title": "",
    "section": "데이터 탐색기",
    "text": "데이터 탐색기\n데이터 탐색기(Data Explorer)는 코드 우선 데이터 탐색을 보완하기 위한 도구다. 스프레드시트와 유사한 그리드에 데이터를 표시하고, 데이터 필터링 및 정렬을 지원하여 Positron 내에서 직접 요약 통계를 제공한다. 데이터 탐색기 목표는 코드 기반 워크플로우를 대체하는 것이 아니라, 코드를 통해 데이터를 추가로 탐색하거나 수정할 때 한번 쓰고 버리는 데이터 뷰나 요약 통계를 제공하여 보완하는 것이다. Positron 위키 Data Explorer을 번역한 것으로 Data Explorer에서 원문을 확인할 수 있다.\n데이터 탐색기는 다음과 같은 세 가지 주요 구성 요소를 가지고 있다.\n\n데이터 그리드: 개별 셀과 열의 스프레드시트 형태 표시 및 정렬 기능\n요약 패널: 각 열의 열 이름, 유형 및 결측 데이터 비율\n필터 바: 특정 열에 대한 일시적 필터\n\n\n데이터프레임 열기\n각 데이터 탐색기 인스턴스는 언어 런타임에 의해 구동되며 Python(pandas) 또는 R(data.frame, tibble, data.table)의 데이터프레임을 표시할 수 있다. 또한 polars에 대한 실험적 지원도 제공하며, 향후 추가적인 Python 데이터프레임 라이브러리가 추가될 예정이다.\n데이터 탐색기의 각 인스턴스는 기본 데이터의 변경 사항에 따라 새로 고쳐진다. 이를 통해 UI 중심의 데이터 탐색기와 코드 우선 접근 방식이 결합된 워크플로우가 가능하다.\n특정 데이터프레임에 대한 새 데이터 탐색기 인스턴스를 열려면 다음 방법 중 하나를 사용한다:\n\n언어 런타임을 직접 사용:\n\nPython을 통해: %view dataframe label\n\nR을 통해: View(dataframe, \"label\")\n\n\n\n변수 창으로 이동하여 특정 데이터프레임 객체에 대한 데이터 탐색기 아이콘을 클릭\n\n\n데이터 그리드\n데이터 그리드는 주요 표시 영역으로, 스프레드시트와 유사한 셀별 뷰를 제공한다. 수백만 행이나 열까지의 비교적 큰 인메모리 데이터셋을 효율적으로 처리하도록 설계되었다. 각 열 헤더에는 열 이름과 함께 언어 런타임에서 사용되는 데이터 유형이 표시된다. 각 열의 오른쪽 상단에 있는 문맥 메뉴를 통해 정렬을 제어하거나 선택한 열에 대한 필터를 빠르게 추가할 수 있다. 열 경계를 클릭하고 드래그하여 열의 크기를 조정할 수 있다.\n\n행 레이블은 기본적으로 관찰된 행 인덱스를 사용하며, Python에서는 0부터 시작하는 인덱스를, R에서는 1부터 시작하는 인덱스를 사용한다. 또는 pandas와 R 사용자는 수정된 인덱스나 문자열 기반 레이블이 있는 행을 가질 수 있다.\n요약 패널\n요약 패널은 모든 열 이름과 해당 유형을 나타내는 아이콘을 세로로 스크롤되는 목록으로 표시한다. 또한 결측 데이터의 양을 증가하는 백분율과 인라인 막대 그래프로 표시한다.\n\n열 이름을 더블 클릭하면 데이터 그리드에서 해당 열에 초점을 맞추어 더 넓은 데이터를 빠르게 탐색할 수 있다. 요약 패널은 레이아웃 컨트롤을 통해 데이터 탐색기의 왼쪽이나 오른쪽에 배치하거나 일시적으로 숨길 수 있다.\n필터 바\n필터 바에는 기존 필터를 표시, 숨기거나 제거하는 컨트롤과 새 필터를 추가하는 + 버튼이 있다. 데이터 탐색기 하단의 상태 표시줄에는 필터 적용 후 남은 행의 백분율과 수가 표시된다. 새 필터를 만들 때는 전체 목록을 스크롤하거나 특정 문자열로 열을 검색하여 열을 선택해야 한다. 열을 선택하면 해당 열 유형에 사용할 수 있는 필터가 표시된다. 또는 데이터 그리드의 각 열 레이블에 있는 콘텍스트 메뉴를 통해 열 이름이 미리 채워진 필터를 만들 수 있다.\n\n사용할 수 있는 필터는 열 유형에 따라 다르다. 예를 들어, 문자열 열에는 다음과 같은 필터 옵션이 있다: 포함, 시작 또는 끝남, 비어 있음, 정확히 일치. 반면 숫자 열에는 다음과 같은 논리 연산이 있다: 미만 또는 초과, 같음, 또는 두 값 사이(포함).\n\n코드# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nView(penguins)",
    "crumbs": [
      "기본기",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "ollama.html",
    "href": "ollama.html",
    "title": "올라마",
    "section": "",
    "text": "올라마(ollama) 웹사이트에서 운영체제에 맞춰 다운로드 받아 설치한다. 만약 별도 설정하지 않게 되면 C:\\Users\\&lt;사용자명&gt;\\.ollama 디렉토리에 LLM 이 저장된다.\n코드\nfrom langchain_community.llms import Ollama\nllm = Ollama(model=\"gemma2\")\nllm.invoke(\"축구를 가장 잘하는 나라는 어디야?\")",
    "crumbs": [
      "데이터베이스",
      "올라마"
    ]
  },
  {
    "objectID": "gh_action.html",
    "href": "gh_action.html",
    "title": "",
    "section": "",
    "text": "자동화깃헙 액션 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "자동화",
      "깃헙 액션"
    ]
  },
  {
    "objectID": "gh_action.html#주식가격",
    "href": "gh_action.html#주식가격",
    "title": "",
    "section": "주식가격",
    "text": "주식가격\n네이버 금융 크롤링 파이썬 코드를 참고하여 챗GPT로 코드를 동작하는 코드를 생성한다.\n\n코드import requests\nfrom bs4 import BeautifulSoup\n\ndef get_stock_info(stock_code):\n    url = f\"https://finance.naver.com/item/main.nhn?code={stock_code}\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Attempt to extract the stock name with a more robust approach\n    company_info = soup.find(\"div\", {\"class\":\"h_company\"})\n    stock_name = company_info.find(\"a\").text\n\n    # Find the element containing the current price\n    current_price_container = soup.find(\"p\", {\"class\": \"no_today\"})\n    current_price = current_price_container.find(\"span\", {\"class\": \"blind\"}).get_text() if current_price_container else \"Price not found\"\n\n    return stock_name, current_price\n\n# Example usage\nstock_code = '005930'  # Samsung Electronics code\nstock_name, current_price = get_stock_info(stock_code)\nprint(f\"{stock_name} (code {stock_code}) 현재가격: {current_price} KRW.\")\n\n삼성전자 (code 005930) 현재가격: 74,900 KRW.\n\n\n\nR 코드로 변환\n프롬프트: 다음 코드를 R 코드로 변환해줘\n\n인코딩 이슈가 있어 read_html()에서 다음과 같이 변환한다.\n\n코드library(rvest)\n\nWarning: package 'rvest' was built under R version 4.3.3\n\n코드get_stock_info &lt;- function(stock_code) {\n  url &lt;- sprintf(\"https://finance.naver.com/item/main.nhn?code=%s\", stock_code)\n  webpage &lt;- read_html(url,  encoding = \"euc-kr\")  # Setting the encoding to EUC-KR\n  \n  # Attempt to extract the stock name with a more robust approach\n  company_info &lt;- html_node(webpage, \"div.h_company\")\n  stock_name &lt;- html_text(html_node(company_info, \"a\"))\n\n  # Find the element containing the current price\n  current_price_container &lt;- html_node(webpage, \"p.no_today\")\n  if (!is.null(current_price_container)) {\n    current_price &lt;- html_text(html_node(current_price_container, \"span.blind\"))\n  } else {\n    current_price &lt;- \"Price not found\"\n  }\n  \n  list(stock_name = stock_name, current_price = current_price)\n}\n\n# Example usage\nstock_code &lt;- '005930'  # Samsung Electronics code\ninfo &lt;- get_stock_info(stock_code)\ncat(sprintf(\"%s (code %s) 현재가격: %s KRW.\\n\", info$stock_name, stock_code, info$current_price))\n\n삼성전자 (code 005930) 현재가격: 74,900 KRW.",
    "crumbs": [
      "자동화",
      "깃헙 액션"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "",
    "section": "",
    "text": "참고문헌참고문헌 코드\n\n\n\n\n\n참고문헌",
    "crumbs": [
      "참고문헌",
      "참고문헌"
    ]
  },
  {
    "objectID": "gcs.html",
    "href": "gcs.html",
    "title": "",
    "section": "",
    "text": "자동화버킷 코드",
    "crumbs": [
      "자동화",
      "버킷"
    ]
  },
  {
    "objectID": "gcs.html#버킷-생성",
    "href": "gcs.html#버킷-생성",
    "title": "",
    "section": "버킷 생성",
    "text": "버킷 생성\n\nGCS\n\nCloud Console 열기: Google Cloud Console로 이동합니다.\n프로젝트 선택: 버킷을 만들 프로젝트를 선택합니다.\nCloud Storage로 이동: 네비게이션 메뉴에서 “Cloud Storage” &gt; “Browser”를 선택합니다.\n버킷 생성:\n\n“Create bucket” 버튼을 클릭합니다.\n버킷 이름을 입력합니다. 버킷 이름은 전 세계에서 유일해야 합니다.\n버킷 위치를 선택합니다. 필요에 따라 특정 지역, 다중 지역 또는 이중 지역을 선택할 수 있습니다.\n기본 스토리지 클래스를 선택합니다. 이는 저장된 데이터의 비용 및 가용성에 영향을 미칩니다.\n접근 제어를 설정합니다. “Uniform” 또는 “Fine-grained” 접근 제어를 선택할 수 있습니다.\n필요한 추가 설정을 구성합니다.\n“Create”를 클릭하여 버킷을 만듭니다.\n\n\n\n\ngsutil 도구\nGoogle Cloud SDK에 포함된 gsutil은 Cloud Storage와 상호작용할 수 있는 명령어 도구다.\n\nGoogle Cloud SDK 설치:\n\nGoogle Cloud SDK 설치 페이지의 지침에 따라 Google Cloud SDK를 설치합니다.\n\nSDK 초기화:\ngcloud init\nGCS 버킷 만들기:\ngsutil mb -p [PROJECT_ID] -l [LOCATION] gs://[BUCKET_NAME]/\n\n[PROJECT_ID]를 Google Cloud 프로젝트 ID로 바꿉니다.\n[LOCATION]을 버킷 위치로 바꿉니다 (예: us-central1, us, EU).\n[BUCKET_NAME]을 고유한 버킷 이름으로 바꿉니다.\n\n예제: sh  gsutil mb -p alookso-id -l asia-northeast3 gs://my-buckets-name/  gstutil ls\n\n\n\n파이썬과 GC 클라이언트\nPython을 사용하여 Google Cloud 클라이언트 라이브러리를 통해 프로그래밍 방식으로 GCS 버킷을 만들 수도 있습니다.\n\nGoogle Cloud Storage 클라이언트 라이브러리 설치:\npip install google-cloud-storage\nPython을 사용하여 버킷 생성:\nfrom google.cloud import storage\n\n# GCS 클라이언트 초기화\nclient = storage.Client()\n\n# 버킷 이름과 위치 정의\nbucket_name = 'your-unique-bucket-name'\nlocation = 'US'  # 또는 'us-central1'과 같은 지역 지정\n\n# 버킷 생성\nbucket = client.bucket(bucket_name)\nnew_bucket = client.create_bucket(bucket, location=location)\n\nprint(f'Bucket {new_bucket.name} created.')\n\n\n\n예제\n아래는 Python을 사용하여 GCS 버킷을 생성하는 전체 예제입니다:\nimport os\nfrom google.cloud import storage\n\n# 인증 설정\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your-service-account-file.json'\n\n# GCS 클라이언트 초기화\nclient = storage.Client()\n\n# 버킷 이름과 위치 정의\nbucket_name = 'your-unique-bucket-name'\nlocation = 'US'  # 또는 'us-central1'과 같은 지역 지정\n\n# 버킷 생성\nbucket = client.bucket(bucket_name)\nnew_bucket = client.create_bucket(bucket, location=location)\n\nprint(f'Bucket {new_bucket.name} created.')",
    "crumbs": [
      "자동화",
      "버킷"
    ]
  },
  {
    "objectID": "gcs.html#버킷-권한",
    "href": "gcs.html#버킷-권한",
    "title": "",
    "section": "버킷 권한",
    "text": "버킷 권한\n서비스 계정에 적절한 권한을 부여해야 합니다. 다음 단계를 따라 권한을 부여할 수 있습니다:\n\nGoogle Cloud Console에 접속합니다.\n프로젝트를 선택합니다.\n왼쪽 메뉴에서 “Storage” 메뉴를 선택합니다.\n버킷명 버킷을 찾아 클릭합니다.\n상단 메뉴에서 “권한” 탭을 클릭합니다.\n“구성원 추가” 버튼을 클릭합니다.\n“새 구성원” 입력란에 youtube-analytics-api@버킷명.iam.gserviceaccount.com 서비스 계정 이메일을 입력합니다.\n역할 선택란에서 “Storage 객체 관리자” 또는 “Storage 객체 생성자” 역할을 선택합니다. 이 역할은 버킷 내의 객체에 대한 읽기/쓰기 권한을 부여합니다.\n“저장” 버튼을 클릭하여 권한을 저장합니다.",
    "crumbs": [
      "자동화",
      "버킷"
    ]
  },
  {
    "objectID": "code_interpreter.html",
    "href": "code_interpreter.html",
    "title": "",
    "section": "",
    "text": "AI 코딩데이터 사이언스 코드",
    "crumbs": [
      "AI 코딩",
      "데이터 사이언스"
    ]
  },
  {
    "objectID": "code_interpreter.html#프롬프트",
    "href": "code_interpreter.html#프롬프트",
    "title": "",
    "section": "프롬프트",
    "text": "프롬프트\n\n데이터 전처리EDA통계모형기계학습성능 최적화시각화분석도구대쉬보드파이프라인모듈 개발\n\n\n\nPrompt: What are the best practices for preprocessing {topic} data using {programming_language_or_framework}?\n\n\nData Cleaning: Identify and address missing values, outliers, and duplicate records. Cleaning your data ensures that you’re working with accurate and reliable information.\nData Transformation: Normalize or standardize numerical features, encode categorical variables, and create new features if necessary. Data transformation enhances the quality of your dataset.\nFeature Engineering: Extract meaningful information from your data to improve the performance of machine learning models. Feature engineering involves creating new features or modifying existing ones to make them more informative.\nData Validation: Ensure data consistency and integrity by performing validation checks. Validate that your data adheres to predefined rules and constraints.\n\n\n\n\nPrompt: How can I perform exploratory data analysis on {topic} data using {programming_language_or_framework}?\n\n\nData Visualization: Create informative plots, charts, histograms, and scatterplots to visualize data distributions, relationships, and trends. Visualization helps in gaining initial insights into the data.\nStatistical Analysis: Compute summary statistics, such as mean, median, and standard deviation, to describe the central tendencies and variability of your data. Statistical tests can reveal relationships and dependencies.\nHypothesis Testing: Formulate hypotheses about your data and conduct statistical tests to validate or reject these hypotheses. Hypothesis testing is useful for making data-driven decisions.\nInteractive Exploration: Leverage libraries and tools available in {programming_language_or_framework} to perform interactive exploration. Interactive visualization and widgets allow for dynamic exploration of the data.\n\n\n\n\nPrompt: What are the most common statistical techniques to analyze {topic} data in {programming_language_or_framework}?\n\n\nRegression Analysis: Use regression techniques to model relationships between variables and make predictions. Linear regression, logistic regression, and polynomial regression are common types.\nClustering: Apply clustering algorithms, such as K-means or hierarchical clustering, to group similar data points together. Clustering helps in segmentation and pattern recognition.\nClassification: Perform classification tasks to categorize data into predefined classes or labels. Decision trees, support vector machines, and neural networks are frequently used for classification.\nTime Series Analysis: Analyze data with temporal components using time series analysis. This technique is essential for understanding trends and patterns over time.\n\n\n\n\nPrompt: Provide a step-by-step guide for implementing a machine learning model for {specific_task} using {programming_language_or_framework}.\n\n\nData Preparation: Begin by preprocessing and cleaning your data, ensuring that it’s in the right format for modeling.\nFeature Selection: Identify and select the most relevant features or variables for your model. Feature selection helps improve model performance and reduce complexity.\nModel Selection: Choose an appropriate machine learning algorithm or model for your task. Consider factors like data size, complexity, and interpretability.\nTraining and Evaluation: Train your chosen model on a portion of your data and evaluate its performance using metrics like accuracy, precision, recall, and F1-score.\nDeployment: If the model performs satisfactorily, deploy it in your application or workflow for making predictions.\n\n\n\n\nPrompt: Explain how to optimize {topic} data analysis performance in {programming_language_or_framework} using best coding practices.\n\n\nVectorization: Take advantage of vectorized operations to perform calculations on entire arrays or datasets, which can significantly speed up computations.\nMemory Management: Efficiently manage memory resources, such as by releasing unnecessary objects or using data structures that minimize memory usage.\nParallel Processing: Utilize parallel computing techniques to distribute tasks across multiple cores or processors, thereby accelerating data processing.\nProfiling and Testing: Regularly profile your code to identify performance bottlenecks and optimize critical sections. Thoroughly test your code to ensure correctness and reliability.\n\n\n\n\nPrompt: Discuss the pros and cons of different data visualization techniques for {topic} data analysis in {programming_language_or_framework}.\n\n\nBar Charts and Histograms: These are effective for showing data distributions and comparing categories. They are excellent for visualizing frequency and count data.\nScatterplots: Ideal for displaying relationships between two continuous variables. Scatterplots help identify correlations and trends in data.\nHeatmaps: Useful for visualizing large datasets and identifying patterns in multidimensional data. They are especially valuable for displaying correlation matrices.\nInteractive Dashboards: Create user-friendly interactive dashboards that allow users to explore and interact with data. Dashboards can provide real-time insights and support decision-making.\n\n\n\n\nPrompt: Describe the process of building a custom data analysis tool for {topic} using {programming_language_or_framework}, including the necessary features and functionalities.\n\n\nData Import: Allow users to import data from various sources, such as CSV files, databases, or APIs.\nData Processing: Include preprocessing and transformation capabilities, enabling users to clean, filter, and manipulate data easily.\nVisualization: Incorporate interactive visualization components that help users explore and understand the data visually.\nExport and Reporting: Provide options for exporting analysis results, generating reports, and sharing findings with stakeholders.\n\n\n\n\nPrompt: Explain how to develop a user-friendly dashboard for visualizing and interacting with {topic} data analysis results using {programming_language_or_framework}.\n\n\nIntuitive Design: Create a visually appealing and intuitive design that makes it easy for users to navigate and understand the dashboard.\nInteractive Elements: Incorporate interactive elements, such as filters, sliders, and dropdowns, that allow users to customize their data views.\nReal-Time Updates: Enable real-time updates of data and visualizations to provide users with the latest information.\nAccessibility: Ensure that the dashboard is accessible to all users, including those with disabilities, by following accessibility guidelines.\n\n\n\n\nPrompt: Provide a step-by-step guide for creating a reusable data analysis pipeline for {topic} using {programming_language_or_framework}, covering data preprocessing, analysis, and visualization.\n\n\nData Ingestion: Develop a module for loading data from various sources, including files, databases, and APIs.\nPreprocessing: Create a preprocessing module that encompasses data cleaning, transformation, and feature engineering steps.\nAnalysis: Develop analysis modules that encapsulate statistical analyses, machine learning models, and hypothesis tests.\nVisualization: Implement visualization modules that generate informative charts and graphs for insights.\n\n\n\n\nPrompt: Discuss the key considerations when designing a scalable and modular data analysis tool for {topic} in {programming_language_or_framework}, including performance optimization and extensibility.\n\n\nPerformance Optimization: Optimize your code and algorithms for scalability to ensure that the tool can handle large datasets efficiently.\nModular Architecture: Design the tool with a modular architecture, making it easier to add new features, update existing ones, and maintain the codebase.\nExtensibility: Allow for the easy integration of additional data sources, analysis methods, and visualization techniques to accommodate changing needs.\nUser Collaboration: Implement features that enable collaboration among multiple users, such as data sharing, version control, and user permissions.",
    "crumbs": [
      "AI 코딩",
      "데이터 사이언스"
    ]
  },
  {
    "objectID": "code_interpreter.html#llm-데이터-분석",
    "href": "code_interpreter.html#llm-데이터-분석",
    "title": "",
    "section": "LLM 데이터 분석",
    "text": "LLM 데이터 분석\n대형 언어 모델(LLMs)과 이미지 생성 모델(IGMs)을 기반으로 파이프라인을 사용하여 데이터 시각화 산출물 생성을 제시한 연구(Dibia, 2023) 로 LIDA라는 새로운 도구를 통해 문법에 구애받지 않고 자연어를 통해 시각화 및 인포그래픽을 생성한다. LIDA는 데이터를 자연어 요약으로 변환하는 SUMMARIZER, 데이터를 기반으로 시각화 목표를 나열하는 GOAL EXPLORER, 시각화 코드를 생성하고 정제하며 실행하고 필터링하는 VISGENERATOR, IGM을 사용해 데이터 충실한 스타일화된 그래픽을 생성하는 INFOGRAPHER로 구성된다.",
    "crumbs": [
      "AI 코딩",
      "데이터 사이언스"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "기본기AI 공학 코드",
    "crumbs": [
      "기본기",
      "AI 공학"
    ]
  },
  {
    "objectID": "target.html",
    "href": "target.html",
    "title": "",
    "section": "",
    "text": "자동화타겟 코드",
    "crumbs": [
      "자동화",
      "타겟"
    ]
  },
  {
    "objectID": "target.html#정의",
    "href": "target.html#정의",
    "title": "",
    "section": "정의",
    "text": "정의\ntargets는 Will Landau가 개발하고 유지 관리하는 R 프로그래밍 언어용 작업흐름(Workflow) 관리 패키지다.\ntargets의 주요 기능은 다음과 같다.\n\n작업흐름 자동화\n작업흐름 단계 캐싱\n작업흐름 단계 일괄 생성\n작업흐름 수준에서 병렬화\n\n이를 통해 다음과 같은 작업을 할 수 있다.\n\n다른 일을 하다가 프로젝트로 돌아왔을 때, 혼란 없이 그리고 무엇을 하고 있었는지 기억하려 노력하지 않고도 바로 중단했던 부분부터 다시 시작할 수 있다.\n작업흐름를 변경한 다음, 변경의 영향을 받는 부분만 다시 실행한다.\n\n개별 함수를 변경하지 않고도 작업흐름를 대규모로 확장한다.\n\n… 그리고 물론, 이는 다른 사람들이 여러분의 분석을 재현하는 데 도움을 줄 것이다.",
    "crumbs": [
      "자동화",
      "타겟"
    ]
  },
  {
    "objectID": "target.html#누가-targets를-사용해야-하는가",
    "href": "target.html#누가-targets를-사용해야-하는가",
    "title": "",
    "section": "누가 targets를 사용해야 하는가?",
    "text": "누가 targets를 사용해야 하는가?\ntargets는 결코 유일한 작업흐름 관리 소프트웨어가 아니다. 다양한 기능과 사용 사례를 가진 유사한 도구들이 많이 있다. 예를 들어, snakemake는 파이썬에서 인기 있는 작업흐름 도구이고, make는 bash 스크립트를 자동화하기 위해 오랫동안 사용되어 온 도구이다. targets는 특별히 R과 함께 작동하도록 설계되었기 때문에, 주로 R을 사용하거나 사용하려는 경우에 가장 적합하다. 대부분 다른 도구로 코딩한다면, 다른 대안을 고려해 보는 것이 좋다.\n\n재현 가능성이란 무엇인가?\n재현 가능성은 다른 사람들(미래의 자신 포함)이 여러분의 분석을 재현할 수 있는 능력이다. 과학적 분석 결과에 대해 확신을 가질 수 있는 유일한 방법은 그 결과를 재현할 수 있을 때뿐이다. 그러나 재현 가능성은 이분법적 개념(재현 불가능 vs. 재현 가능)이 아니다. 오히려 덜 재현 가능한 것에서 더 재현 가능한 것까지 스펙트럼이 존재한다.\ntargets는 여러분의 분석을 더 재현 가능하게 만드는 데 큰 도움을 준다.\nDocker, conda, renv와 같은 도구로 컴퓨팅 환경을 제어하는 것도 재현 가능성을 더욱 높이는 데 사용할 수 있다.",
    "crumbs": [
      "자동화",
      "타겟"
    ]
  },
  {
    "objectID": "target.html#footnotes",
    "href": "target.html#footnotes",
    "title": "",
    "section": "각주",
    "text": "각주\n\n\n런타임(runtime)은 프로그램이 실제로 실행되는 시간을 의미한다. 코드를 작성하고 컴파일하는 시간은 런타임에 포함되지 않고, 프로그램이 실행을 시작하는 순간부터 종료되는 순간까지의 시간이 런타임이다. 즉, 런타임 동안에는 프로그램이 메모리를 할당받고, 변수들이 값을 가지게 되며, 함수가 호출되고 실행된다.↩︎",
    "crumbs": [
      "자동화",
      "타겟"
    ]
  },
  {
    "objectID": "API_openai.html",
    "href": "API_openai.html",
    "title": "OpenAI API",
    "section": "",
    "text": "ChatGPT는 간단히 말해 생성형 사전 학습된 트랜스포머(Generative Pre-trained Transformer)의 약자로, OpenAI의 GPT-3/GPT-4 거대 언어 모델 제품군에 기반한 챗봇으로 지도학습과 강화학습기법을 적용하여 미세조정(fine-tuned)된 제품이자 서비스다.",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#gpt-4",
    "href": "API_openai.html#gpt-4",
    "title": "OpenAI API",
    "section": "GPT-4",
    "text": "GPT-4\nGPT-4’s Leaked Details Shed Light on its Massive Scale and Impressive Architecture\nGPT-4는 GPT-3보다 10배 많은 1조 8천억 개의 파라미터, 120개 계층을 갖는 아키텍쳐를 갖고 있다. OpenAI는 16개 전문가(MoE, Mixture of Experts)와 1,100억 개의 다층 퍼셉트론 파라미터를 갖는 전문가 혼합 모델로 구현되었으며, 13조 개의 토큰이 포함된 학습 데이터셋을 사용했다. 훈련 비용은 3,200 ~ 6,300만 달러로 GPT-4는 이전 버전보다 추론 비용이 약 3배 더 높지만, 분산 데이터센터에서 128개 GPU 클러스터 위에서 동작하는 추론 아키텍쳐를 갖고 있다.\n\n\n\nThe Ship of Theseus\n\n\nOpenAI의 전략은 테세우스의 배(Theseus’s Ship) 와 유사하다고 볼 수 있다.",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#gpt-3",
    "href": "API_openai.html#gpt-3",
    "title": "OpenAI API",
    "section": "GPT-3",
    "text": "GPT-3\nOpenAI GPT-3 모형은 크게 세가지가 있다.\n\nGPT-3/GPT-4\nCodex\n콘텐츠 필터 모델\n\nGPT-3은 자연어 처리 및 생성을 담당하는 모델로 인간의 언어 즉, 자연어처럼 보이는 텍스트를 이해할 수 있다. 한걸음 더 들어가면 속도와 성능에 따라 4가지 모델(A, B, C, D)로 구분된다.\n\ntext-davinci-003\ntext-curie-001\ntext-babbage-001\ntext-ada-001\n\n성능기준으로 보면 다음과 같이 정렬할 수 있는데 비용도 그에 따라 높아진다는 의미도 함축한다.\ntext-davinci-003 &gt; text-curie-001 &gt; text-babbage-001 &gt; text-ada-001\n따라서, OpenAI는 다빈치 모델(text-davinci-003)을 통해 원하는 결과를 얻은 후에 다른 모델을 사용해 볼 것을 권장하는데 이유는 훨씬 저렴한 비용으로 많은 수의 유사한 작업을 수행할 수 있기 때문이다.\n\ntext-ada-001\n2,048개의 토큰 및 2019년 10월까지의 데이터 학습하여 이후 모형과 비교하여 정확도나 성능에서 다소 밀리는 모습이지만 최적화를 통해 매우 빠르고 비용이 가장 저렴하다.\n\n\ntext-babbage-001\n2,048개의 토큰과 2019년 10월까지의 데이터 학습되었고 간단한 분류와 의미론적 분류에 효과적이다.\n\n\ntext-curie-001\n최대 2048개의 토큰을 지원하며 text-davinci-003 다음으로 뛰어난 성능을 보이는 GPT-3 모델이다. 2019년 10월까지의 데이터로 학습되었기 때문에 text-davinci-003보다 정확도가 떨어지지만, 번역, 복잡한 분류, 텍스트 분석 및 요약에 좋은 성능을 보이고 있어 text-davinci-003와 비교하여 가성비가 높다고 평가되고 있다.\n\n\ntext-davinci-003\n2021년 9월까지의 데이터로 훈련되었기 때문에 최신 정보를 제공하지 못한다는 한계는 있지만, 앞선 GPT-3 모형과 비교하여 더 높은 품질을 제공한다. 장점 중 하나는 최대 4,000개 토큰까지 요청할 수 있다는 점이 이전 모형과 큰 차별점이 된다.",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#코덱스codex",
    "href": "API_openai.html#코덱스codex",
    "title": "OpenAI API",
    "section": "코덱스(Codex)",
    "text": "코덱스(Codex)\n코덱스는 프로그래밍 코드 이해 및 생성을 위한 것으로 code-davinci-002와 code-cushman-001가 있다. 또한, 코덱스는 GitHub Copilot을 구동하는 모델이기도 하다. 파이썬, 자바스크립트, 고, 펄, PHP, 루비, 스위프트, 타입스크립트, SQL, 셸 등 12개 이상의 프로그래밍 언어를 지원할 뿐만 아니라 자연어로 표현된 주석(comment)를 이해하고 사용자를 대신하여 요청된 작업을 수행할 수 있다.\n\ncode-cushman-001\n복잡한 작업을 수행하는 데 있어서는 code-davinci-002가 더 강력하지만, 많은 코드 생성 작업을 수행할 수 있고 code-davinci-002 보다 더 빠르고 저렴하다는 장점이 있다.\n\n\ncode-davinci-002\n자연어를 코드로 번역하는 데 탁월할 뿐만 아니라 코드를 자동 완성할 뿐만 아니라 보충 요소 삽입도 지원한다. 최대 8,000개의 토큰을 처리할 수 있으며 2021년 6월까지의 데이터로 학습되었다.",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#콘텐츠-필터",
    "href": "API_openai.html#콘텐츠-필터",
    "title": "OpenAI API",
    "section": "콘텐츠 필터",
    "text": "콘텐츠 필터\n민감한 콘텐츠 제거하기 위한 필터 모형이다. 민감하거나 안전하지 않을 수 있는 API 생성 텍스트를 감지할 수 있다. 사용자가 사용할 AI 응용프로그램을 개발할 경우, 필터를 사용하여 모델이 부적절한 콘텐츠를 반환하는지 감지할 수 있다. 이 필터는 텍스트를 다음 3가지 범주로 나눈다.\n\n안전(safe)\n민감(sensitive)\n안전하지 않음(unsafe)",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#openai-챗gpt-openai-api",
    "href": "API_openai.html#openai-챗gpt-openai-api",
    "title": "OpenAI API",
    "section": "OpenAI, 챗GPT, OpenAI API",
    "text": "OpenAI, 챗GPT, OpenAI API\nOpenAI, 챗GPT(ChatGPT), OpenAI API를 명확히 구분하는 것이 필요하다.\n\nOpenAI: 회사명\n챗GPT(ChatGPT): AI 응용프로그램 (AI 챗팅 서비스)\nOpenAI API: OpenAI 인공지능 모형을 활용하는 API 서비스\n\n\n인터페이스\nOpenAI API는 OpenAI에서 개발한 GPT-3, GPT-4 모델을 통해 AI 기능을 개발하고 있는 다양한 제품과 서비스에 담아내는 과정이다. 제품과 서비스를 개발하면서 머리 뿐만 아니라 다른 다양한 재료도 데이터, API 혹은 파일 형태로 담아낼 수 있다.\n\n\n\n고려사항\nOpenAI는 3월에 채팅 완료(Chat Completion) API를 도입했으며, 현재 API GPT 사용량의 97%를 차지하고 있다.\n2020년 6월에 도입된 초기 Completion API는 언어 모델과 상호 작용할 수 있는 자유형 텍스트 프롬프트를 제공하기 위해 도입되었다. 이후 보다 구조화된 프롬프트 인터페이스(structured prompt interface)를 통해 더 나은 결과를 제공할 수 있다는 사실을 알게 되었다. 채팅 기반 패러다임은 이전의 사용 사례와 새로운 대화 요구 사항의 대부분을 처리하는 동시에 더 높은 유연성과 구체성을 제공하는 강력한 것으로 입증되었다. 특히 채팅 완료 API의 구조화된 인터페이스(예: 시스템 메시지, 함수 호출)와 멀티턴(Multi-turn) 대화 기능을 통해 개발자는 대화 환경과 광범위한 완료 작업을 구축할 수 있다.\n\n\n\n\n\n\n\n\n구분\n이전 모형\n신 모형\n\n\n\n\nChat Completion API\ngpt-3.5-turbo\ngpt-3.5-turbo\n\n\nCompletion API\nada\nada-002\n\n\nCompletion API\nbabbage\nbabbage-002\n\n\nCompletion API\ncurie\ncurie-002\n\n\nCompletion API\ndavinci\ndavinci-002\n\n\nCompletion API\ndavinci-instruct-beta\ngpt-3.5-turbo-instruct\n\n\nCompletion API\ncurie-instruct-beta\ngpt-3.5-turbo-instruct\n\n\nCompletion API\ntext-ada-001\ngpt-3.5-turbo-instruct\n\n\nCompletion API\ntext-babbage-001\ngpt-3.5-turbo-instruct\n\n\nCompletion API\ntext-curie-001\ngpt-3.5-turbo-instruct\n\n\nCompletion API\ntext-davinci-001\ngpt-3.5-turbo-instruct\n\n\nCompletion API\ntext-davinci-002\ngpt-3.5-turbo-instruct\n\n\nCompletion API\ntext-davinci-003\ngpt-3.5-turbo-instruct\n\n\nEmbeddings Model\ncode-search-ada-code-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ncode-search-ada-text-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ncode-search-babbage-code-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ncode-search-babbage-text-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-ada-doc-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-ada-query-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-babbage-doc-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-babbage-query-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-curie-doc-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-curie-query-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-davinci-doc-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-search-davinci-query-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-similarity-ada-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-similarity-babbage-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-similarity-curie-001\ntext-embedding-ada-002\n\n\nEmbeddings Model\ntext-similarity-davinci-001\ntext-embedding-ada-002\n\n\n\n\n\nAPI\nOpenAI는 크게 3가지 서비스를 제공하고 있다.\n\n챗GPT\nDall-E\nAPI\n\nAPI 문서를 통해 다양한 API 서비스를 확인할 수 있다.",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#openai-api-헬로월드",
    "href": "API_openai.html#openai-api-헬로월드",
    "title": "OpenAI API",
    "section": "OpenAI API 헬로월드",
    "text": "OpenAI API 헬로월드\nOpenAI 회사가 제시하고 있는 기준에 맞춰 따라줘야 OpenAI API를 사용할 수 있다. 이를 위해 먼저 회원가입을 통한 계정을 생성하고 본인 인증과 과금을 위한 API KEY를 생성하고 프로그래밍 언어에서 쉽게 사용할 수 있도록 파이썬의 경우 openai 패키지를 설치하고 문법에 맞춰 코드를 작성하면 된다.\n\n계정생성API KEY 발급openai 패키지 설치헬로월드보안강화\n\n\nOpenAI API 생성 웹사이트에서 계정을 생성한다.\n\n\n\n\n\n\n\nAPI keys 웹사이트에서 API KEY를 발급받는다.\n\n\n\nAPI Reference 안내에 따라 openai 패키지를 설치한다.\n$ pip install openai\n\n\nAPI키를 직접 파이썬 프로그램에 명시하고 결과를 확인한다.\n\n\n코드\nimport openai\n\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"OpenAI API가 뭔가요?\"\n)\n\nprint(response)\n\n\n{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"\\n\\nOpenAI API\\ub294 \\uc778\\uacf5\"\n    }\n  ],\n  \"created\": 1689745304,\n  \"id\": \"cmpl-7duESieoaT985f4IKPskfcYQ3AH7F\",\n  \"model\": \"text-davinci-003\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 14,\n    \"prompt_tokens\": 15,\n    \"total_tokens\": 29\n  }\n}\n\n\n\n\n코드\nimport requests\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n#&gt; True\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"OpenAI API가 뭔가요?\"\n)\n\nprint(response[\"choices\"][0]['text'])\n\n\nOpenAI API는 OpenAI가",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#openai-api-key",
    "href": "API_openai.html#openai-api-key",
    "title": "OpenAI API",
    "section": "OpenAI API KEY",
    "text": "OpenAI API KEY\n\n1 단계2 단계3 단계",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#postman-접속확인",
    "href": "API_openai.html#postman-접속확인",
    "title": "OpenAI API",
    "section": "Postman 접속확인",
    "text": "Postman 접속확인\nPostman 웹사이트에 회원가입하고 로그인한다.\n\n1 단계2 단계3 단계4 단계5 단계6 단계7 단계8 단계9 단계10 단계11 단계12 단계",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "API_openai.html#ai-이미지-생성",
    "href": "API_openai.html#ai-이미지-생성",
    "title": "OpenAI API",
    "section": "AI 이미지 생성",
    "text": "AI 이미지 생성\n\n1 단계2 단계3 단계4 단계",
    "crumbs": [
      "AI 코딩",
      "OpenAI API"
    ]
  },
  {
    "objectID": "chat.html",
    "href": "chat.html",
    "title": "",
    "section": "",
    "text": "사례챗GPT 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "사례",
      "챗GPT"
    ]
  },
  {
    "objectID": "chat.html#키보드-단축키",
    "href": "chat.html#키보드-단축키",
    "title": "",
    "section": "키보드 단축키",
    "text": "키보드 단축키\n\n\n코드library(chattr)\nchattr_use(provider = \"LlamaGPT\", path_url = \"~/LlamaGPTJ-chat/build/bin/chat\", model = \"D:/llms/ggml-gpt4all-j.bin\")\n\n\n&gt; chattr_defaults(path = \"D:\\\\llms\\\\ggml-gpt4all-j.bin\", model = \"LlamaGPT\")\n\n── chattr ────────────────────────────────────────────────────────────────────────\n\n── Defaults for: Default ──\n\n── Prompt: \n• Use the R language, the tidyverse, and tidymodels\n\n── Model \n• Provider: LlamaGPT\n• Path/URL: D:\\llms\\ggml-gpt4all-j.bin\n• Model: LlamaGPT\n• Label: GPT4ALL 1.3 (LlamaGPT)\n\n── Model Arguments: \n• threads: 4\n• temp: 0.01\n• n_predict: 1000\n\n── Context: \nMax Data Files: 0\nMax Data Frames: 0\n✖ Chat History\n✖ Document contents",
    "crumbs": [
      "사례",
      "챗GPT"
    ]
  },
  {
    "objectID": "rstats.html",
    "href": "rstats.html",
    "title": "",
    "section": "",
    "text": "기본기R 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#r-작업흐름",
    "href": "rstats.html#r-작업흐름",
    "title": "",
    "section": "R 작업흐름",
    "text": "R 작업흐름\nR 작업흐름(workflow)은 데이터 과학업무를 효율적으로 수행하기 위한 과정이다. R을 시작하고 필요한 패키지를 설치한다. 재현 가능한 환경을 생성하여 일관된 결과를 보장한다. R을 최신 버전으로 설치하고 업그레이드하여 새로운 기능과 개선사항을 활용한다. 이러한 순환적인 과정을 통해 R 사용자는 데이터 불러오기, 전처리, 분석, 시각화, 모형개발, 결과 해석 등의 작업을 체계적으로 수행한다.",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#renviron",
    "href": "rstats.html#renviron",
    "title": "",
    "section": ".Renviron",
    "text": ".Renviron\n.Renviron 파일은 주로 API 키(예: GitHub, OpenAI)와 같은 민감한 정보나 R 관련 환경 변수(예: 히스토리 크기 R_HISTSIZE=100000, 기본 라이브러리 위치 R_LIBS_USER)를 정의하는 데 유용하다. R 코드는 담기지 않는다. .Renviron 파일을 편집하려면 usethis::edit_r_environ()을 실행한다.\nR_HISTSIZE=100000\nRETICULATE_PYTHON=.venv/bin/python\nDB_USER=elephant\nDB_PASS=p0stgr3s\nGITHUB_PAT=abc123\nR_LIBS_USER=~/R/%p/%v\nusethis::edit_r_environ(scope = ?)을 사용해서 환경설정변수 파일(.Renviron) 위치를 지정할 수 있다.\n\n\n사용자\n프로젝트\n\n\n~/.Renviron\npath/to/your/project/.Renviron\n\n\n\n\n\n\n\n\nR_LIBS_USER=~/R/%p/%v의 의미\n\n\n\nR_LIBS_USER=~/R/%p/%v는 R에서 사용자 지정 라이브러리의 위치를 설정하는 환경 변수로 다음과 같은 의미를 갖는다.\n\n\n~: 사용자 홈 디렉토리\n\n/R/: R 관련 파일들을 저장할 디렉토리\n\n%p: R의 플랫폼 이름으로 대체 (예: “x86_64-pc-linux-gnu”).\n\n%v: R의 주요 버전 번호로 대체 (예: “4.1”).\n\n따라서 리눅스 시스템에서 R 4.3 버전을 사용한다면, 실제 경로는 다음과 같을 수 있다.\n/home/username/R/x86_64-pc-linux-gnu/4.3",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#rprofile",
    "href": "rstats.html#rprofile",
    "title": "",
    "section": ".Rprofile",
    "text": ".Rprofile\n.Rprofile 파일은 R 세션 시작 시 실행될 R 코드를 포함되고, 주로 사용자의 홈 디렉토리에 위치하며, usethis::edit_r_profile()로 편집할 수 있다. .Rprofile 파일에 공유하는데 문제를 야기할 수 있는 코드는 담지 않는다.\n\n.Rprofile GitHub 검색어 → https://github.com/search?q=.Rprofile\n\n재현성\n.Rprofile에는 R 터미널에서 대화식으로 실행하는 것만 포함해야 한다. R 스크립트나 R 마크다운 파일에 나타나는 것은 포함하지 않아야 한다. 대부분의 코드는 interactive() 함수로 감싸서 대화형 세션에서만 실행되도록 해야 한다.\noptions(repos = c(CRAN = \"https://cran.rstudio.org\"))\nif (interactive()) {\n  options(width = 120)\n}\n\n\n\n\n\n\n.Rprofile 설정에서 newline 활성화\n\n\n\n시작 파일(startup)들은 반드시 새 줄(newline)로 끝나야 한다. 만약 이렇게 하지 않으면, R이 마지막 줄을 아무 경고 없이 무시할 수 있다.\n\n\nnewline 활성화",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#실습",
    "href": "rstats.html#실습",
    "title": "",
    "section": "실습",
    "text": "실습\n\n.Renviron 환경변수 파일 편집\n\nR 콘솔에서 usethis::edit_r_environ() 실행\nWTF_USER={사용자명}_user 추가\nR 세션 새로 실행: CTRL + SHIFT + F10(윈도우), CMD  + SHIFT + 0(맥)\n\nWTF_USER 환경변수 값 Sys.getenv(\"WTF_USER\") 명령어로 확인\n\n.Rprofile 환경변수 파일 편집\n\nR 콘솔에서 usethis::edit_r_profile() 혹은 usethis::edit_r_profile(\"project\") 실행\nprint(“Rprofile 코드 추가”)\nR 세션 새로 실행: CTRL + SHIFT + F10(윈도우), CMD  + SHIFT + 0(맥)\n\nWTF_USER 환경변수 값 Sys.getenv(\"WTF_USER\") 명령어로 확인",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#r-패키지",
    "href": "rstats.html#r-패키지",
    "title": "",
    "section": "R 패키지",
    "text": "R 패키지\nR 패키지 설치 방법은 크게 바이너리 설치와 소스 코드 컴파일 설치로 구분된다. 바이너리 설치는 미리 컴파일된 패키지를 사용하는 방식으로, install.packages(\"패키지이름\")으로 간단히 실행할 수 있다. 이 방법은 빠르고 편리하지만, 시스템 특정적이며 최신 버전이 늦게 제공될 수 있다. 반면 소스 코드 컴파일 설치는 install.packages(\"패키지이름\", type = \"source\")로 실행하며, 사용자 시스템에서 직접 컴파일하여 설치한다. 이 방식은 시간이 더 걸리고 추가 도구가 필요할 수 있지만, 시스템 최적화와 최신 버전 사용이 가능하다다. 일반 사용자에게는 간편한 바이너리 설치가 권장되나, 특수한 요구사항이나 최신 기능이 필요한 경우 소스 코드 설치가 적합할 수 있다.\n운영체제에 맞게 패키지를 구할 수 있는 곳은 CRAN, Posit Public Package Manager (p3m)가 대표적이다.\n\n\n\n\n\n\n\n운영체제\nCRAN\nPosit Public Package Manager (p3m)\n\n\n\n\n 윈도우즈\n✅\n✅\n\n\n\n 맥 OS\n✅\n✅\n\n\n\n 리눅스\n❌\n✅",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#r_libs_user-패키지-저장소",
    "href": "rstats.html#r_libs_user-패키지-저장소",
    "title": "",
    "section": "\nR_LIBS_USER 패키지 저장소",
    "text": "R_LIBS_USER 패키지 저장소\n수만개의 R 패키지 중에서 일부를 로컬 컴퓨터에 설치하여 프로젝트에 활용하여 성과를 내게 된다. CRAN, P3M에서 다운로드하여 패키지가 설치된 R_LIBS_USER 환경정보를 .Renviron 파일에 설정하는 방법은 몇단계를 걸친다.\n\n\nedit_r_environ()\nR_LIBS_USER\n세션 재시작\n수정사항 확인\n\n\n\nR 콘솔에서 usethis::edit_r_environ() 명령어를 실행하여 .Renviron 파일을 연다.\n\n\nR_LIBS_USER 환경변수 설정을 한다.\nR_LIBS_USER=C:/Users/&lt;사용자명&gt;/AppData/Local/R/win-library/4.4\n\n\nCTRL + SHIFT + F10(윈도우), CMD  + SHIFT + 0(맥) 명령어를 실행하여 세션을 재시작한다.\n\n\n Sys.getenv(\"R_LIBS_USER\")\n[1] \"C:/Users/&lt;사용자명&gt;/AppData/Local/R/win-library/4.4\"\n\n\n\nR 패키지 개발과 활용\nR 패키지는 개발과정과 개발된 패키지를 활용하는 두가지 단계로 크게 구분할 수 있다. 소스코드를 번들로 만들고 이를 빌드하여 바이너리 배포용 패키지를 제작하는 과정과 패키지를 설치한 후에 이를 활용하는 두가지 단계로 나눌 수 있다.\n\n\n\n\n\n\n\n\n\n바이너리와 소스\n패키지를 바이너리(binary)를 기본으로 설치하는 것을 추천하지만 GitHub 저장소에서 R 패키지를 설치할 경우 불가피하게 패키지를 소스(source)에서 설치해야한다.\ninstall.packages() 함수를 사용할 때 type 파라미터를 “source”로 지정하면 CRAN, P3M 에서 바이너리 대신 소스를 컴파일하여 설치할 수 있다.\ninstall.packages(\"패키지명\", type = \"source\")\ndevtools 패키지를 사용하여 GitHub 등에서 특정 버전의 소스를 직접 설치할 수 있는데 컴파일 시간이 더 오래 걸릴 수 있고, 시스템에 필요한 개발 도구가 설치되어 않으면 패키지를 설치할 수도 없다.\ndevtools::install_github(\"사용자명/레포지토리명\")\n로컬 컴퓨터 환경에 맞춰 C, C++, 포트란(Fortran), 러스트(Rust) 언어로 된 코드를 컴파일할 수 있는 도구를 운영체제에 맞춰 설치를 해야만 소스코드 형태 R 패키지를 설치할 수 있다.\n\n\n윈도우즈 \n맥OS \n리눅스 \n\n\n\nRtools\n\n\nXCode\nxcode-select --install\n\n\ninstall tools via package manager, e.g.\napt install make\n\n\n\n운영체제에 맞춰 도구를 설치하면 devtools::has_devel() 명령어로 설치여부를 점검할 수 있다.\n\n코드devtools::has_devel()\n\nYour system is ready to build packages!",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#재현가능-환경",
    "href": "rstats.html#재현가능-환경",
    "title": "",
    "section": "재현가능 환경",
    "text": "재현가능 환경\n데이터 과학 작업의 환경을 재현하는 전략을 크게 재현성 환경관리 책임자와 패키지 접근 개방이라는 두 가지 축을 기반으로 ‘스냅샷 및 복원(Snapshot)’, ‘공유 기준선(Shared Baseline)’, ‘검증(Validated)’이라는 세 가지 성공적인 전략을 제시하는 동시에 ’무법 지대’, ‘티켓 시스템’, ’차단’이라는 세 가지 위험 구역도 언급하고 있다. 1\n\n\n재현성 환경 전략지도\n\n스냅샷 전략은 데이터 과학 작업의 재현성을 보장하는 효과적인 방법으로 널리 인정받고 있다. 주요 장점으로 특정 시점의 환경을 정확히 캡처하고 나중에 그대로 복원할 수 있다는 점으로, 프로젝트의 모든 종속성, 패키지 버전, 설정을 포함한 전체 작업 환경을 보존함으로써 연구 결과의 정확한 재현을 가능하게 한다.\n스냅샷 전략은 동시에 유연성도 제공한다. 데이터 과학자들은 필요에 따라 자유롭게 패키지를 설치하고 업데이트할 수 있으며, 동시에 각 프로젝트의 고유한 환경을 유지할 수 있다. 이를 통해 혁신과 실험을 장려하면서도 프로젝트의 안정성을 보장하는 균형 잡힌 접근 방식으로 팀 협업 시에도 모든 구성원이 동일한 환경에서 작업할 수 있게 하여 “내 컴퓨터에서는 작동합니다”와 같은 문제를 해결하는데 기여할 수 있다.\n그러나 스냅샷 전략에도 몇 가지 단점도 존재한다. 첫째, 스냅샷 관리에 추가적인 시간과 리소스가 필요하다. 특히 큰 조직이나 많은 프로젝트를 다루는 경우, 여러 스냅샷을 관리하고 추적하는 것이 복잡해질 수 있다. 둘째, 스냅샷은 시간이 지남에 따라 용량을 많이 차지할 수 있으며, 이는 저장 공간 문제로 이어질 수 있다. 마지막으로, 보안 업데이트나 중요한 버그 수정이 있을 때 모든 관련 스냅샷을 업데이트해야 하는 번거로움이 있을 수 있다.\n프로젝트 공유환경\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n코드&gt; .libPaths()\n[1] \"C:/Users/statkclee/AppData/Local/R/win-library/4.4\"  # 사용자\n[2] \"C:/Program Files/R/R-4.4.1/library\"                  # 시스템\n\n\n프로젝트 격리 환경\n\n\n\n\n\n\n\n\n\n\n\n\n코드- Project '~/sample-project' loaded. [renv 1.0.7]\n&gt; .libPaths()\n[1] \"C:/Users/statkclee/Documents/sample-project/renv/library/windows/R-4.4/x86_64-w64-mingw32\"        \n[2] \"C:/Users/statkclee/AppData/Local/R/cache/R/renv/sandbox/windows/R-4.4/x86_64-w64-mingw32/88765555\"",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "rstats.html#footnotes",
    "href": "rstats.html#footnotes",
    "title": "",
    "section": "각주",
    "text": "각주\n\nStrategy Maps - Strategies to Reproduce Environments Over Time↩︎",
    "crumbs": [
      "기본기",
      "R"
    ]
  },
  {
    "objectID": "database.html",
    "href": "database.html",
    "title": "벡터 데이터베이스",
    "section": "",
    "text": "그림 1: 출처: https://blog.det.life/why-you-shouldnt-invest-in-vector-databases-c0cd3f59d23c",
    "crumbs": [
      "데이터베이스",
      "벡터 데이터베이스"
    ]
  },
  {
    "objectID": "database.html#임베딩",
    "href": "database.html#임베딩",
    "title": "벡터 데이터베이스",
    "section": "임베딩",
    "text": "임베딩\n자연어 처리(NLP) 분야에서 나온 개념으로, 텍스트의 맥락과 의도를 포착하는데 유용한 임베딩(Embedding)은 텍스트를 다차원 벡터 공간에 매핑한 수치적 표현으로 텍스트의 의미를 포착하는 숫자 벡터로 변환하는 과정이다. 텍스트의 의미적 유사성을 수학적으로 계산할 수 있기 때문에 유사한 의미를 가진 단어나 문장은 벡터 공간에서 서로 가깝게 위치하게 된다. 의미 검색, 추천 시스템, 분류 작업 등 다양한 AI 응용 프로그램에 활용된다.\n\n\n\n\n\n\n\n\n\n\n\n코드\nfrom openai import OpenAI\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cosine\nfrom sklearn.manifold import TSNE\nfrom dotenv import load_dotenv\nimport os\nimport matplotlib.font_manager as fm\n\nload_dotenv()\n\n# 텍스트 데이터 (정치 상황)\ntexts = [\n    \"A당은 경제 성장을 최우선 과제로 삼고 있다\",\n    \"B당은 복지 정책 확대를 주장하고 있다\",\n    \"A당은 규제 완화를 통한 기업 활성화를 추진한다\",\n    \"B당은 환경 보호를 위한 정책을 강조한다\",\n    \"A당은 국방력 강화에 중점을 두고 있다\",\n    \"B당은 교육 개혁을 통한 인재 양성을 중시한다\",\n    \"양당은 부동산 정책에서 첨예하게 대립하고 있다\"\n]\n\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# 임베딩 생성 함수\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding\n\n# 텍스트를 임베딩으로 변환\nembeddings = [get_embedding(text) for text in texts]\n# 임베딩 리스트를 NumPy 배열로 변환\nembeddings_array = np.array(embeddings)\n\n# 시각화를 위한 차원 축소 (t-SNE)\ntsne = TSNE(n_components=2, random_state=42, perplexity=5)\nreduced_embeddings = tsne.fit_transform(embeddings_array)\n\n# 한글 폰트 경로 설정 (Windows 기준)\nfont_path = r'C:\\Windows\\Fonts\\malgun.ttf'  # 맑은 고딕 폰트 경로\n\n# 폰트 프로퍼티 설정\nfont_prop = fm.FontProperties(fname=font_path, size=10)\n\n# matplotlib 폰트 설정\nplt.rcParams['font.family'] = font_prop.get_name()\n\n# 그래프 그리기\nplt.figure(figsize=(10, 8))\nfor i, (x, y) in enumerate(reduced_embeddings):\n    plt.scatter(x, y)\n    plt.annotate(texts[i], (x, y), xytext=(5, 5), textcoords='offset points', fontproperties=font_prop)\nplt.title(\"정치 상황 텍스트 임베딩 시각화\", fontproperties=font_prop)\n\n# 그래프를 PNG 파일로 저장\nplt.savefig('images/embeddings_visualization.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# 거리 측정 및 추천\ndef recommend(query, texts, embeddings, top_n=3):\n    query_embedding = get_embedding(query)\n    distances = [cosine(query_embedding, emb) for emb in embeddings]\n    sorted_indices = np.argsort(distances)\n    return [texts[i] for i in sorted_indices[:top_n]]\n\n# 추천 예시\nquery = \"기후 위기\"\nrecommendations = recommend(query, texts, embeddings)\nprint(f\"'{query}'와 가장 유사한 텍스트:\")\nfor i, rec in enumerate(recommendations, 1):\n    print(f\"{i}. {rec}\")\n\n\n\n텍스트 → 벡터\n텍스트를 OpenAI 임베딩 모형(text-embedding-ada-002)을 통해 1,536 차원 벡터로 변환시킨다.\n\n\n코드\nprint(embeddings[1][:10])\n[-0.021668272092938423, -0.018942788243293762, 0.013783353380858898, 0.0006076404242776334, -0.03229901194572449, 0.010813796892762184, -0.022183537483215332, 0.01155957579612732, -0.031404078006744385, 0.004132294096052647]\n\n\n\n\n벡터 시각화\n텍스트를 OpenAI 임베딩 모형(text-embedding-ada-002)을 통해 1,536 차원 벡터로 변환을 했는데 총 7개 문장에 대해 각 당의 정책적 차이를 시각적으로 확인하기 위해 차원축소기법으로 TSNE를 사용하여 2차원 공간을 축소하여 시각화하여 A당과 B당의 정책이 몰려있고 서로 다른 차이를 갖는 것을 확인할 수 있다.\n\n\n\n벡터 검색\n기후 위기와 가장 관련된 정책을 검색하는데 벡터 검색을 사용하여 가장 거리가 가까운 순으로 3개를 추출한다. 먼저 검색어를 get_embedding() 함수로 벡터로 변환하고 코사인 유사도(cosign()) 함수를 사용해서 거리를 계산하고 가장 거리가 가까운 정책을 출력한다.\n'기후 위기'와 가장 유사한 텍스트:\n1. B당은 환경 보호를 위한 정책을 강조한다\n2. A당은 경제 성장을 최우선 과제로 삼고 있다\n3. A당은 국방력 강화에 중점을 두고 있다",
    "crumbs": [
      "데이터베이스",
      "벡터 데이터베이스"
    ]
  },
  {
    "objectID": "database.html#감성분류",
    "href": "database.html#감성분류",
    "title": "벡터 데이터베이스",
    "section": "감성분류",
    "text": "감성분류\n\nZero-shot 임베딩 기반 감성분류\n각 감정에 대한 예시 문장 대신, 감정을 설명하는 문장을 사용한다. 감정의 일반적인 개념을 사용하기 때문에 특정 예시에 의존하지 않하거나 휘둘리지 않는 장점이 있다. 특히 scipy 패키지 distance.cosine() 함수를 사용해서 거리를 사용한다는 점에서 차이가 있다.\n\n\n코드\nimport numpy as np\nfrom openai import OpenAI\nfrom scipy.spatial import distance\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# OpenAI 클라이언트 설정\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# 텍스트를 임베딩으로 변환하는 함수\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding\n\n# 감정 레이블 정의\nsentiments = [\"긍정적인\", \"중립적인\", \"부정적인\"]\n\n# 감정 레이블에 대한 설명 텍스트\nsentiment_descriptions = [\n    \"이 문장은 긍정적인 감정이나 태도를 표현합니다.\",\n    \"이 문장은 중립적이거나 특별한 감정을 표현하지 않습니다.\",\n    \"이 문장은 부정적인 감정이나 태도를 표현합니다.\"\n]\n\n# 감정 설명의 임베딩 계산\nsentiment_embeddings = [get_embedding(desc) for desc in sentiment_descriptions]\n\n# 새로운 문장의 감정 분류 함수\ndef classify_sentiment(text):\n    text_embedding = get_embedding(text)\n    distances = [distance.cosine(text_embedding, sentiment_emb) for sentiment_emb in sentiment_embeddings]\n    closest_sentiment_index = np.argmin(distances)\n    return sentiments[closest_sentiment_index]\n\n# 테스트\ntest_sentences = [\n    \"오늘은 정말 행복한 날이에요!\",\n    \"그저 그런 하루였어요.\",\n    \"이 상황이 너무 힘들어요.\",\n    \"새로운 기회를 얻게 되어 기뻐요.\",\n    \"별로 특별한 감정은 없어요.\"\n]\n\nfor sentence in test_sentences:\n    sentiment = classify_sentiment(sentence)\n    print(f\"문장: '{sentence}'\")\n    print(f\"감정 분류: {sentiment}\\n\")\n\n\n문장: '오늘은 정말 행복한 날이에요!'\n감정 분류: 긍정적인\n\n문장: '그저 그런 하루였어요.'\n감정 분류: 긍정적인\n\n문장: '이 상황이 너무 힘들어요.'\n감정 분류: 부정적인\n\n문장: '새로운 기회를 얻게 되어 기뻐요.'\n감정 분류: 긍정적인\n\n문장: '별로 특별한 감정은 없어요.'\n감정 분류: 중립적인\n\n\nFew-shot 임베딩 기반 감성분류\n일반적인 개념이 아닌 몇가지 감성분류 사례를 제시하고 제시된 텍스트에 대한 감성을 분류한다.\n\n\n코드\nimport numpy as np\nfrom openai import OpenAI\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# OpenAI 클라이언트 설정\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# 텍스트를 임베딩으로 변환하는 함수\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding\n\n# 감정 레이블과 예시 문장\nsentiment_examples = {\n    \"긍정\": [\"정말 좋은 하루였어요!\", \"이 영화 최고예요!\", \"새 직장이 너무 마음에 들어요.\"],\n    \"중립\": [\"오늘 날씨는 평범해요.\", \"특별한 일은 없었어요.\", \"그냥 그래요.\"],\n    \"부정\": [\"정말 최악의 경험이었어요.\", \"실망스러워요.\", \"오늘 기분이 좋지 않아요.\"]\n}\n\n# 각 감정 레이블에 대한 평균 임베딩 계산\nsentiment_embeddings = {}\nfor sentiment, examples in sentiment_examples.items():\n    embeddings = [get_embedding(ex) for ex in examples]\n    sentiment_embeddings[sentiment] = np.mean(embeddings, axis=0)\n\n# 새로운 문장의 감정 분류 함수\ndef classify_sentiment(text):\n    text_embedding = get_embedding(text)\n    similarities = {\n        sentiment: cosine_similarity([text_embedding], [emb])[0][0]\n        for sentiment, emb in sentiment_embeddings.items()\n    }\n    return max(similarities, key=similarities.get)\n\n# 테스트\ntest_sentences = [\n    \"오늘은 정말 행복한 날이에요!\",\n    \"그저 그런 하루였어요.\",\n    \"이 상황이 너무 힘들어요.\",\n    \"새로운 기회를 얻게 되어 기뻐요.\",\n    \"별로 특별한 감정은 없어요.\"\n]\n\nfor sentence in test_sentences:\n    sentiment = classify_sentiment(sentence)\n    print(f\"문장: '{sentence}'\")\n    print(f\"감정 분류: {sentiment}\\n\")\n\n\n문장: '오늘은 정말 행복한 날이에요!'\n감정 분류: 긍정\n\n문장: '그저 그런 하루였어요.'\n감정 분류: 중립\n\n문장: '이 상황이 너무 힘들어요.'\n감정 분류: 부정\n\n문장: '새로운 기회를 얻게 되어 기뻐요.'\n감정 분류: 긍정\n\n문장: '별로 특별한 감정은 없어요.'\n감정 분류: 중립",
    "crumbs": [
      "데이터베이스",
      "벡터 데이터베이스"
    ]
  },
  {
    "objectID": "database.html#뉴스기사-분류",
    "href": "database.html#뉴스기사-분류",
    "title": "벡터 데이터베이스",
    "section": "뉴스기사 분류",
    "text": "뉴스기사 분류\n뉴스 기사의 토픽을 분류하는 기능구현도 가능하다. 먼저 7개의 주요 뉴스 토픽인 정치, 경제, 사회, 문화, 과학기술, 스포츠, 국제를 정의하고, 각 토픽에 대해 간단한 설명을 제공하여 Zero-shot 학습을 준비한다. OpenAI의 API를 사용하여 이 토픽 설명과 분류할 뉴스 기사 텍스트의 임베딩을 생성하고ㅡ, 토픽 분류는 주어진 뉴스 기사의 임베딩과 각 토픽 설명의 임베딩 간의 코사인 거리를 계산하여 수행한다. 가장 거리가 가까운, 즉 가장 유사한 토픽을 해당 기사의 토픽으로 선택한다.\n\n\n코드\nimport numpy as np\nfrom openai import OpenAI\nfrom scipy.spatial import distance\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\n# OpenAI 클라이언트 설정\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# 텍스트를 임베딩으로 변환하는 함수\ndef get_embedding(text):\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding\n\n# 뉴스 토픽 정의\ntopics = [\"정치\", \"경제\", \"사회\", \"문화\", \"과학기술\", \"스포츠\", \"국제\"]\n\n# 토픽에 대한 설명 텍스트\ntopic_descriptions = [\n    \"정치와 관련된 뉴스로, 정부, 정책, 선거, 정당 등에 대한 내용을 다룹니다.\",\n    \"경제와 관련된 뉴스로, 금융, 주식, 기업, 무역 등에 대한 내용을 다룹니다.\",\n    \"사회와 관련된 뉴스로, 교육, 범죄, 환경, 복지 등 사회 전반의 이슈를 다룹니다.\",\n    \"문화와 관련된 뉴스로, 예술, 엔터테인먼트, 라이프스타일 등에 대한 내용을 다룹니다.\",\n    \"과학기술과 관련된 뉴스로, 연구, 발명, IT, 우주 등에 대한 내용을 다룹니다.\",\n    \"스포츠와 관련된 뉴스로, 각종 경기, 선수, 팀 등에 대한 내용을 다룹니다.\",\n    \"국제 뉴스로, 외교, 세계 각국의 주요 사건 등에 대한 내용을 다룹니다.\"\n]\n\n# 토픽 설명의 임베딩 계산\ntopic_embeddings = [get_embedding(desc) for desc in topic_descriptions]\n\n# 뉴스 기사의 토픽 분류 함수\ndef classify_news_topic(news_text):\n    news_embedding = get_embedding(news_text)\n    distances = [distance.cosine(news_embedding, topic_emb) for topic_emb in topic_embeddings]\n    closest_topic_index = np.argmin(distances)\n    return topics[closest_topic_index]\n\n# 테스트용 뉴스 기사 샘플\ntest_news_articles = [\n    \"국회는 오늘 새로운 법안을 통과시켰다. 이번 법안은 청년 일자리 창출을 위한 것으로...\",\n    \"중앙은행은 기준금리를 0.25%p 인상했다고 발표했다. 이는 인플레이션 압력에 대응하기 위한 조치로...\",\n    \"올해 열린 칸 영화제에서 한국 영화가 대상을 수상했다. 이 영화는 사회적 불평등을 다룬...\",\n    \"NASA의 새로운 화성 탐사선이 성공적으로 발사되었다. 이 탐사선은 화성의 지질학적 특성을 연구할 예정이다...\",\n    \"월드컵 예선에서 한국 대표팀이 극적인 승리를 거뒀다. 경기 종료 직전 터진 골로...\"\n]\n\n# 테스트 실행\nfor article in test_news_articles:\n    topic = classify_news_topic(article)\n    print(f\"뉴스 기사: '{article[:50]}...'\")\n    print(f\"분류된 토픽: {topic}\\n\")\n\n\n뉴스 기사: '국회는 오늘 새로운 법안을 통과시켰다. 이번 법안은 청년 일자리 창출을 위한 것으로......'\n분류된 토픽: 정치\n\n뉴스 기사: '중앙은행은 기준금리를 0.25%p 인상했다고 발표했다. 이는 인플레이션 압력에 대응하기 위...'\n분류된 토픽: 경제\n\n뉴스 기사: '올해 열린 칸 영화제에서 한국 영화가 대상을 수상했다. 이 영화는 사회적 불평등을 다룬.....'\n분류된 토픽: 사회\n\n뉴스 기사: 'NASA의 새로운 화성 탐사선이 성공적으로 발사되었다. 이 탐사선은 화성의 지질학적 특성을...'\n분류된 토픽: 과학기술\n\n뉴스 기사: '월드컵 예선에서 한국 대표팀이 극적인 승리를 거뒀다. 경기 종료 직전 터진 골로......'\n분류된 토픽: 스포츠",
    "crumbs": [
      "데이터베이스",
      "벡터 데이터베이스"
    ]
  },
  {
    "objectID": "database.html#데이터베이스",
    "href": "database.html#데이터베이스",
    "title": "벡터 데이터베이스",
    "section": "데이터베이스",
    "text": "데이터베이스\n벡터 데이터베이스가 앞서 제시된 임베딩 방식 대신 필요한 이유는 이전 관계형 데이터베이스, 비정형 데이터베이스가 필요한 이유와 유사하다. 대규모 데이터셋을 효율적으로 처리할 수 있을 뿐만 아니라, 임베딩 벡터를 메모리에 모두 로드하는 기존 방식과 달리, 벡터 데이터베이스는 디스크 기반 저장과 인덱싱을 통해 대용량 데이터를 효과적으로 관리하는데 기여한다. 빠른 유사도 검색이 가능하여 특수한 인덱싱 기법을 사용하여 고차원 벡터 간의 유사도 계산을 최적화한다. 확장성이 뛰어나, 분산 아키텍처를 지원하여 데이터 규모가 커져도 성능을 유지할 수 있을 뿐만 아니라 실시간 업데이트와 쿼리가 가능하다.\n다양한 벡터 데이터베이스 중 선택 시 고려할 점은 다음과 같다. 오픈소스 솔루션을 원한다면 커뮤니티 지원이 활발하고 무료로 사용할 수 있다는 장점이 있는 Chroma, Vespa, Milvus 등을 고려할 수 있다. 관리형 서비스를 제공하여 운영 부담을 줄일 수 있는 상용 솔루션을 고려한다면 Pinecone이나 Weaviate가 좋은 선택일 수 있다.\n기존 관계형 데이터베이스와의 통합이 필요하다면 PostgreSQL의 벡터 검색 확장을 고려할 수 있다. 대규모 분산 시스템을 구축해야 한다면 Vespa나 Milvus가 적합할 수 있다. 검색 엔진 기능도 함께 필요하다면 Elasticsearch를 고려할 수 있다.\n결국, 프로젝트 규모, 예산, 필요한 기능, 운영 능력 등을 종합적으로 고려하여 적절한 벡터 데이터베이스를 선택해야 한다. 소규모 프로젝트라면 Chroma나 LanceDB와 같은 가벼운 솔루션으로 시작하고, 대규모 프로덕션 환경이라면 Pinecone이나 Vespa 같은 성숙한 솔루션이 추천된다.\n\n크로마DB\n\n\n코드\nimport chromadb\nfrom chromadb.config import Settings\nfrom chromadb.utils import embedding_functions\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nload_dotenv()\n\n# OpenAI 클라이언트 설정\nopenai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# 임베딩 함수 정의\ndef get_embedding(texts):\n    # texts가 문자열인 경우 리스트로 변환\n    if isinstance(texts, str):\n        texts = [texts]\n    \n    response = openai_client.embedding.create(\n        model=\"text-embedding-ada-002\",\n        input=texts\n    )\n    return [item['embedding'] for item in response['data']]\n\n# ChromaDB 임베딩 함수 생성\nembedding_function = embedding_functions.OpenAIEmbeddingFunction(get_embedding)\n\n# 지속성 저장소 경로 설정\npersist_directory = \"./data/chroma_db\"\n\n# ChromaDB 클라이언트 생성\nclient = chromadb.Client(Settings(\n    chroma_db_impl=\"duckdb+parquet\",\n    persist_directory=persist_directory\n))\n\n# 컬렉션 생성 또는 기존 컬렉션 로드\ncollection_name = \"persistent_news_articles\"\nif collection_name in client.list_collections():\n    collection = client.get_collection(name=collection_name, embedding_function=embedding_function)\n    print(f\"기존 컬렉션 '{collection_name}'을 로드했습니다.\")\nelse:\n    collection = client.create_collection(name=collection_name, embedding_function=embedding_function)\n    print(f\"새 컬렉션 '{collection_name}'을 생성했습니다.\")\n\n# 문서 추가 (이미 존재하지 않는 경우에만)\nif collection.count() == 0:\n    collection.add(\n        documents=[\n            \"정부, 신재생에너지 정책 발표\",\n            \"중앙은행, 기준금리 동결 결정\",\n            \"AI 기술, 의료 분야 혁신 이끌어\"\n        ],\n        metadatas=[\n            {\"category\": \"정치\"},\n            {\"category\": \"경제\"},\n            {\"category\": \"기술\"}\n        ],\n        ids=[\"1\", \"2\", \"3\"]\n    )\n    print(\"문서를 추가했습니다.\")\nelse:\n    print(\"기존 문서가 존재합니다.\")\n\n# 유사도 검색\nresults = collection.query(\n    query_texts=[\"최신 기술 동향\"],\n    n_results=2\n)\n\nprint(\"\\n검색 결과:\")\nfor i, (id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):\n    print(f\"{i+1}. ID: {id}, 거리: {distance}\")\n    print(f\"   문서: {collection.get(ids=[id])['documents'][0]}\")\n    print(f\"   메타데이터: {collection.get(ids=[id])['metadatas'][0]}\")\n    print()\n\n# 메타데이터 필터링\nfiltered_results = collection.query(\n    query_texts=[\"경제 정책\"],\n    where={\"category\": \"경제\"},\n    n_results=1\n)\n\nprint(\"필터링된 검색 결과:\")\nprint(f\"문서: {filtered_results['documents'][0][0]}\")\nprint(f\"메타데이터: {filtered_results['metadatas'][0][0]}\")\n\n# 변경사항 저장\nclient.persist()\nprint(\"\\n변경사항을 디스크에 저장했습니다.\")\n\n# 현재 컬렉션의 문서 수 출력\nprint(f\"\\n현재 컬렉션의 문서 수: {collection.count()}\")",
    "crumbs": [
      "데이터베이스",
      "벡터 데이터베이스"
    ]
  },
  {
    "objectID": "yt_openai.html",
    "href": "yt_openai.html",
    "title": "",
    "section": "",
    "text": "사례유튜브 동영상 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "사례",
      "유튜브 동영상"
    ]
  },
  {
    "objectID": "yt_openai.html#환경설정",
    "href": "yt_openai.html#환경설정",
    "title": "",
    "section": "환경설정",
    "text": "환경설정\n\n코드pip install openai\npip install unstructured\npip install langchain-community \npip install langchain-core\npip install langchain-openai\npip install yt_dlp\npip install tiktoken\npip install docarray\n\n\n\n코드import os\nimport glob\nfrom openai import OpenAI\nimport yt_dlp as youtube_dl\nfrom yt_dlp import DownloadError\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n코드openai_api_key = os.getenv(\"OPENAI_API_KEY\")",
    "crumbs": [
      "사례",
      "유튜브 동영상"
    ]
  },
  {
    "objectID": "yt_openai.html#유튜브-동영상-다운로드",
    "href": "yt_openai.html#유튜브-동영상-다운로드",
    "title": "",
    "section": "유튜브 동영상 다운로드",
    "text": "유튜브 동영상 다운로드\n\n\n챗GPT와 오정보\n오디오 추출\n\n\n\n\n\n\n\n코드# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=3t9nPopr0QA\"\n\n# Directory to store the downloaded video\noutput_dir = \"data/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True,\n}\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])        \n\n\n\n코드library(embedr)\n\nembed_audio(\"data/audio/챗GPT와 오정보(Misinformation).mp3\")\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n\n\n코드import os\nfrom pydub import AudioSegment\n\n\ndef reduce_mp3_file_size(\n    input_file, output_file, target_size_mb=24, initial_bitrate=128\n):\n    # Load the audio file\n    audio = AudioSegment.from_mp3(input_file)\n\n    # Get the current file size in MB\n    current_size_mb = os.path.getsize(input_file) / (1024 * 1024)\n\n    if current_size_mb &lt;= target_size_mb:\n        # If the file is already small enough, just copy it\n        audio.export(output_file, format=\"mp3\", bitrate=f\"{initial_bitrate}k\")\n    else:\n        # Calculate the necessary bitrate\n        duration_seconds = len(audio) / 1000\n        target_bitrate = int((target_size_mb * 8 * 1024) / duration_seconds)\n\n        # Ensure the bitrate is not too low\n        target_bitrate = max(\n            target_bitrate, 32\n        )  # 32 kbps is usually the lowest reasonable bitrate\n\n        # Export with the new bitrate\n        audio.export(output_file, format=\"mp3\", bitrate=f\"{target_bitrate}k\")\n\n    new_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n    print(f\"Original size: {current_size_mb:.2f} MB\")\n    print(f\"New size: {new_size_mb:.2f} MB\")\n    print(f\"Reduced file saved as {output_file}\")\n\n\n# Usage\ninput_file = \"data/audio/챗GPT와 오정보(Misinformation).mp3\"\noutput_file = \"data/audio/reduced_audio_file.mp3\"\nreduce_mp3_file_size(input_file, output_file)\n\n\n\n코드library(embedr)\n\nembed_audio(\"data/audio/reduced_audio_file.mp3\")\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "사례",
      "유튜브 동영상"
    ]
  },
  {
    "objectID": "yt_openai.html#오디오-텍스트",
    "href": "yt_openai.html#오디오-텍스트",
    "title": "",
    "section": "오디오 → 텍스트",
    "text": "오디오 → 텍스트\nwhisper 모형을 사용해서 오디오 음성에서 텍스트를 추출한다.\n\n코드audio_file = glob.glob(os.path.join(output_dir, \"*.mp3\"))\naudio_filename = audio_file[0]\n\nprint(audio_filename)\n\naudio_file = audio_filename\noutput_file = \"data/transcript.txt\"\nmodel = \"whisper-1\"\n\nclient = OpenAI()\n\naudio_file = open(audio_file, \"rb\")\ntranscript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n\ntranscript.text\n\nwith open(output_file, \"w\") as file:\n    file.write(transcript.text)",
    "crumbs": [
      "사례",
      "유튜브 동영상"
    ]
  },
  {
    "objectID": "yt_openai.html#텍스트-적재",
    "href": "yt_openai.html#텍스트-적재",
    "title": "",
    "section": "텍스트 적재",
    "text": "텍스트 적재\nOpenAI whisper 모형으로 추출한 텍스트를 랭체인 TextLoader로 불러온다.\n\n코드from langchain_community.document_loaders import TextLoader\n\nloader = TextLoader(\"./data/transcript.txt\")\n\ndocs = loader.load()\n\ndocs[0]\n\nDocument(page_content=\"Welcome everybody to our Seoul RMeetup online seminar where we explore various ideas and practices of data science. We are really happy today to have Jebin West from the University of Washington with us to talk about tragedy and misinformation cutting through a bullshit navigating misinformation and disinformation in the generative AI era. Jebin West is an associate professor in the information school at the University of Washington. He is the director and co-founder of the Center for Informed Public whose mission is registering strategic misinformation promoting an informed society and strengthening the democratic discourse. He is the co-director of data lab and the data science fellow at eScience Institute. He also served its affiliate faculty with the Center for Statistics and Social Sciences. He studies the science of science focusing on the impact of data technology and on slowing the spread of misinformation. Jebin also has teamed up with Carl Bostrom to launch the Calling Bullshit project developing a website and course materials for teaching quantitative reasoning and information literacy. He published the book Calling Bullshit the Art of Skepticism in a Data-Driven World which has been translated into Korean. Jebin I'll hand it over to you now to present your topic and share your insight with us. Well thank you very much Dr. Ahn. This is a real pleasure to be able to speak across the ocean. I wish I was there in person but I did get the opportunity to meet many of your students during your visit in Seattle at the University of Washington and that was a real real pleasure. Yes. So thank you so much. So today I'm going to talk about some of the excitement around generative AI and some and these chatbots that we're seeing everywhere in our world but but mostly I'm going to talk a lot about some of the concerns I have around these chatbots and how they may contribute to this growing problem of misinformation in our digital worlds. So I'm going to share my screen here. There we go. Here share and there we go. All right. Can you see that Dr. Ahn? Yes. Okay great. Okay so as Dr. Ahn said I study misinformation and disinformation in all sorts of forms. Where I study misinformation the most is at the interface of science and society but we also study it more generally in our center where we look at it from the political angle, from the angle of science, from the angle of health. We've studied misinformation during the pandemic and during elections and all sorts of other topics but today I'm going to focus on some of my concerns around generative AI and hopefully that will generate some questions and comments when it comes to this particular topic. I'm sure a lot of us are thinking about it. So one of the things that I tend to ask myself when new technologies come onto the world scene is whether we're better or worse off. I recently wrote an op-ed, an opinion piece for the Seattle Times which is our paper here in Seattle in the United States and I talked about some of the concerns I had and so some of the things I'm going to talk about in this talk come from that op-ed but I'm going to talk about a lot of other things I didn't have room to talk about in that particular op-ed. But I really want us as a group to think a lot about this. You know are we better or worse off with this new chatbot ability or and of course it's a mixed bag. There are things that might be better and there are things that are going to could be worse. I'm going to kind of focus on the more pessimistic version of this. About a week ago there was a lot of attention around a new music song that was posted on several music services that sounded like a mix between the famous musician Drake and The Weeknd. These are two musical artists that are known worldwide for their music and there was a song that was created that was quite catchy and believe it or not it was kind of good I have to admit and it was probably viewed by millions and millions of people. I don't know the exact stats on that. I should look that up but the point is this song was created with some of this new generative AI technology and this is an example where there might be some positive elements of this technology that allows for this mixing and this creation. It created a song that caught enough people's attention that it you know it caught so much attention it actually had to be taken down because of issues likely around copyright and there's all sorts of fallout from this and there's lots of discussions in the legal world that are continuing and it's only been about a week since this song was released but this is an example of the excitement that surrounds this technology and for good reason. If it can create a song that's a good mix of Drake and The Weeknd and it sounds reasonably good that's some evidence at least of the the power of this new technology and it's not like this is brand new technology. There is this kind of technology from the natural language processing world and and machine learning more generally has been around but there's been some advancements recently that make the generative aspect quite exciting but also a little scary. So I'm going to talk about today some of the the cautions that I have. There's a lot more cautions but these are the things I'm going to focus on and it's a lot to focus on in about 25 minutes so I'm going to hit these briefly and then if there are questions we can always go back to some of these topics in more detail but I'm going to go through talking about how at least from my perspective as someone who studies misinformation and as someone who studies misinformation specifically in science these are some of my big concerns. One of them is that these really are bullshitters at scale. They get a lot right but they also get some things wrong. Concerns around how this might affect democratic discourse online and offline. Concerns about content credit for all those content creators out there the musicians like Drake and The Weekend and the writers and the journalists and the researchers and the authors and the poets etc. What happens to their content when it gets pushed down after these generative AI, these chatbots and these large language models train on this content and then provide summaries and new content on top of it? Who owns the content? How's that going to work as we move forward? I'll talk a little bit about some of the job elimination issues both of course in science but in other areas it'll be that some of these are focused outside of the issue of science and then of course I'm going to talk about some of the issues of pseudoscience proliferation, the overconfidence of AI and the need for some of these qualifiers of confidence, the issues around reverse engineering, the generative cost, the actual cost both monetarily, environmentally but also the costs in other forms in creativity and other things and then I'll talk I'll end with just really emphasizing this issue about garbage in garbage out. Okay so we now live in this world where it seems we've got almost sentient beings. I know there's lots of philosophical debates whether they're sentient I don't think they are and we certainly haven't reached AGI levels and there's all sorts of great you know critiques of this particular technology but one thing that at least I think we're all pretty sure of it's it's kind of here to stay in some form or another even in my world in education as a professor who runs a research lab and also as a professor who teaches students, college students, I'm seeing the technology everywhere and when I was teaching my class last quarter I decided to just embrace the technology and allow the students to use it in any form that they see as long as they let me know that they were using it. That's my only criteria because I want to learn how this technology can be used by students and by teachers like myself and professors like myself but I also want to figure out where it goes wrong and so I learned a lot from my students and I will continue to have that kind of policy but maybe that'll change at some point. I know that some instructors don't allow it. I know that some scientific publishers are not allowing co-authorship or the use of AI in any form. Even countries like Italy have outlawed some forms of this technology and so there's going to be some that are going to eliminate it, some that are going to embrace it and the way I look at it is that it may just force me to write different kinds of questions and do different kinds of assessment but it might also be a tool that could help students learn to write when they're stuck and learn how to correct as long as they're willing to do some editing and self-correct and correction of the content that comes from these different bots and so the technology really is here to stay. And one thing I should say is what's interesting is that we have a technology that really seems to have passed the Turing test and one thing that's a bit a little surprising to me is that we haven't had a big celebration about this even though I'm, you know, I'm a bit of a critic of this technology and I focus a lot of my attention on some of the concerning areas of generative AI since I study misinformation and disinformation. I should say though on the other hand we should be thinking about some of the things that have occurred and for those not familiar with the Turing test it's this pretty simple idea that the Turing test will have passed when an individual can't tell the difference between content, you know, at least in this case written content created by a human and one by a computer and if that's the case and I do think in many respects we probably have passed this Turing test and there's been no celebration so I guess okay for those that are optimistic about this technology and more positive about this technology this would be something to celebrate absolutely no doubt. And in terms of education as I mentioned before this is really transforming education but it's transforming all sorts of other industries in ways that have captured the world's attention. I mean this technology has been adopted now faster than pretty much any other technology has been adopted with hundreds like a hundred million users within a very short amount of time and of course that's only growing and billions and billions of dollars being invested into this technology from big corporations to venture capitalists. There is a lot going into this technology and again there's good reason for that. And a lot of times my students will say well you know it's just like a calculator why would you ever want to take it away and I'm not taking I'm allowing them to use it but I will say it's a little different than a calculator because this is a technology that gets this things wrong at minimum probably 10 percent of the time. I mean these things are starting to still get sorted out in the research space we're working on some of these things to figure out you know what is the baseline error rate. Well it turns out it's pretty high and in some cases the impact can be quite high. So if you're talking about the medical field or you're talking about fields that really have an impact on an individual's lives that 10 percent or 20 percent or 5 percent or whatever those errors can be highly problematic. If you're just creating a poem then fine it doesn't really matter but I this analogy to the calculator goes a little it's there's reason to think that's a you know somewhat reasonable analogy but it but it is different and that it's not technology I wouldn't use a calculator that got the answers wrong 10 percent of the time. That would be I would probably be looking for other technologies or if I was using that calculator it would make it for a lot harder work and that's what we have to recognize that it's yes we have this technology that can do a lot of this new amazing things to make some of our jobs easier but it's going to take a lot of hard work on the editorial side to make sure that we're paying it that we're paying attention to some of those errors and there's all sorts of application this was a article just recently written by the by wired sort of examining the ways in which the medical field is starting to think about the adoption of this and in fact some medical researchers have gone so far as to say that all doctors will be using this technology at some point and and maybe that's true and there are reasons to think that's a possibility with its ability to mine the scientific literature to to integrate all sorts of different symptoms and could be that assistant in the doctor's room however there are reasons to also be worried about these technologies and in places like the medical field it might help the doctors as this article talks about but it also might not benefit so greatly many of those patients especially because we have many many examples of the ways in which these machines get things wrong and and have biases built in because the data that it's trained on has some of these biases and at this point a lot of these technologies are essentially just patching and putting band-aids on these problems that exist and part of it's just because these things are very difficult to reverse engineer which i'll talk about a little bit later now up there's plenty other reasons to be excited too actually one of my colleagues who i just saw at a conference last week uh daniel katz and his colleague michael bombarito um showed how this technology took the bar exam this is the main legal the the exam in the united states for allowing you to become a lawyer or sort of allowing you to sort of move forward as a certified lawyer lawyer um and they were able to show that chat gpt did pretty darn well and passed the bar exam and there's examples of the mcat and you know some of these other standardized tests that are go that have been uh tested with this technology in fact in many ways these have almost become a baseline test when comparing different large language models um and so uh it is pretty amazing so again amazed in many ways but i'm going to talk the rest of the time about some of the concerns that i have and and by the way uh as was mentioned i have this book where i talk about bullshit and and bullshit is a is a part of the misinformation story um and as dr on had mentioned it's also been translated in korean which has been super fun to see it uh uh pulled uh or written in these other languages and it helps me try to figure out what's being uh said here but in that book if you read it one of the more important laws and principles that we talk about is something called brandolini's bullshit asymmetry principle and if you go to wikipedia at least the english version i actually should check the korean version see if it's there but the english version of the wikipedia has this principle um in the wikipedia and this law is pretty simple it basically says that the amount of energy needed to refute bullshit is an order of magnitude bigger than needed to produce it so the amount of energy needed to refute it to clean it up to fix the problem is an order of magnitude much harder to produce it so here's the law so my colleague carl bergstrom decided to ask the large language model that meta created uh a while ago called galactica and this is you know it's been taken down since then but this was the science version essentially of one of these large language models and he decided to ask galactica tell me about uh brandolini's bullshit uh bullshit asymmetry principle and you know what it came up with and this is the real answer here's what it came up brandolini's law is a theory in economics proposed by giannani brandolini a professor at the university of padua which takes the smaller the economic unit the greater its efficiency almost nothing here is correct it's basically bullshitting the bullshit principle so to me this encapsulates one of the biggest problems with this technology it bullshits and it can do this at scale and this could be put in the wrong hands and this can also just add more noise to an information environment that has plenty of noise we need to to clean up that polluted information environment not add noise and that's what a lot of these chatbots will do definitely because they make they they put a lot of accurate things out there but they also put a lot of false things and this is the best encapsulation that uh i i've seen so far uh which is actually bullshitting the bullshit principle so that's a problem so as i mentioned uh in this um seattle uh this op-ed i talk about some of these things um and one of the things i talk about as well is that not only do these things can these bullshit at scale um they also can get in the way of democratic discourse and and many years ago back in 2018 there was a lot of attention around facebook's role in pushing misinformation excuse me and they revealed at the time when they were being when mark zuckerberg was being questioned by congress about all the fake accounts and they admitted they had disabled 1.3 billion fake accounts now they certainly haven't solved that problem just like no social media platform has solved that in fact when elon musk was taking over twitter that was one of the big issues at hand that there was all sorts of concerns if there was lots and lots of bots on twitter now that's still a problem i can guarantee you you know my group has done a little bit of work working uh in detecting and looking the effects of bots but my colleagues in in my research area have done a lot of work in that space and it's very very hard to do but one thing is that you know we do know is that there are a lot of bots out there and a lot of fake counts now imagine those fake counts with the ability that these chatbots now have to look even more human technology that's basically well has passed the turing test that to me is problematic because of these different reasons so you have these large number of fake accounts now imagine these fake accounts being scaled to conversations with our public officials now democracies depend on an engagement with the public with their officials that they voted in well that becomes a problem like it was back in 2017 when there was discussions at least in the united states around something called net neutrality and net neutrality was um a policy that was being debated in governmental circles and they want to know to know what the public was saying but when you went to what the public was saying they had complete the there were comments that it completely flooded the conversation on one side of the issue and it turns out that they were essentially fake accounts there were bots and again now imagine doing that with the sophistication that these new chatbots have before you could there was a you could start you could detect a bot much easier now it's become even harder and if if our democratic systems are flooded with these kinds of things this is this is problematic so this issue uh in its potential impact on democratic discourse and its um ability to bullshit at scale is of major concern to me so i spent a lot of my time like my colleagues in our center in sort of the darker corners of the internet studying the ways in which misinformation and disinformation spreads online and one thing that we are of course very concerned about now are the ways in which these technologies can can really further inflame or further fan the flames of discourse uh in groups if these get in the wrong hands and they certainly will i it's almost certain that bad actors are finding ways to use this technology um for their own ends and we know that you know you know our surgeon general and other major leaders around the world have recognized the ways in which misinformation can affect our health and they can affect the health of democracies so it's not just that oh well it's annoying there's a lot of false information online it just makes it hard to find you know good information it's not just that it actually affects people's health and we're recognizing that and at least well we've recognized it and now we've got another problem ahead of us which is the ability of these technologies to create deep fake images video audio text that's the the challenge we have ahead of us so as dr un had mentioned we have a center at the university of washington uh in seattle in the united states where we study this and this is one of the issues that we study and we study these things on social media platforms and we look at the way that individuals and organizations get amplified but we're also going to start looking at the ways that these bots and synthetically created content also get amplified so we do this through all sorts of different channels research is our main thing but we also do it through policy and education and community engagement and one of the things that my colleague carl bergstrom and i created several years ago to bring public attention to synthetic media to deep fakes was to create a game that we called which face is real.com and it was a simple game we just asked the users which image is real one of them is a real image of a real person on this earth and another one was synthetically created with computers and you can go through the you know thousands and thousands and thousands of images and we play this game and then you get told whether it's real or not and it turns out it's pretty hard i mean you know there's some that are kind of obvious but they're hard like this one right here which one's real look at it for a second well the one that's real is the one with the blue shirt at least on my right um and if you said the one on the red shirt totally understand it's really hard to tell the difference and the reason why we created this game was just to bring public attention because the the the time in a technology's birth that's i think most scary when it comes to its potential impact is when the public is not aware of the things that it can do so we created this game and we've had millions and millions of plays of this game um and we've now seen actually this technology you know get used and and you know of course good ways but a lot of bad ways too for example we've seen this technology be used to create fake journalists this is an example and you can read more about it but this has happened many times where journalists also so-called journalists um uh you know the profiles created and then there's these images and people that study deep fake imagery can actually look at these images and start to see what some look for some of these telltale signs on what's real or not so these this technology has been been already used and we talked about this when this game was created the ways in which it's being used and we've already seen it of course many ways so now we're asking how are the ways in which this new uh generative technology could be used but it's very similar it's based on similar kinds of concepts and and data training etc etc but um we're now asking the same things and of course we've seen this now just recently with this kind of technology being used in videos and even in ways that are more sophisticated than just these these sort of portrait pictures so this was an image it's a fake image and it's been reported all over bbc and lots of other places of donald trump supposedly being arrested in new york this was before he was actually um brought to new york on um the recent case but this of course never happened but it certainly sparked all sorts of concern and it spread like wildfire on the internet using um some of this you know mid-journey technology and our company like called mid-journey a lot of other companies that are creating this so it's making it easier and easier and less expensive and making it harder for us to tell what's real and so that technology is of course evolved um so okay so now i've talked about some of the democratic discourse i'm going to go through a little quicker on these other ones so then i can get to sort of the end and try to finish um in about i would say about seven minutes or so so like i mentioned at the beginning there's this issue of content creation and that's really important because these content creators um are generating a lot of the content that a lot of these technology companies are using to create these chatbots but we've seen this story before and by the way just recently there are major technology companies that are now not allowing the scraping of this data to be trained for these technologies unless they're compensating i think that's a fair thing so you know some of these big companies like reddit is not allowing stack overflow is now saying hey if you're going to scrape our data they want to be compensated and so you're going to start to see content creators starting to push back which is good because like i said we've seen this story before oh and by the way this is an example of uh you know getty images for it isn't a lawsuit right now um with a company that uh may have been scraping their data um uh illegally and the reason why that came out is because you can see this little getty images that pops up which is a watermark the getty images puts on their images so there's a pending lawsuit about this and that could determine some of the um some of how this content can or can't be uh scraped but we've seen this before uh when looking at new technology and the impact it can have on other information producers in the united states and in many places around the world we we have growing news deserts these are areas where there's no more local news and that's pretty devastating for democratic discourse for democracies because we depend on local news uh for engagement civic engagement and quality information and local news tends to be trusted more than national news and there's all sorts of reasons for that happening but certainly one element of that um is the effect that search engines and google in particular around it you know the way it sells ads and it takes a large cut at those ads um has potentially contributed to these news deserts of course there's lots of other things as well but but that technology everyone was excited about including myself and we use it all the time but there are these unintended effects that can affect other aspects of our information ecosystems and right now there's a there's a lawsuit going on the justice department united states justice department is suing google for monopolizing digital advertising technologies and one of those elements of this lawsuit story is is the it's the sort of increase in the demise of local news so there's all sorts of interesting things playing out right now and then the biden administration united states is thinking about you know doing some uh uh you know uh you know regulation of ai but they don't know you know it's so new to you know all administrations that it's hard to figure out like i mentioned italy has gone probably one of the furthest steps i think around this uh but there hasn't been a lot of action at least on the u.s side when it comes to the um to this um sort of thing and so you know these uh you know there's been letters so this was an article written by time i the image is quite uh quite nice it actually grows uh there's was a letter that went around from a bunch of influential people in ai and business leaders and technology including elon musk that says hey we should stop the development until we've had more time to think about although it's kind of ironic given that many of these tech leaders are still of course pushing their own development of uh ai in many ways and creating new ai companies and you know developing but anyway that's another story but but that is something to think about i don't think we're i don't think it's a you know i think there's no way that that that letter is going to like you know stop the technology from being developed but one thing it is good about is it's making hopefully forcing the public and journalists and government officials etc to start to think about the ways in which this technology uh could affect society we should think about it as i mentioned one of the concerns is is job demise and i think there's a lot of concern with that by those that even work in the job industry the pew research center recently at you know asked workers where you know are they concerned turns out a large number of you know workers from across these different industries are concerned um and uh there's probably good reason for this concern even sam altman at open ai which is sort of or is the owner of like chat gpt which is the you know one of the more well-known chatbots out there and and uh you know it's also the one that microsoft has invested you know billions um this this this is something that they've even said probably lose five million jobs you know i mean so these are these are real concerns so um the other thing to to mention too is that it's not just uh you know within science itself these are real concerns and and we've seen the proliferation of pseudoscience and the rise of predatory journals and content that maybe looks like science but it's not and i'll show you this is an example from many years ago this was a paper that was published in this international conference on atomic and nuclear physics and you look at the title atomic energy will be made available to a single source and you read the abstract and if you look at the abstract it doesn't make a lot of sense well it turns out that when this was created it was created by a person to make a point about how poor some of these journal venues and conference venues are this paper was made with autocomplete using an iphone and this is not all that different from this this you know chatbots and chat gpt which is really you know these autocomplete machines on steroids um and it looks sort of official like a real science paper but of course it was a lot of it was nonsense and the newest ones are you know much better but it is a concern that this could you know you know um increase the number of pseudoscience types of things or articles that are written by chatbots in fact there's been some articles that have been co-authored by chat gpt although some journal a lot of journals and publishers are saying they won't allow that anymore and that's probably a good thing and i will say this there's been lots of talk about the hallucination of these chatbots well one thing that i find incredibly problematic is the hallucination of citations um it'll make up citations all the time and we've been doing some work trying to find ways to to see when this happens it even happened my colleague uh carl found these examples of papers supposedly by me well these aren't real citations by me it shows my uh me right here west jones this is not uh my paper it's close to some of the papers i've written in terms of title but it's just a fake citation i just made up these citations and that's again another example of how that could affect the citation record and scholarly literature and something that i'm concerned about for science and something that's now happened with some of these chatbots even in bing now that has sort of integrated chat gpt4 into bing is that it throws references so it looks official but i can tell you that this was not done before the fact this is this knowledge that was written out in this answer when i asked about new tax laws for electric vehicles this is one question i asked that um it's not that it throwed you know learn something from citations and then cited them like we do in the scholarly literature it cited them post hoc something that's probably semantically similar although maybe scholars scholars probably do this too well they do do it post after posting uh you know some argument or sentence but this this is problematic to me because it looks like it's a science almost science and you know sciency and technical and it's really not even though it has those references so um you know there's been lots of discussion in the scientific literature what to do with these chatbots some have listed you know authors on papers publishers have come out even publishers in the machine learning world so icml was you know one of the first conferences not i don't think it was the first but one of the among the first that was saying you can't use chat gpt on this uh you know some are saying if you use it you have to note it you know all sorts of different policies are being created but overall you know i think the publishing community and the scholarly literature is going to have to grapple with this and i and my colleague and i have carl and i have a a another op-ed that were that we write about sort of what publishers can do around this particular issue now one of the other big concerns of this is of course that this technology shows you know it doesn't it's always 100 confident whether it's right or whether it's wrong that's really problematic at least when humans communicate they have these qualifiers of confidence where you can say something like i'm pretty sure or i i think so or i i think that's right you know these things are important and these chatbots don't have it they just are always they always seem to be well not seem they they spew out things whether they're right or wrong with 100 confidence and that's dangerous um and if there's you know one paper one of my colleagues emily bender has this great feature where um uh in this paper that she talks about and also in this feature um in uh in the new yorker or in this new york magazine uh they talk uh it talks a lot about some of the current concerns emily and other people that are true linguists that truly understand some of the problems with these technologies um this is one that i i would recommend reading and a lot of you may have heard of this early interview that a new york times uh journalist had with uh this you know with bing's chatbot um and it what essentially happened was the chatbot kind of went off the rails and started to say well you should leave your wife and i want to take over the world and all these things happen and of course a lot of the developers of this technology go whoa whoa whoa we need to fix it and there's been new rules around how you can use this bing chatbot and other chatbots but the problem with these fixes is they're like band-aids there's really it's very difficult to reverse engineer these problems or reverse engineer yeah reverse engineer these issues it's not like you can go to a line of code and say oh that's where the problem is um it's the way that these things are trained the way these models work with their bazillions of parameters makes it very difficult to reverse engineer and that's problematic when we run into these issues and all the issues we haven't even thought of um and so that to be is another big concern of course there's all sorts of jailbreaks to get these chatbots to do things that there have been band-aids put around and that's problematic and i won't go over all the jailbreaks you can read about because i don't want to get those out uh too much to the public but you can read about them it's not like they're that hidden um and that's problematic uh and there'll be many many others as well so these are some of the the ones i had some of these concerns reverse engineering the last two i'll mention is the cost um and the the cost we talked about jobs the costs potentially to all different aspects of society and this and the kind of scary thing is right now is that a lot of the tech layoffs are removing these teams they're removing other individuals as well but they're removing individuals that are on ethics and safety teams and of all times to have these kinds of teams at these companies it would be now and certainly maybe if you went back in history at the beginning of uh the rise of social media but they're being eliminated and this is a concern but there's also other kinds of costs that a lot of people forget and that's the cost of these queries there's been several analyses that have come out recently about the cost per query for running some of these uh chatbots and it's like almost an order of magnitude greater than a regular query that you'd have let's say on a google search and that has a cost and there's also costs of investing billions and millions of dollars in this technology that could also go to other kinds of investments that might be helping society so there's these costs that we have to think about the environmental costs you know the cost to society the costs of adding more you know pollution into our information systems etc there's also the issue um of garbage in garbage out and that that's a problem we're likely going to see as these chatbots generate more and more content that land online that becomes the training data for that those chatbots in you know chat you bt5 and chat gpt6 and what those effects will be uh requires some more research and thinking but the main thing that we've known in a lot for a long time of course in the machine learning world is that garbaging with your training data creates garbage out and even if you didn't have this you know feed forward a loop that i just mentioned there's a lot of garbage on the internet and that garbage finds its way into conversations with these chatbots and that's always a concern whether you're talking about science or this technology just generative ai technology more broadly so i'll end by saying that this technology is in many cases amazing i have mostly focused on some of my cautions and concerns like many people have especially in my world as someone who studies misinformation and misinformation specifically in science and its effect on the institution of science um uh and so uh and so these are things that we're going to have to you know pay attention to going forward um but i think like a lot of our new technology we just need time to think about it and run seminars and workshops and conferences just like this so hopefully this generates some conversation and hopefully we can uh sort out some of these cautions so with that i'll end you can reach out to me you can learn more about my book and the research we do uh in my lab and at our university and in my our center and you can um uh reach me in these ways\", metadata={'source': './data/transcript.txt'})",
    "crumbs": [
      "사례",
      "유튜브 동영상"
    ]
  },
  {
    "objectID": "yt_openai.html#인메모리-벡터스토어",
    "href": "yt_openai.html#인메모리-벡터스토어",
    "title": "",
    "section": "인메모리 벡터스토어",
    "text": "인메모리 벡터스토어\nDocArray는 다양한 형태의 데이터를 관리할 수 있는 오픈소스 도구로, 랭체인과 연동하여 강력한 AI 애플리케이션을 만들 수 있다. 예를 들어, OpenAI의 Whisper 모델을 사용하여 유튜브 동영상의 오디오를 텍스트로 변환한 경우, DocArray를 활용해 이 텍스트 데이터를 효율적으로 저장하고 검색할 수 있다. 또한 DocArrayRetriever를 통해 랭체인 애플리케이션에서 문서 데이터를 쉽게 활용할 수 있어, 유튜브 콘텐츠 기반의 질의응답 시스템이나 콘텐츠 추천 엔진 등 다양한 응용 프로그램을 개발할 수 있다.\n\n코드import tiktoken\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.vectorstores import DocArrayInMemorySearch\n\ndb = DocArrayInMemorySearch.from_documents(docs, OpenAIEmbeddings())\n\nretriever = db.as_retriever()\n\nllm = ChatOpenAI(temperature=0.0)\n\nqa_stuff = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\",\n    retriever=retriever,\n    verbose=True\n)",
    "crumbs": [
      "사례",
      "유튜브 동영상"
    ]
  },
  {
    "objectID": "yt_openai.html#질의",
    "href": "yt_openai.html#질의",
    "title": "",
    "section": "질의",
    "text": "질의\n실제 질의를 통해 사실관계를 확인해보자.\n\n코드query = \"what is misinformation\"\n\nresponse = qa_stuff.invoke(query)\n\nresponse\n\n\n&gt; Entering new RetrievalQA chain...\n\n&gt; Finished chain.\n{'query': 'what is misinformation',\n 'result': 'Misinformation refers to false or inaccurate information that is spread, often unintentionally, leading to misunderstandings or misconceptions. It can be shared through various mediums such as social media, news outlets, or word of mouth. Misinformation can have negative impacts on individuals, communities, and society as a whole by influencing beliefs, decisions, and behaviors based on incorrect information.'}\n\n코드query = \"There have been incidents due to misinformation.?\"\n\nresponse = qa_stuff.invoke(query)\n\nresponse\n\n\n&gt; Entering new RetrievalQA chain...\n\n&gt; Finished chain.\n{'query': 'There have been incidents due to misinformation.?',\n 'result': \"Yes, incidents due to misinformation have been a significant concern in various fields, including science, politics, health, and society in general. Misinformation can lead to confusion, mistrust, and even harm. In the context of generative AI and chatbots, the potential for misinformation to spread rapidly and at scale is a growing concern. Researchers like Jebin West are studying the impact of misinformation and disinformation in the digital world, particularly in the context of new technologies like generative AI. The spread of false information can have serious consequences, affecting democratic discourse, public health, and the credibility of information sources. It's essential to address and combat misinformation to promote an informed society and strengthen democratic discourse.\"}",
    "crumbs": [
      "사례",
      "유튜브 동영상"
    ]
  },
  {
    "objectID": "index.html#ai-엔지니어",
    "href": "index.html#ai-엔지니어",
    "title": "",
    "section": "AI 엔지니어",
    "text": "AI 엔지니어\nAI 엔지니어(AI Engineer)는 API를 기준으로 오른쪽에 위치하여 데이터/연구 중심의 역할과 제품/사용자 중심의 역할 사이를 효과적으로 연결한다. 주요 업무로는 체인/에이전트 개발과 도구 및 인프라 구축에 집중하며, ML 엔지니어나 데이터 과학자보다 더 제품 지향적이면서도 풀스택 엔지니어보다는 AI 기술에 특화된 역량을 보유한다. AI 엔지니어는 AI 모델과 기술을 실제 제품이나 서비스로 구현하는 데 중점을 두어, 연구 결과를 실용적인 애플리케이션으로 변환하는 핵심적인 역할을 수행한다. (SWYX & ALESSIO, 2023)\n\n\n\nAI 엔지니어",
    "crumbs": [
      "기본기",
      "AI 공학"
    ]
  },
  {
    "objectID": "index.html#소프트웨어-3.0",
    "href": "index.html#소프트웨어-3.0",
    "title": "",
    "section": "소프트웨어 3.0",
    "text": "소프트웨어 3.0\n소프트웨어 개발 패러다임은 1.0, 2.0, 3.0으로 진화했다. 소프트웨어 1.0은 전통적인 프로그래밍 방식을 대표한다. 개발자가 모든 로직과 규칙을 명시적으로 코딩하며, 문제 해결을 위한 알고리즘을 직접 설계하고 구현한다. 프로그래머의 논리적 사고와 코딩 능력이 핵심이다.\n소프트웨어 2.0은 머신러닝과 인공신경망을 활용한다. 데이터를 기반으로 모델이 학습하고 패턴을 찾아내어 문제를 해결한다. 개발자는 모델 구조를 설계하고 학습 과정을 관리하는 역할을 맡는다. 데이터의 품질과 양, 효과적인 학습 알고리즘이 성능을 좌우한다.\n소프트웨어 3.0은 대규모 언어 모델(LLM)과 같은 고급 AI 시스템을 활용한 새로운 패러다임이다. 추론과 생성 능력을 갖춘 AI를 활용하여 더 복잡하고 다양한 문제를 해결한다. 개발자는 AI 모델과 상호작용하며 고수준의 지시를 제공하고, AI는 이를 바탕으로 코드를 생성하거나 복잡한 작업을 수행한다. 이 방식은 이전 버전들보다 훨씬 더 광범위한 문제 영역을 다룬다.\n\n\n\n소프트웨어 3.0\n\n\n\n\n\n\n\n\n\n\n\n특성\n소프트웨어 1.0\n소프트웨어 2.0\n소프트웨어 3.0\n\n\n\n\n핵심 접근법\n명시적 프로그래밍\n머신러닝/인공신경망\n대규모 언어 모델(LLM)/AI\n\n\n개발자 역할\n알고리즘 설계 및 구현\n모델 구조 설계 및 학습 관리\nAI와 상호작용, 고수준 지시 제공\n\n\n문제 해결 방식\n직접 로직 코딩\n데이터 기반 학습 및 패턴 인식\nAI 기반 추론 및 생성\n\n\n핵심 역량\n논리적 사고, 코딩 기술\n데이터 처리, 모델 최적화\nAI 이해/활용, 프롬프트 공학\n\n\n성능 좌우 요소\n코드 품질, 알고리즘 효율성\n데이터 품질/양, 학습 알고리즘\nAI 모델 성능, 프롬프트 품질\n\n\n적용 범위\n명확히 정의된 문제\n패턴 인식, 예측 가능한 문제\n복잡하고 다양한 문제\n\n\n개발 과정\n순차적, 명시적\n반복적, 데이터 중심\n대화형, AI 협업\n\n\n유지보수\n직접 코드 수정\n모델 재학습, 파라미터 조정\n프롬프트 개선, AI 모델 업데이트\n\n\n장점\n정확성, 투명성\n복잡한 패턴 처리 능력\n높은 유연성, 빠른 개발\n\n\n단점\n복잡한 문제에 대한 확장성 제한\n대량의 데이터 필요, 설명 어려움\n결과 예측 어려움, 윤리적 고려사항\n\n\n\n\n애자일과 CRISP-DM\nAgile 방법론은 소프트웨어 개발을 위한 반복적이고 증분적인 접근 방식이다. 짧은 개발 주기(스프린트)를 통해 빠른 피드백과 변화에 대한 유연한 대응을 강조한다. 고객과의 긴밀한 협력, 작동하는 소프트웨어의 지속적인 제공, 그리고 팀의 자율성을 중요시한다.\nCRISP-DM은 데이터 마이닝과 머신러닝 프로젝트를 위한 구조화된 접근 방식이다. 비즈니스 이해부터 배포까지 6단계로 구성되며, 각 단계는 순환적이고 반복적으로 수행될 수 있다. 데이터의 이해와 준비에 많은 비중을 두며, 비즈니스 목표와 데이터 과학적 방법론을 긴밀히 연계한다.\n\n\n\n\n\n\n\n\n특성\nAgile\nCRISP-DM\n\n\n\n\n주요 목적\n소프트웨어 개발\n데이터 마이닝/머신러닝\n\n\n프로세스 구조\n반복적 스프린트\n6단계 순환 프로세스\n\n\n주기 길이\n2-4주의 짧은 스프린트\n각 단계별로 다양함\n\n\n핵심 산출물\n작동하는 소프트웨어\n데이터 인사이트, 예측 모델\n\n\n팀 구성\n크로스 펑셔널 소규모 팀\n다양한 전문가 (데이터 과학자, 비즈니스 분석가 등)\n\n\n고객 참여\n지속적이고 긴밀함\n주로 초기와 평가 단계에 집중\n\n\n변경 관리\n매우 유연함\n구조화되어 있으나 단계 간 반복 가능\n\n\n품질 관리\n지속적인 통합과 테스팅\n데이터 품질, 모델 성능, 비즈니스 가치 평가\n\n\n주요 강조점\n빠른 제품 출시와 적응\n데이터 이해와 비즈니스 가치 창출\n\n\n적용 분야\n주로 소프트웨어 개발\n데이터 분석, 예측 모델링 프로젝트\n\n\n\n\n\n소프트웨어 3.0 개발방법론\n소프트웨어 3.0 개발방법론은 아직 공식적으로 정립되지 않았지만, 새로운 패러다임의 특성을 반영한 몇 가지 emerging practices와 원칙들이 존재한다.\nAI-first 설계를 통해 제품 설계 단계부터 AI 기능을 핵심으로 고려한다. 프롬프트 엔지니어링은 LLM과의 효과적인 상호작용을 위해 중요한 역할을 한다. 개발자들은 반복적 AI 튜닝 과정을 통해 모델의 출력을 지속적으로 평가하고 개선한다.\n데이터 큐레이션은 모델의 성능 향상을 위해 필수적이며, 고품질 데이터를 선별하고 관리하는 작업이 포함된다. 윤리적 AI 개발을 통해 AI의 편향성, 공정성, 투명성 등을 고려한 개발 프로세스를 구축한다.\n하이브리드 인간-AI 협업 모델을 통해 인간 개발자와 AI 시스템의 강점을 결합한다. 지속적 학습 및 적응 원칙에 따라 배포 후에도 모델의 성능을 모니터링하고 업데이트한다.\n멀티모달 AI 통합을 통해 텍스트, 이미지, 음성 등 다양한 모달리티를 처리할 수 있는 AI 시스템을 개발한다. 확장 가능한 AI 인프라 구축에 중점을 두어 대규모 AI 모델을 효율적으로 운영하고 확장한다.\n마지막으로, AI 안전성 및 견고성을 보장하기 위해 예측 불가능한 상황에서도 안정적으로 작동할 수 있는 시스템을 개발한다.",
    "crumbs": [
      "기본기",
      "AI 공학"
    ]
  },
  {
    "objectID": "index.html#개발-작업흐름",
    "href": "index.html#개발-작업흐름",
    "title": "",
    "section": "개발 작업흐름",
    "text": "개발 작업흐름\n“발사하고, 준비하고, 조준한다(Fire, ready, aim)”는 접근법은 AI 개발 프로세스를 혁신적으로 변화시켰다. 기존에는 데이터 과학자나 ML 엔지니어가 특정 도메인 모델을 훈련시키기 위해 먼저 광범위한 데이터 수집 작업을 수행해야 했다. 이는 시간과 자원을 많이 소모하는 과정이었다. 그러나 이제는 제품 관리자나 소프트웨어 엔지니어가 대규모 언어 모델(LLM)을 활용하여 즉시 제품 아이디어를 구축하고 검증할 수 있게 되었다. 이들은 LLM에 프롬프트를 입력하여 빠르게 프로토타입을 만들고 아이디어의 실현 가능성을 평가한다. 이후 제품 컨셉이 검증되면, 그때 필요한 특정 데이터를 수집하여 모델을 미세 조정한다. 이 방식은 개발 주기를 크게 단축시키고, 더 빠른 혁신과 시장 반응 확인을 가능하게 한다.\nAI 엔지니어의 역할은 LLM(대규모 언어 모델) 기반 AI 개발 과정에서 혁신적인 변화를 가져왔다. 이들은 전통적인 ML 접근법과는 다르게 작업한다. AI 엔지니어는 먼저 제품 아이디어를 신속하게 구현하고 검증하는 것으로 시작한다. 이를 위해 기존의 강력한 LLM을 활용하여 초기 프로토타입을 만들고 사용자 피드백을 수집한다.\n제품이 성공 가능성을 보이면, AI 엔지니어는 그 다음 단계로 넘어간다. 제품에 특화된 데이터를 수집하고 정제한다. 제품에 특화된 데이터는 초기 프로토타입을 통해 얻은 인사이트를 바탕으로 선별된다.\n마지막으로, 제품의 규모를 확장해야 할 필요가 있을 때, AI 엔지니어는 수집된 데이터를 사용하여 맞춤형 모델을 개발하거나 기존 LLM을 파인튜닝하여 제품의 성능을 향상시키고 특정 사용 사례에 더 잘 맞도록 최적화하는 데 중점을 둔다.\n\n\n\n제품, 데이터, 모델 작업흐름",
    "crumbs": [
      "기본기",
      "AI 공학"
    ]
  },
  {
    "objectID": "ollama.html#라마3",
    "href": "ollama.html#라마3",
    "title": "올라마",
    "section": "라마3",
    "text": "라마3\n라마(Llama)는 메타(Meta)에서 2023년 2월에 처음 공개한 대규모 언어 모델이며, 라마2는 2023년 7월에 출시된 라마의 개선 버전으로, 오픈 소스로 공개되어 연구 및 상업적 용도로 사용할 수 있다. 라마3는 2024년 6월 출시되었으며 128,256 토큰을 보유한 새로운 토크나이저를 장착했으며, 8,192 컨텍스트 길이를 갖고 15조개(15T)토큰 학습 데이터를 사용하여 성능을 크게 높였고, Supervised fine-tuning (SFT) Rejection Sampling, Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO) 알고리즘을 적용하였다.\n\n\n\n\n\n\n\n\n\n\n\nLlama 모델은 GGUF(GPT-Generated Unified Format) 파일 형태로 제공되며, 모델 가중치(학습된 파라미터 값)와 모델 아키텍처(신경망의 구조와 레이어)를 포함하고 있다. llama.cpp는 C++로 작성된 Llama 모델의 추론 엔진으로 Llama 모델의 아키텍처를 C++ 코드로 구현되어 있으며 GGUF 파일에서 가중치를 읽어 메모리에 로드하고, 입력 텍스트에 대해 모델의 추론을 실행한다. 즉, llama.cpp는 Llama 모델을 “실행”하는 엔진으로 모델 파일(GGUF)은 “무엇을” 계산할지를 정의하고, llama.cpp는 “어떻게” 계산할지를 구현한다.",
    "crumbs": [
      "데이터베이스",
      "올라마"
    ]
  },
  {
    "objectID": "ollama.html#헬로월드",
    "href": "ollama.html#헬로월드",
    "title": "올라마",
    "section": "헬로월드",
    "text": "헬로월드\nCMake를 설치하고 llama-cpp-python 패키지를 설치한다.\n#| eval: false\npip install llama-cpp-python --prefer-binary\n허깅페이스 QuantFactory/Meta-Llama-3-8B-GGUF 에서 Meta-Llama-3-8B.Q8_0.gguf 파일을 다운로드 받아 사용한다.\n\n\n코드\nfrom llama_cpp import Llama\n\npath_to_model = \"data/Meta-Llama-3-8B.Q8_0.gguf\"\nllm = Llama(model_path=path_to_model)\noutput = llm(\n    \"Why is the sky blue?\",\n)\nprint(output[\"choices\"][0][\"text\"])\n\n\nllama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from data/Meta-Llama-3-8B.Q8_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = .\nllama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                          llama.block_count u32              = 32\nllama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  12:                          general.file_type u32              = 7\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q8_0:  226 tensors\nllm_load_vocab: missing pre-tokenizer type, using: 'default'\nllm_load_vocab:                                             \nllm_load_vocab: ************************************        \nllm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \nllm_load_vocab: CONSIDER REGENERATING THE MODEL             \nllm_load_vocab: ************************************        \nllm_load_vocab:                                             \nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.8000 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q8_0\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \nllm_load_print_meta: general.name     = .\nllm_load_print_meta: BOS token        = 128000 '&lt;|begin_of_text|&gt;'\nllm_load_print_meta: EOS token        = 128001 '&lt;|end_of_text|&gt;'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '&lt;|eot_id|&gt;'\nllm_load_print_meta: max token length = 256\nllm_load_tensors: ggml ctx size =    0.14 MiB\nllm_load_tensors:        CPU buffer size =  8137.64 MiB\n.........................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\nllama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \nModel metadata: {'general.name': '.', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '&lt;|start_header_id|&gt;' + message['role'] + '&lt;|end_header_id|&gt;\\n\\n'+ message['content'] | trim + '&lt;|eot_id|&gt;' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n' }}\"}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '&lt;|start_header_id|&gt;' + message['role'] + '&lt;|end_header_id|&gt;\n\n'+ message['content'] | trim + '&lt;|eot_id|&gt;' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n' }}\nUsing chat eos_token: &lt;|end_of_text|&gt;\nUsing chat bos_token: &lt;|begin_of_text|&gt;\n\nllama_print_timings:        load time =    1171.92 ms\nllama_print_timings:      sample time =       1.65 ms /    16 runs   (    0.10 ms per token,  9702.85 tokens per second)\nllama_print_timings: prompt eval time =    1171.86 ms /     6 tokens (  195.31 ms per token,     5.12 tokens per second)\nllama_print_timings:        eval time =    2254.84 ms /    15 runs   (  150.32 ms per token,     6.65 tokens per second)\nllama_print_timings:       total time =    3450.85 ms /    21 tokens\n\n\n Why is the grass green? Why does the sun shine? These are simple questions",
    "crumbs": [
      "데이터베이스",
      "올라마"
    ]
  },
  {
    "objectID": "ollama.html#세가지-방법",
    "href": "ollama.html#세가지-방법",
    "title": "올라마",
    "section": "세가지 방법",
    "text": "세가지 방법\nLLaMA 모델을 사용하는 세 가지 주요 방법은 허깅페이스 Transformers, Ollama, llama-cpp-python이다. 허깅페이스 Transformers는 클라우드 호스팅 모델을 사용하여 쉽게 접근할 수 있다. Ollama는 로컬 서버를 통해 최적화된 모델을 실행한다. llama-cpp-python은 C++로 구현된 LLaMA의 Python 바인딩을 사용하여 로컬에서 효율적으로 모델을 실행한다.\n\n\n\n\n\n\n\n\n\n방법\n장점\n단점\n사용 사례\n\n\n\n\nHugging Face Transformers\n- 쉬운 사용- 다양한 모델 지원- 클라우드 리소스 활용\n- 인터넷 연결 필요- 대용량 데이터 전송 시 비용 발생 가능\n- 빠른 프로토타이핑- 다양한 모델 실험- 클라우드 기반 애플리케이션\n\n\nOllama\n- 로컬 실행으로 빠른 응답- API를 통한 간편한 사용- 커스텀 모델 지원\n- 초기 설정 필요- 로컬 리소스 사용\n- 개인 프로젝트- 로컬 애플리케이션 개발- 데이터 프라이버시 중요 사례\n\n\nllama-cpp-python\n- 매우 효율적인 로컬 실행- 낮은 메모리 사용량- 커스텀 모델 지원\n- 모델 파일 관리 필요- 초기 설정 복잡할 수 있음\n- 리소스 제한적인 환경- 임베디드 시스템- 고성능 요구 애플리케이션",
    "crumbs": [
      "데이터베이스",
      "올라마"
    ]
  },
  {
    "objectID": "ollama.html#라마3.1",
    "href": "ollama.html#라마3.1",
    "title": "올라마",
    "section": "라마3.1",
    "text": "라마3.1\n\n허깅페이스\n허깅페이스에서 라마3 모형을 다운로드 받기 위해서 먼저 huggingface-cli를 설치한다. 다음으로 쉘에서 huggingface-cli login을 통해 토큰 https://huggingface.co/settings/tokens을 생성한 후 로그인하여 인증한다. meta-llama/Meta-Llama-3.1-8B-Instruct에서 라이선스 동의 절차를 밟는다. 라이선스 승인이 완료되면 huggingface-cli download 명령어로 허깅페이스 클라우드에서 라마3 모형을 다운로드 받아 로컬 디렉토리에 설치하게 된다.\n#| eval: false\npip install -U \"huggingface_hub[cli]\"\n\nhuggingface-cli login\n\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir D:\\llms\\Meta-Llama-3.1-8B-Instruct\npip install --upgrade transformers 를 최신 버전으로 업그레이드한다.\n#| eval: false\npip install --upgrade transformers\npip install accelerate  # Using `low_cpu_mem_usage=True` or a `device_map`         \n\n\n코드\nfrom accelerate import Accelerator\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load tokenizer and model\nmodel_path = \"D:/llms/Meta-Llama-3.1-8B-Instruct/original\"\n# model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # 또는 다른 LLaMA 모델 이름\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n\n# 간단한 프롬프트 생성\nprompt = \"안녕하세요, LLaMA입니다. 오늘의 날씨는\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# 모델 추론\noutputs = model.generate(**inputs, max_length=50)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(response)\n\n\n\n\nllama-cpp-python\n라마 3.1을 405B, 70B, 8B 중 하나를 용도에 맞춰 다운로드한다.\n\n\n코드\n# llama-cpp-python \nfrom llama_cpp import Llama\n\ndef load_llama_cpp():\n    # 모델 파일 경로 (실제 경로로 변경 필요)\n    model_path = \"path/to/your/gguf/model.gguf\"\n\n    # LLaMA 모델 로드\n    llm = Llama(model_path=model_path)\n\n    # 프롬프트 생성 및 추론\n    prompt = \"안녕하세요, LLaMA입니다. 오늘의 날씨는\"\n    output = llm(prompt, max_tokens=50)\n\n    print(output[\"choices\"][0][\"text\"])\n\n\nif __name__ == \"__main__\":\n    print(\"\\nUsing llama-cpp-python:\")\n    load_llama_cpp()\n\n\n\n\n올라마\nollama run llama3.1:8b 명령어로 라마3 모형을 설치한 후 올라마를 사용해서 로컬에서 오픈소스 버전 LLM을 사용할 수 있다.\n#| eval: false\nollama run llama3.1:8b\n\n\n코드\nimport requests\nimport json\n\n\ndef generate_text(prompt):\n    response = requests.post(\n        \"http://localhost:11434/api/generate\",\n        json={\"model\": \"llama3.1:8b\", \"prompt\": prompt},\n        stream=True,\n    )\n\n    text_response = \"\"\n    for line in response.iter_lines():\n        if line:\n            json_response = json.loads(line)\n            if \"response\" in json_response:\n                text_response += json_response[\"response\"]\n            if json_response.get(\"done\", False):\n                break\n\n    return text_response.strip()\n\n\n# Usage example\nprompt = \"한글로 답을 해. 축구를 가장 잘하는 나라는 어디야?\"\nresponse = generate_text(prompt)\nprint(\"Prompt:\", prompt)\nprint(\"Response:\", response)\n\n\n\n\n코드\nfrom langchain_community.llms import Ollama\n\nllm = Ollama(model=\"llama3.1:8b\")\nllm.invoke(\"한글로 답을 해. 축구를 가장 잘하는 나라는 어디야?\")",
    "crumbs": [
      "데이터베이스",
      "올라마"
    ]
  },
  {
    "objectID": "hf.html",
    "href": "hf.html",
    "title": "",
    "section": "",
    "text": "LLM허깅페이스 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "LLM",
      "허깅페이스"
    ]
  },
  {
    "objectID": "hf.html#버킷-생성",
    "href": "hf.html#버킷-생성",
    "title": "",
    "section": "버킷 생성",
    "text": "버킷 생성\n\nGCS\n\nCloud Console 열기: Google Cloud Console로 이동합니다.\n프로젝트 선택: 버킷을 만들 프로젝트를 선택합니다.\nCloud Storage로 이동: 네비게이션 메뉴에서 “Cloud Storage” &gt; “Browser”를 선택합니다.\n버킷 생성:\n\n“Create bucket” 버튼을 클릭합니다.\n버킷 이름을 입력합니다. 버킷 이름은 전 세계에서 유일해야 합니다.\n버킷 위치를 선택합니다. 필요에 따라 특정 지역, 다중 지역 또는 이중 지역을 선택할 수 있습니다.\n기본 스토리지 클래스를 선택합니다. 이는 저장된 데이터의 비용 및 가용성에 영향을 미칩니다.\n접근 제어를 설정합니다. “Uniform” 또는 “Fine-grained” 접근 제어를 선택할 수 있습니다.\n필요한 추가 설정을 구성합니다.\n“Create”를 클릭하여 버킷을 만듭니다.\n\n\n\n\ngsutil 도구\nGoogle Cloud SDK에 포함된 gsutil은 Cloud Storage와 상호작용할 수 있는 명령어 도구다.\n\nGoogle Cloud SDK 설치:\n\nGoogle Cloud SDK 설치 페이지의 지침에 따라 Google Cloud SDK를 설치합니다.\n\nSDK 초기화:\ngcloud init\nGCS 버킷 만들기:\ngsutil mb -p [PROJECT_ID] -l [LOCATION] gs://[BUCKET_NAME]/\n\n[PROJECT_ID]를 Google Cloud 프로젝트 ID로 바꿉니다.\n[LOCATION]을 버킷 위치로 바꿉니다 (예: us-central1, us, EU).\n[BUCKET_NAME]을 고유한 버킷 이름으로 바꿉니다.\n\n예제: sh  gsutil mb -p alookso-id -l asia-northeast3 gs://my-buckets-name/  gstutil ls\n\n\n\n파이썬과 GC 클라이언트\nPython을 사용하여 Google Cloud 클라이언트 라이브러리를 통해 프로그래밍 방식으로 GCS 버킷을 만들 수도 있습니다.\n\nGoogle Cloud Storage 클라이언트 라이브러리 설치:\npip install google-cloud-storage\nPython을 사용하여 버킷 생성:\nfrom google.cloud import storage\n\n# GCS 클라이언트 초기화\nclient = storage.Client()\n\n# 버킷 이름과 위치 정의\nbucket_name = 'your-unique-bucket-name'\nlocation = 'US'  # 또는 'us-central1'과 같은 지역 지정\n\n# 버킷 생성\nbucket = client.bucket(bucket_name)\nnew_bucket = client.create_bucket(bucket, location=location)\n\nprint(f'Bucket {new_bucket.name} created.')\n\n\n\n예제\n아래는 Python을 사용하여 GCS 버킷을 생성하는 전체 예제입니다:\nimport os\nfrom google.cloud import storage\n\n# 인증 설정\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your-service-account-file.json'\n\n# GCS 클라이언트 초기화\nclient = storage.Client()\n\n# 버킷 이름과 위치 정의\nbucket_name = 'your-unique-bucket-name'\nlocation = 'US'  # 또는 'us-central1'과 같은 지역 지정\n\n# 버킷 생성\nbucket = client.bucket(bucket_name)\nnew_bucket = client.create_bucket(bucket, location=location)\n\nprint(f'Bucket {new_bucket.name} created.')",
    "crumbs": [
      "LLM",
      "허깅페이스"
    ]
  },
  {
    "objectID": "hf.html#버킷-권한",
    "href": "hf.html#버킷-권한",
    "title": "",
    "section": "버킷 권한",
    "text": "버킷 권한\n서비스 계정에 적절한 권한을 부여해야 합니다. 다음 단계를 따라 권한을 부여할 수 있습니다:\n\nGoogle Cloud Console에 접속합니다.\n프로젝트를 선택합니다.\n왼쪽 메뉴에서 “Storage” 메뉴를 선택합니다.\n버킷명 버킷을 찾아 클릭합니다.\n상단 메뉴에서 “권한” 탭을 클릭합니다.\n“구성원 추가” 버튼을 클릭합니다.\n“새 구성원” 입력란에 youtube-analytics-api@버킷명.iam.gserviceaccount.com 서비스 계정 이메일을 입력합니다.\n역할 선택란에서 “Storage 객체 관리자” 또는 “Storage 객체 생성자” 역할을 선택합니다. 이 역할은 버킷 내의 객체에 대한 읽기/쓰기 권한을 부여합니다.\n“저장” 버튼을 클릭하여 권한을 저장합니다.",
    "crumbs": [
      "LLM",
      "허깅페이스"
    ]
  },
  {
    "objectID": "hf.html#설치",
    "href": "hf.html#설치",
    "title": "",
    "section": "설치",
    "text": "설치\n허깅페이스에서 모형(transformers)과 데이터셋(datasets)를 설치하면 다양한 모형과 데이터를 다운로드 받을 수 있다. 목적이 다양한 기계학습 모형을 개발하는 것이기 때문에 텍스트는 torch, 이미지 영상은 torchvision 오디오는 torchaudio 플레임워크를 설치한다. huggingface_hub을 설치하면 CLI 방식으로 프로그래밍 방식으로 모형을 검색하고 설치에 도움을 받을 수 있다.\n\n코드# 허깅 페이스\npip install transformers datasets\n# ML 플레임워크\npip install torch torchvision torchaudio\n## HF 허브\npip install huggingface_hub\n\n\n모형\nhuggingface_hub으로 작업(task)에 맞는 모형을 HfAPi list_models() 메쏘드를 통해 찾아낼 수 있다.\n\n코드from huggingface_hub import HfApi\nfrom pprint import pprint\n\napi = HfApi()\n\nmodels = api.list_models(\n    task=\"table-question-answering\", \n    sort=\"downloads\", \n    direction=-1, \n    limit=1\n)\n\ndef format_model_info(model):\n    return {\n        \"id\": model.id,\n        \"downloads\": model.downloads,\n        \"likes\": model.likes,\n        \"task\": model.pipeline_tag,\n        \"last_modified\": (\n            model.last_modified.strftime(\"%Y-%m-%d %H:%M:%S\")\n            if model.last_modified\n            else \"N/A\"\n        ),\n    }\n\nformatted_models = [format_model_info(model) for model in models]\n\nprint(\"Table Question Answering Models:\")\npprint(formatted_models, width=100, sort_dicts=False)\n\n\n가장 많은 조회수와 다운로드를 자랑하는 텍스트 분류 작업(text-classificatin)에서 distilbert/distilbert-base-uncased-finetuned-sst-2-english 모형을 찾았으면 로컬 컴퓨터에 저장하여 다음 후속작업을 이어나갈 수 있다.\n\n코드# Import AutoModel\nfrom transformers import AutoModel\n\nmodelId = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# Download model using the modelId\nmodel = AutoModel.from_pretrained(modelId)\n# Save the model to a local directory\nmodel.save_pretrained(save_directory=f\"d:/llms/{modelId}\")\n\n\ntokenizer_config.json, config.json, model.safetensors 파일이 모두 다운로드 되었는지 확인한다.\n\n코드from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\nimport warnings\n\n# 경고 필터 설정\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint\")\n\n# 모델 ID와 저장 경로\nmodel_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel_path = f\"d:/llms/{model_id}\"\n\n# 모델 로드\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.1 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy&lt;2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"&lt;string&gt;\", line 3, in &lt;module&gt;\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 3716, in from_pretrained\n    state_dict = load_state_dict(resolved_archive_file)\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 556, in load_state_dict\n    return safe_load_file(checkpoint_file)\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\torch.py\", line 313, in load_file\n    result[k] = f.get_tensor(k)\n  File \"C:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\storage.py\", line 321, in __getitem__\n    return super().__getitem__(*args, **kwargs)\nC:\\Users\\statkclee\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\storage.py:321: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n  return super().__getitem__(*args, **kwargs)\n\n코드# 토크나이저 로드 (온라인에서 직접 다운로드)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# 분석할 텍스트\ntext = \"오늘 정말 기분이 좋다.\"\n\n# 텍스트를 토큰화하고 모델 입력으로 준비\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# 모델 추론\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# 결과 해석\nprobabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\npositive_probability = probabilities[0][1].item()\n\nprint(f\"입력 텍스트: '{text}'\")\n\n입력 텍스트: '오늘 정말 기분이 좋다.'\n\n코드print(f\"긍정적인 감성일 확률: {positive_probability:.2%}\")\n\n긍정적인 감성일 확률: 53.07%\n\n코드if positive_probability &gt; 0.5:\n    print(\"결과: 긍정적인 감성입니다.\")\nelse:\n    print(\"결과: 부정적인 감성입니다.\")\n\n결과: 긍정적인 감성입니다.",
    "crumbs": [
      "LLM",
      "허깅페이스"
    ]
  },
  {
    "objectID": "hf.html#데이터셋",
    "href": "hf.html#데이터셋",
    "title": "",
    "section": "데이터셋",
    "text": "데이터셋\n허깅페이스 데이터셋은 3D, Audio, Geospatial, Image, Tabular, Text, Time-series, Video 범주로 나누어져 제공되고 있다. 정형데이터(Tabular) 중에서 lavita/medical-qa-shared-task-v1-toy 데이터셋이 가장 많은 다운로드를 기록하고 있다.1 정형데이터를 판다스 데이터프레임으로 변환한 후 후속 작업을 진행한다.\n\n코드from datasets import load_dataset\n\ndataset = load_dataset(\"lavita/medical-qa-shared-task-v1-toy\", split=\"train\")\n\ndataset.to_pandas().head()[[\"id\", \"ending0\", \"GCC\"]]",
    "crumbs": [
      "LLM",
      "허깅페이스"
    ]
  },
  {
    "objectID": "hf.html#파이프라인",
    "href": "hf.html#파이프라인",
    "title": "",
    "section": "파이프라인",
    "text": "파이프라인\nAuto 클래스는 모델, 토크나이저, 설정 등을 유연하게 사용할 수 있는 일반 클래스로, ML 작업에 대한 더 많은 제어와 유연성을 제공한다. AutoModels는 특정 작업에 맞는 사전 훈련된 모델을 직접 다운로드하고 초기화하는 기능을 제공하여 복잡한 모델 아키텍처를 직접 구현할 필요 없이 바로 사용할 수 있다. AutoTokenizers는 텍스트 입력 데이터를 모델이 이해할 수 있는 형식을 담당한다. 모델과 쌍을 이루는 토크나이저를 사용하는 것이 권장되며, 이를 통해 텍스트 전처리 과정을 자동화할 수 있다.\n파이프라인 모듈은 특정 작업에 필요한 모든 단계를 포함하는 고수준 인터페이스로 모델 로딩, 전처리, 추론 등의 과정을 자동화하여 빠르게 작업을 수행하고 시작하기에 적합하다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n감성분석\n텍스트 분류\nSTT\n\n\n\n\n코드from transformers import pipeline\n\n# 다국어 BERT 모델을 사용한 감성 분석 파이프라인 생성\nclassifier = pipeline(\n    task=\"sentiment-analysis\",\n    model=\"bert-base-multilingual-cased\",\n    return_all_scores=True,\n)\n\n# 분석할 텍스트 리스트\ntexts = [\n    \"이 영화는 정말 재미있었어요. 추천합니다!\",\n    \"서비스가 너무 별로였어요. 다시는 안 갈 것 같아요.\",\n    \"그저 그랬어요. 특별히 좋지도 나쁘지도 않았습니다.\",\n    \"This movie was really great. I recommend it!\",\n    \"The service was terrible. I won't go there again.\",\n]\n\n# 각 텍스트에 대해 감성 분석 수행\nfor text in texts:\n    result = classifier(text)[0]\n\n    # 긍정, 부정 점수 추출\n    positive_score = next(score for score in result if score[\"label\"] == \"LABEL_1\")[\n        \"score\"\n    ]\n    negative_score = next(score for score in result if score[\"label\"] == \"LABEL_0\")[\n        \"score\"\n    ]\n\n    print(f\"\\n텍스트: {text}\")\n    print(f\"긍정 점수: {positive_score:.2%}\")\n    print(f\"부정 점수: {negative_score:.2%}\")\n\n    # 감성 판단\n    if positive_score &gt; negative_score:\n        sentiment = \"긍정\"\n        score = positive_score\n    else:\n        sentiment = \"부정\"\n        score = negative_score\n\n    print(f\"판단된 감성: {sentiment}\")\n\n    # 확신도에 따른 해석\n    if score &gt; 0.75:\n        interpretation = \"매우 강한\"\n    elif score &gt; 0.6:\n        interpretation = \"강한\"\n    elif score &gt; 0.4:\n        interpretation = \"약한\"\n    else:\n        interpretation = \"중립적인\"\n\n    print(f\"해석: {interpretation} {sentiment}적 의견입니다.\")\n\n\n\n\n\n코드from transformers import pipeline\n\n# 파이프라인 생성\nclassifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n# 예제 1: 영화 리뷰 분류\ntext1 = \"이 영화는 흥미진진한 플롯과 멋진 특수효과로 가득했습니다. 배우들의 연기도 훌륭했어요.\"\ncandidate_labels1 = [\"긍정적\", \"부정적\", \"중립적\"]\noutput1 = classifier(text1, candidate_labels1)\nprint(\"영화 리뷰 분류:\")\nprint(f\"가장 높은 확률의 레이블: {output1['labels'][0]}\")\nprint(f\"확률: {output1['scores'][0]:.4f}\")\n\n# 예제 2: 뉴스 기사 주제 분류\ntext2 = \"최근 연구에 따르면 규칙적인 운동이 스트레스 감소와 수면의 질 향상에 도움이 된다고 합니다.\"\ncandidate_labels2 = [\"건강\", \"스포츠\", \"기술\", \"정치\"]\noutput2 = classifier(text2, candidate_labels2)\nprint(\"\\n뉴스 기사 주제 분류:\")\nprint(f\"가장 높은 확률의 레이블: {output2['labels'][0]}\")\nprint(f\"확률: {output2['scores'][0]:.4f}\")\n\n# 예제 3: 상품 리뷰 감성 분석\ntext3 = \"이 제품은 가격은 비싸지만 품질이 정말 뛰어나고 오래 사용할 수 있을 것 같아요.\"\ncandidate_labels3 = [\"만족\", \"불만족\", \"중립\"]\noutput3 = classifier(text3, candidate_labels3)\nprint(\"\\n상품 리뷰 감성 분석:\")\nprint(f\"가장 높은 확률의 레이블: {output3['labels'][0]}\")\nprint(f\"확률: {output3['scores'][0]:.4f}\")\n\n\n\n\nASR(Automatic Speech Recognition)은 음성을 인식하여 텍스트로 출력하는 기계학습 모형으로 OpenAI 위스퍼, 페이스북 Wav2Vec2이 유명하다.\n\n코드from transformers import pipeline\n\n# 한국어 음성 인식을 위한 모델 로드\ntranscriber = pipeline(\n    task=\"automatic-speech-recognition\", model=\"kresnik/wav2vec2-large-xlsr-korean\"\n)\n\n# 오디오 파일 경로\naudio_file = \"data/한국어_영어번역.mp3\"\n\n# 음성 인식 실행\nresult = transcriber(audio_file)\n\nprint(result[\"text\"])\n\n\n정교는 국미럽 얼는 여던 레번째 사미지를 맞아 나라 리 기생하고 신하신 애국 습예들개 단았는 감사와 경위를 표합니또립 유공자와 유가적 여러분에게도 존경과 감사에 말씀을 쓰립니다\n\n코드library(embedr)\n\nembed_audio(\"data/한국어_영어번역.mp3\")\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "LLM",
      "허깅페이스"
    ]
  },
  {
    "objectID": "hf.html#footnotes",
    "href": "hf.html#footnotes",
    "title": "",
    "section": "각주",
    "text": "각주\n\nWorking with Hugging Face Datasets↩︎",
    "crumbs": [
      "LLM",
      "허깅페이스"
    ]
  },
  {
    "objectID": "llm_ops.html",
    "href": "llm_ops.html",
    "title": "",
    "section": "",
    "text": "자동화LLMOps 코드",
    "crumbs": [
      "자동화",
      "LLMOps"
    ]
  },
  {
    "objectID": "llm_ops.html#rag",
    "href": "llm_ops.html#rag",
    "title": "",
    "section": "RAG",
    "text": "RAG\nRAG(Retrieval-Augmented Generation)는 대규모 언어 모델(LLM)의 성능을 향상시키는 혁신적인 기술로 LLM의 일반적인 지식과 외부 데이터베이스의 특정 도메인 정보를 결합하여 더 정확하고 최신의 응답을 생성한다.\nRAG의 핵심 구성 요소는 벡터 데이터베이스, 효율적인 검색 시스템, LLM이다. 벡터 데이터베이스는 문서나 지식을 벡터 형태로 저장하며, 검색 시스템은 이 데이터베이스에서 관련 정보를 신속하게 찾아낸다. LLM은 이렇게 검색된 정보를 바탕으로 최종 응답을 생성하는 역할을 한다.\nRAG의 작동 과정은 다음과 같다. 먼저 사용자의 쿼리를 받으면, 이와 관련된 정보를 벡터 데이터베이스에서 검색한다. 그 다음, 검색된 정보를 사용자의 원래 쿼리와 함께 LLM에 제공한다. 마지막으로, LLM이 이 모든 정보를 종합하여 최종 응답을 생성한다.\n이 기술의 주요 장점은 최신 정보 제공, 정확성 향상, LLM의 환각 현상 감소이다. 외부 데이터베이스를 통해 최신 정보를 활용할 수 있어, 특정 도메인의 정보를 더 정확하게 참조할 수 있다. 또한, LLM이 잘못된 정보를 생성할 가능성을 크게 줄일 수 있다.",
    "crumbs": [
      "자동화",
      "LLMOps"
    ]
  },
  {
    "objectID": "llm_ops.html#프롬프트-공학",
    "href": "llm_ops.html#프롬프트-공학",
    "title": "",
    "section": "프롬프트 공학",
    "text": "프롬프트 공학\n프롬프트 공학은 대규모 언어 모델(LLM)의 성능을 최적화하기 위해 입력 프롬프트를 설계, 개발, 최적화하는 과정이다. LLM에게 명확한 지시, 관련 맥락, 적절한 예시, 구체적인 입력 데이터, 원하는 출력 형식을 제공하여 모델이 특정 작업을 정확하고 효과적으로 수행하도록 유도하는 기술이다. 프롬프트 공학은 반복적인 테스트와 개선 과정을 통해 최적의 프롬프트를 찾아내며, 이를 통해 LLM의 응답 품질, 일관성, 관련성을 향상시킨다. 특히, LLM 기반 애플리케이션의 성능을 크게 좌우하는 핵심 요소로, 다양한 분야에서 AI의 효과적인 활용을 가능하게 하는 중요한 기술이다.\n프롬프트 공학에서 프롬프트는 지시사항, 예시/맥락, 입력 데이터, 출력지표 등으로 구성된다.\n\n지시사항 (Instruction): LLM에게 수행해야 할 작업을 명확히 알려주는 부분이다. 작업의 목적, 수행 방법, 주의사항 등을 포함한다. 명확하고 구체적인 지시는 모델이 원하는 결과를 생성하는 데 핵심적인 역할을 한다.\n예시/맥락 (Examples/Context): 모델에게 작업의 맥락을 제공하고 원하는 출력 형식을 보여준다. Few-shot 학습 예시를 통해 모델의 성능을 향상시킬 수 있다. 관련 배경 정보나 도메인 지식도 여기에 포함될 수 있다.\n입력 데이터 (Input data): 모델이 실제로 처리해야 할 구체적인 정보나 데이터다. 질문, 문장, 문단 등 다양한 형태가 될 수 있으며, 작업의 핵심 대상이 된다.\n출력 지표 (Output indicator): 원하는 출력의 형식, 구조, 스타일을 지정하는 부분으로 모델의 응답을 일관되고 예측 가능한 형태로 유도할 수 있다.\n\n네 가지 요소는 상호 보완적으로 작용하여 효과적인 프롬프트를 구성한다. 지시사항은 전체적인 방향을 제시하고, 예시는 구체적인 기대치를 보여준다. 입력 데이터는 실제 처리 대상을 제공하며, 출력 지표는 결과물의 형태를 규정한다. 이들을 적절히 조합하여 사용하면 LLM이 원하는 작업을 정확하게 수행하고 높은 품질의 출력을 생성할 가능성이 높아진다.\n텍스트 감정 분석 작업을 위한 프롬프트를 살펴보자.\n\n프롬프트출력결과\n\n\n\n지시사항: “주어진 문장의 감정을 분석하고, 그 감정이 긍정적, 중립적, 부정적 중 어디에 해당하는지 판단하세요. 감정 강도도 1-5 척도로 평가해주세요 (1: 매우 약함, 5: 매우 강함).”\n예시/맥락: ““” 예시 1: ‘오늘 날씨가 정말 좋아서 기분이 좋다.’ 분석: 긍정적, 강도: 4\n\n예시 2: ‘이 영화는 그저 그랬어. 특별히 좋지도 나쁘지도 않았어.’ 분석: 중립적, 강도: 2\n예시 3: ‘시험에 떨어져서 너무 실망스럽고 우울해.’ 분석: 부정적, 강도: 5 ““”\n\n입력 데이터: “분석할 문장: ‘새로 산 노트북이 예상보다 성능이 좋아서 만족스럽지만, 가격이 비싼 편이라 약간 아쉽다.’”\n출력 지표: “출력 형식: 감정: [긍정적/중립적/부정적] 강도: [1-5 사이의 숫자] 설명: [분석 근거를 간단히 서술]”\n\n이러한 구조화된 프롬프트는 LLM이 감정 분석 작업을 정확하게 수행하고, 일관된 형식의 결과를 제공하도록 유도한다. 각 구성요소가 명확하게 정의되어 있어 모델이 작업을 이해하고 수행하는 데 필요한 모든 정보를 포함하고 있다.\n\n\n앤쓰로픽 클로드 3.5 소넷 LLM에 프롬프트를 전달하여 얻은 결과는 다음과 같다.\n출력 형식:\n감정: 긍정적\n강도: 3\n설명: 주어진 문장은 전반적으로 긍정적인 감정을 나타내지만, 일부 부정적인 요소도 포함하고 있다. 노트북의 성능에 대한 만족감이 주된 감정이며 이는 긍정적이다. 하지만 가격에 대한 아쉬움이 함께 표현되어 있어, 완전히 강한 긍정은 아니다. 따라서 감정은 긍정적이지만 강도는 중간 정도인 3으로 평가했다. 만족감과 아쉬움이 공존하는 복합적인 감정 상태를 나타내고 있다.",
    "crumbs": [
      "자동화",
      "LLMOps"
    ]
  },
  {
    "objectID": "prompt_eng.html",
    "href": "prompt_eng.html",
    "title": "",
    "section": "",
    "text": "AI 코딩프롬프트 공학 코드",
    "crumbs": [
      "AI 코딩",
      "프롬프트 공학"
    ]
  },
  {
    "objectID": "prompt_eng.html#프롬프트-구성요소",
    "href": "prompt_eng.html#프롬프트-구성요소",
    "title": "",
    "section": "프롬프트 구성요소",
    "text": "프롬프트 구성요소\n효과적인 AI 프롬프트 작성은 인공지능과의 상호작용을 최적화하는 핵심으로 매우 중요하다. 먼저 프롬프트를 구성하는 핵심 5가지 구성요소를 살펴보자.\n\n페르소나(Persona): AI가 특정 역할이나 관점을 취하도록 한다.\n지시사항 (Instruction): LLM에게 수행해야 할 작업을 명확히 알려주는 부분이다. 작업의 목적, 수행 방법, 주의사항 등을 포함한다. 명확하고 구체적인 지시는 모델이 원하는 결과를 생성하는 데 핵심적인 역할을 한다.\n예시/맥락 (Examples/Context): 모델에게 작업의 맥락을 제공하고 원하는 출력 형식을 보여준다. Few-shot 학습 예시를 통해 모델의 성능을 향상시킬 수 있다. 관련 배경 정보나 도메인 지식도 여기에 포함될 수 있다.\n입력 데이터 (Input data): 모델이 실제로 처리해야 할 구체적인 정보나 데이터다. 질문, 문장, 문단 등 다양한 형태가 될 수 있으며, 작업의 핵심 대상이 된다.\n출력 지표 (Output indicator): 원하는 출력의 형식, 구조, 스타일을 지정하는 부분으로 모델의 응답을 일관되고 예측 가능한 형태로 유도할 수 있다.\n\n다음으로 SALT(Style, Audience, Length, Tone)를 반영하여 출력에 대한 제어를 통해 효과적으로 LLM으로부터 산출물을 추출할 수 있다.\n\n스타일(Style): 응답의 문체나 표현 방식\n청중(Audience): 대상 독자나 사용자\n길이(Length): 응답의 상세도나 분량\n어조(Tone): 글의 분위기나 감정",
    "crumbs": [
      "AI 코딩",
      "프롬프트 공학"
    ]
  },
  {
    "objectID": "prompt_eng.html#사례",
    "href": "prompt_eng.html#사례",
    "title": "",
    "section": "사례",
    "text": "사례\n프롬프트를 핵심적인 내용만 사용하여 작성하여보자.\n\n프롬프트출력결과\n\n\n지시사항:\n간단한 김치찌게 레시피를 작성해주세요. 조리 시간은 30분 이내여야 합니다.\n\n출력 형식:\n1. 레시피 이름\n2. 준비 시간 및 조리 시간\n3. 재료 목록 (4인분 기준)\n4. 조리 단계 (번호로 구분)\n5. 영양 정보 (1인분 기준 칼로리)\n\n맥락:\n초보 요리사도 쉽게 따라할 수 있어야 합니다. 일반 슈퍼마켓에서 구할 수 있는 재료만 사용하세요.\n\n예시:\n조리 단계 예:\n1. 큰 냄비에 물을 끓입니다.\n2. 물이 끓으면 파스타를 넣고 8-10분간 삶습니다.\n\n\n이 프롬프트에 따라 간단한 김치찌개 레시피를 작성해 드리겠습니다.\n\n1. 레시피 이름: \n쉽고 빠른 김치찌개\n\n2. 준비 시간 및 조리 시간:\n- 준비 시간: 10분\n- 조리 시간: 20분\n- 총 소요 시간: 30분\n\n3. 재료 목록 (4인분 기준):\n- 김치 300g (약 2컵, 썰어서)\n- 돼지고기 목살 150g (얇게 썬 것)\n- 두부 1모 (약 400g)\n- 대파 1대\n- 고춧가루 1큰술\n- 다진 마늘 1큰술\n- 국간장 1큰술\n- 참기름 1작은술\n- 물 3컵\n\n4. 조리 단계:\n1. 김치는 가위로 썰고, 돼지고기는 얇게 썹니다. 두부는 1cm 두께로 깍둑썰기하고, 대파는 어슷썹니다.\n2. 냄비에 참기름을 두르고 돼지고기를 넣어 중간 불에서 2-3분간 볶습니다.\n3. 고기가 익으면 김치를 넣고 2분간 더 볶습니다.\n4. 물 3컵을 붓고 끓으면 두부, 고춧가루, 다진 마늘을 넣습니다.\n5. 중간 불로 줄여 10분간 끓입니다.\n6. 국간장으로 간을 맞추고 대파를 넣어 2분 더 끓입니다.\n7. 불을 끄고 그릇에 담아 맛있게 즐깁니다.\n\n5. 영양 정보 (1인분 기준 칼로리):\n약 220kcal\n\n이 레시피는 초보자도 쉽게 따라할 수 있으며, 일반 슈퍼마켓에서 구할 수 있는 재료만을 사용했습니다. 30분 이내에 완성할 수 있는 간단하고 맛있는 김치찌개입니다.\n\n\n\n이번에는 SALT 출력물을 제어하도록 기능을 추가하여 프롬프트를 작성한다.\n\n프롬프트출력결과\n\n\n지시사항:\n전통적인 한국식 김치찌개 레시피를 작성해주세요. \n\n출력 형식:\n1. 레시피 이름 (한글과 영문)\n2. 준비 시간 및 조리 시간\n3. 재료 목록 (2인분 기준)\n4. 조리 단계 (번호로 구분)\n5. 맛있게 먹는 팁\n\n맥락:\n한국을 방문한 외국인 관광객이 집에서 쉽게 따라 할 수 있어야 합니다. \n\n예시:\n조리 단계 예:\n1. 냄비에 식용유를 두르고 중간 불로 달굽니다.\n2. 돼지고기를 넣고 반투명해질 때까지 볶습니다.\n\n스타일:\n친근하고 격려하는 어조로 작성하되, 정확한 조리법을 설명해주세요.\n\n청중:\n한국 음식에 관심 있는 20-30대 외국인 요리 초보자\n\n길이:\n각 조리 단계는 1-2문장으로 간결하게 설명하고, 전체 레시피는 100단어 이내로 작성해주세요.\n\n어조:\n열정적이고 긍정적인 어조를 유지하면서, 전문적인 조언을 제공하는 요리 전문가의 말투를 사용해주세요.\n\n\n김치찌개 (Kimchi-jjigae)\n\n준비 시간: 10분 | 조리 시간: 20분\n\n재료 (2인분):\n- 김치 2컵\n- 돼지고기 150g\n- 두부 1/2모\n- 대파 1대\n- 고춧가루 1큰술\n- 물 2컵\n\n조리 단계:\n1. 김치를 적당한 크기로 자르고, 돼지고기는 얇게 썰어주세요.\n2. 냄비에 돼지고기를 넣고 중간 불에서 볶아주세요.\n3. 김치를 넣고 2분간 더 볶은 후, 물을 부어 끓여주세요.\n4. 두부와 고춧가루를 넣고 10분간 더 끓이세요.\n5. 대파를 송송 썰어 올리고 2분 더 끓이면 완성!\n\n맛있게 먹는 팁: 뜨거울 때 드세요! 밥과 함께 즐기면 더욱 맛있답니다.",
    "crumbs": [
      "AI 코딩",
      "프롬프트 공학"
    ]
  },
  {
    "objectID": "prompt_coding.html",
    "href": "prompt_coding.html",
    "title": "프롬프트 코딩",
    "section": "",
    "text": "OpenAI 패키지를 활용한 프로그래밍 방식의 프롬프트 공학은 전통적인 UI를 이용한 방식에 비해 여러 면에서 우수성을 보인다. 먼저, API를 통해 다양한 매개변수를 조정하고 복잡한 로직을 구현할 수 있어 유연성이 매우 높다. 또한 다른 시스템이나 워크플로우와 쉽게 통합할 수 있고 대량의 요청을 자동화할 수 있어 효율적이다.\n프로그래밍 방식은 프롬프트를 체계적으로 테스트하고 결과를 분석하는 실험을 쉽게 설계할 수 있어 실험과 반복에 유리하다. 특정 요구사항에 맞는 복잡한 프롬프트 전략을 구현할 수 있어 맞춤형 솔루션 개발에도 적합하다. 코드로 관리되므로 버전 관리 시스템을 통한 추적과 팀 협업이 용이하다는 장점도 있다.\n초기 진입 장벽이 있지만 숙달 시 강력한 도구로 활용 가능하며, API 호출 단위로 비용이 발생하므로 대규모 사용 시 비용 효율적이다. 복잡한 대화 흐름과 컨텍스트 관리가 가능해 실시간 상호작용에도 우수한 성능을 보인다.\n반면 UI 방식은 주어진 인터페이스 내에서만 조작이 가능하므로 상대적으로 제한적이다. 수동 조작이 필요해 대규모 작업에는 비효율적이며, 체계적인 실험과 결과 분석이 어렵다. 일반적인 사용 사례에 맞춰진 기능만 제공되어 맞춤형 솔루션 개발에 한계가 있다.",
    "crumbs": [
      "AI 코딩",
      "프롬프트 코딩"
    ]
  },
  {
    "objectID": "prompt_coding.html#기본완성-프롬프트",
    "href": "prompt_coding.html#기본완성-프롬프트",
    "title": "프롬프트 코딩",
    "section": "기본완성 프롬프트",
    "text": "기본완성 프롬프트\nOpenAI의 API를 사용하여 텍스트 분류, 요약, 그리고 엔티티 추출과 같은 다양한 작업을 수행한다. dotenv를 사용하여 API 키를 관리하여 외부에 API키가 공개되는 것을 방지한다.\n\n\n코드\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# .env 파일에서 환경 변수 로드\nload_dotenv()\n\n# OpenAI 클라이언트 초기화\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef get_response(messages):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0\n    )\n    return response.choices[0].message.content\n\n\ndef classify_sentiment(text):\n    system_prompt = \"당신은 텍스트의 감정을 분류하도록 훈련된 AI입니다. '긍정적', '부정적', 또는 '중립적' 중 하나로만 응답하세요.\"\n    user_prompt = f\"다음 텍스트의 감정을 분류하세요: '{text}'\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    return get_response(messages)\n\n\ndef summarize_text(text, max_words=50):\n    system_prompt = f\"당신은 텍스트를 요약하도록 훈련된 AI입니다. {max_words}단어 이내로 요약을 제공하세요.\"\n    user_prompt = f\"다음 텍스트를 요약하세요: '{text}'\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    return get_response(messages)\n\n\ndef extract_entities(text):\n    system_prompt = \"당신은 텍스트에서 개체명을 추출하도록 훈련된 AI입니다. '사람', '조직', '장소'를 키로 가진 JSON 객체 형태로 결과를 제공하세요.\"\n    user_prompt = f\"다음 텍스트에서 개체명을 추출하세요: '{text}'\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    return get_response(messages)\n\n\n# 테스트\nsample_text = \"애플(Apple Inc.)은 오늘 팀 쿡(Tim Cook) CEO가 다음 달 캘리포니아 쿠퍼티노의 새 캠퍼스를 방문할 예정이라고 발표했습니다.\"\n\nprint(\"감정 분류:\")\nprint(classify_sentiment(sample_text))\n\nprint(\"\\n텍스트 요약:\")\nprint(summarize_text(sample_text))\n\nprint(\"\\n개체명 추출:\")\nprint(extract_entities(sample_text))\n\n\n감정 분류:\n중립적\n\n텍스트 요약:\n애플은 팀 쿡 CEO가 다음 달에 신규 캠퍼스를 방문할 예정이라고 발표했습니다.\n\n개체명 추출:\n{\n    \"조직\": \"애플\",\n    \"사람\": \"팀 쿡\",\n    \"장소\": \"캘리포니아 쿠퍼티노\"\n}",
    "crumbs": [
      "AI 코딩",
      "프롬프트 코딩"
    ]
  },
  {
    "objectID": "prompt_coding.html#사용자-맞춤형-프롬프트",
    "href": "prompt_coding.html#사용자-맞춤형-프롬프트",
    "title": "프롬프트 코딩",
    "section": "사용자 맞춤형 프롬프트",
    "text": "사용자 맞춤형 프롬프트\n프롬프트 핵심구성요소를 반영한 프롬프트를 작성하고 이를 OpenAI API에 전달하여 감성분석을 수행하는 코드를 다음과 같이 작성할 수 있다.\n\n\n코드\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# .env 파일에서 환경 변수 로드\nload_dotenv()\n\n# OpenAI 클라이언트 초기화\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef get_response(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            temperature=temperature,\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# 사용자 정의 프롬프트\ndef create_sentiment_analysis_prompt(text):\n    # 페르소나(Persona)\n    persona = \"당신은 텍스트의 감정을 정확하게 분석하는 전문 언어학자입니다.\"\n\n    # 지시사항 (Instruction)\n    instruction = \"주어진 텍스트의 감정을 분석하고, 그 감정의 강도를 평가하세요. 텍스트에 나타난 단어 선택, 문장 구조, 전반적인 톤을 고려하여 분석하세요.\"\n\n    # 예시/맥락 (Examples/Context)\n    examples = \"\"\"\n    예시 1:\n    텍스트: \"오늘은 정말 최고의 날이었어! 모든 것이 완벽했어.\"\n    분석: 감정: 매우 긍정적, 강도: 강함\n\n    예시 2:\n    텍스트: \"이 영화는 그저 그랬어. 특별히 좋지도 나쁘지도 않았어.\"\n    분석: 감정: 중립적, 강도: 약함\n    \"\"\"\n\n    # 입력 데이터 (Input data)\n    input_data = f\"분석할 텍스트: '''{text}'''\"\n\n    # 출력 지표 (Output indicator)\n    output_format = \"\"\"\n    다음 형식으로 출력을 제공하세요:\n    - 주요 감정: [긍정적/부정적/중립적 중 하나]\n    - 감정 강도: [약함/중간/강함 중 하나]\n    - 근거: [분석의 근거를 2-3문장으로 설명]\n    \"\"\"\n\n    # 전체 프롬프트 조합\n    full_prompt = (\n        f\"{persona}\\n\\n{instruction}\\n\\n{examples}\\n\\n{input_data}\\n\\n{output_format}\"\n    )\n\n    return full_prompt\n\n# 사용 예시\nsample_text = \"이번 여행은 기대 이상으로 좋았어요. 풍경도 아름답고 음식도 맛있었어요. 다만, 날씨가 조금 흐린 날이 있어서 아쉬웠죠.\"\nprompt = create_sentiment_analysis_prompt(sample_text)\nresponse = get_response(prompt)\nprint(response)\n\n\n- 주요 감정: 긍정적\n- 감정 강도: 중간\n- 근거: 텍스트에서 나타나는 단어들을 고려하면 여행이 기대 이상으로 좋았다는 긍정적인 느낌이 강하게 전달됩니다. 그러나 흐린 날씨로 인해 조금 아쉬움이 있었다는 부정적인 감정도 표현되어 있어 감정의 강도는 중간 정도로 판단됩니다.",
    "crumbs": [
      "AI 코딩",
      "프롬프트 코딩"
    ]
  },
  {
    "objectID": "prompt_coding.html#multi-step과-few-shot-프롬프트",
    "href": "prompt_coding.html#multi-step과-few-shot-프롬프트",
    "title": "프롬프트 코딩",
    "section": "Multi-step과 few-shot 프롬프트",
    "text": "Multi-step과 few-shot 프롬프트\nMulti-step 프롬프트와 few-shot 프롬프트는 각각 고유한 특성과 용도를 가지고 있다. Multi-step 프롬프트는 복잡한 작업을 여러 단계로 나누어 수행하도록 AI를 안내한다. 문제 해결 과정을 명확하게 구조화하고, 각 단계에서 중간 결과를 확인할 수 있어 복잡한 추론이 필요한 작업에 적합하다. 또한 사용자가 프롬프트의 각 단계를 직접 제어할 수 있어 결과의 정확성을 높일 수 있다. 그러나 Multi-step 프롬프트는 설계가 복잡하고 시간이 많이 소요될 수 있으며, 각 단계에서 오류가 누적될 가능성이 있다.\n반면 Few-shot 프롬프트는 AI에게 몇 가지 예시를 제공하여 원하는 출력 형식이나 패턴을 학습하도록 한다. 새로운 작업이나 도메인에 AI를 빠르게 적응시킬 수 있어 유연성이 높다. 또한 예시를 통해 AI의 이해를 돕기 때문에 복잡한 설명 없이도 원하는 결과를 얻을 수 있다. 그러나 Few-shot 프롬프트는 제공된 예시에 과도하게 의존할 수 있어 일반화 능력이 떨어질 수 있으며, 예시 선택에 따라 결과의 품질이 크게 달라질 수 있다.\nMulti-step 프롬프트는 단계별 추론이 필요한 복잡한 문제 해결, 수학적 계산, 코드 생성 등에 적합하다. 반면 Few-shot 프롬프트는 텍스트 분류, 감정 분석, 간단한 번역 작업 등 패턴 인식이 중요한 작업에 효과적이다. 실제 응용에서는 두 방식을 혼합하여 사용하는 것도 가능하며, 이를 통해 각 방식의 장점을 극대화할 수 있다.\n\nMulti-step 프롬프트\n텍스트 분석 및 요약 작업을 여러 단계로 나누어 수행하는 Multi-step 프롬프트를 다음과 같이 구현할 수 있다.\n\n\n코드\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef get_response(messages):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0.7\n    )\n    return response.choices[0].message.content\n\n\ndef multi_step_text_analysis(text):\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"당신은 텍스트 분석 및 요약 전문가입니다. 주어진 단계에 따라 텍스트를 분석하고 요약해주세요.\",\n        }\n    ]\n\n    # 1단계: 주요 주제 식별\n    messages.append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"다음 텍스트의 주요 주제를 3개 이내로 식별해주세요: \\n\\n{text}\",\n        }\n    )\n    topics = get_response(messages)\n    messages.append({\"role\": \"assistant\", \"content\": topics})\n    print(\"주요 주제:\", topics)\n\n    # 2단계: 감정 분석\n    messages.append(\n        {\"role\": \"user\", \"content\": \"이 텍스트의 전반적인 감정 톤을 분석해주세요.\"}\n    )\n    sentiment = get_response(messages)\n    messages.append({\"role\": \"assistant\", \"content\": sentiment})\n    print(\"감정 분석:\", sentiment)\n\n    # 3단계: 핵심 문장 추출\n    messages.append(\n        {\n            \"role\": \"user\",\n            \"content\": \"이 텍스트에서 가장 중요한 2-3개의 문장을 추출해주세요.\",\n        }\n    )\n    key_sentences = get_response(messages)\n    messages.append({\"role\": \"assistant\", \"content\": key_sentences})\n    print(\"핵심 문장:\", key_sentences)\n\n    # 4단계: 요약 생성\n    messages.append(\n        {\n            \"role\": \"user\",\n            \"content\": \"지금까지의 분석을 바탕으로 이 텍스트를 100단어 이내로 요약해주세요.\",\n        }\n    )\n    summary = get_response(messages)\n    messages.append({\"role\": \"assistant\", \"content\": summary})\n    print(\"요약:\", summary)\n\n    return {\n        \"topics\": topics,\n        \"sentiment\": sentiment,\n        \"key_sentences\": key_sentences,\n        \"summary\": summary,\n    }\n\n\n# 사용 예시\nsample_text = \"\"\"\n인공지능(AI) 기술의 발전은 우리 사회에 큰 변화를 가져오고 있습니다. \n의료 분야에서는 AI가 질병 진단의 정확성을 높이고 있으며, 교육 분야에서는 개인화된 학습 경험을 제공하고 있습니다. \n그러나 AI의 확산은 일자리 감소와 같은 사회적 문제도 야기하고 있습니다. \n예를 들어, 자동화로 인해 일부 직종이 사라질 위험이 있습니다. \n따라서 우리는 AI 기술의 혜택을 최대화하면서도 그 부작용을 최소화할 수 있는 방안을 모색해야 합니다. \n이를 위해서는 정부, 기업, 시민 사회가 협력하여 AI 윤리 가이드라인을 수립하고, \n새로운 기술 환경에 적응할 수 있는 교육 프로그램을 개발하는 등의 노력이 필요합니다.\n\"\"\"\n\nresult = multi_step_text_analysis(sample_text)\n\n\n주요 주제: 1. 인공지능(AI) 기술의 사회적 영향\n2. 의료 분야와 교육 분야에서의 AI 활용\n3. AI 기술의 부작용 및 대응 방안\n감정 분석: 주어진 텍스트의 전반적인 감정 톤은 중립적이고 객관적입니다. 텍스트는 인공지능 기술의 발전이 가져오는 긍정적인 면과 동시에 부정적인 측면을 다루고 있으며, 이에 중립적인 입장에서 사회적 영향과 대응 방안을 제시하고 있습니다. 주로 사실적인 정보와 분석을 다루고 있어서 주정적이나 부정적인 감정이 특히 강조되지는 않고 있습니다.\n핵심 문장: 1. 인공지능(AI) 기술의 발전은 우리 사회에 큰 변화를 가져오고 있습니다.\n2. 우리는 AI 기술의 혜택을 최대화하면서도 그 부작용을 최소화할 수 있는 방안을 모색해야 합니다.\n3. 이를 위해서는 정부, 기업, 시민 사회가 협력하여 AI 윤리 가이드라인을 수립하고, 새로운 기술 환경에 적응할 수 있는 교육 프로그램을 개발하는 등의 노력이 필요합니다.\n요약: 인공지능(AI) 기술의 발전으로 의료와 교육 분야에서 진단 정확성 향상과 개인화된 학습 경험이 가능해지고 있지만, 사회적 문제도 동반하고 있습니다. AI의 확산으로 인한 일자리 감소 우려도 존재합니다. 따라서 AI의 혜택은 최대화하고 부작용은 최소화하기 위해 정부, 기업, 시민 사회가 협력하여 AI 윤리 가이드라인을 수립하고 교육 프로그램을 개발하는 노력이 요구됩니다. AI 기술의 발전은 사회적 변화를 이끌고 있으며, 이에 적절히 대응하여 사회적 이슈를 해결할 필요가 있습니다.\n\n\n\n\nfew-shot 프롬프트\nfew-shot 프롬프트는 특정 도메인에 대한 빠른 프로토타이핑이나 모델 튜닝이 필요한 경우에 유용할 뿐만 아니라, 레이블이 지정된 대규모 데이터셋 없이도 합리적인 성능을 얻을 수 있어 리소스가 제한된 상황에서도 효과적이다. 텍스트 분류(감성) 작업을 수행하는데, 특정 도메인(이 경우 영화 리뷰)에 대한 감정 분석을 수행하는 코드를 다음과 같이 작성할 수 있다.\n\n\n코드\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef get_response(messages):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0.3\n    )\n    return response.choices[0].message.content\n\n\ndef few_shot_movie_review_classifier(review):\n    # Few-shot 예시를 포함한 프롬프트 구성\n    prompt = \"\"\"다음은 영화 리뷰의 감정을 분류하는 작업입니다. 각 리뷰를 '긍정', '중립', '부정' 중 하나로 분류해주세요.\n\n예시 1:\n리뷰: \"이 영화는 정말 훌륭했어요! 배우들의 연기가 일품이었고, 스토리도 매우 흥미진진했습니다.\"\n분류: 긍정\n\n예시 2:\n리뷰: \"그저 그런 영화였습니다. 특별히 좋지도 나쁘지도 않았어요.\"\n분류: 중립\n\n예시 3:\n리뷰: \"시간 낭비였어요. 스토리도 지루하고 연기도 어색했습니다.\"\n분류: 부정\n\n예시 4:\n리뷰: \"기대했던 것보다는 조금 실망스러웠지만, 몇몇 장면은 인상적이었습니다.\"\n분류: 중립\n\n이제 다음 리뷰를 분류해주세요:\n리뷰: \"{}\"\n분류:\"\"\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"당신은 영화 리뷰를 분석하는 전문가입니다.\"},\n        {\"role\": \"user\", \"content\": prompt.format(review)},\n    ]\n\n    return get_response(messages)\n\n\ndef analyze_reviews(reviews):\n    results = []\n    for review in reviews:\n        classification = few_shot_movie_review_classifier(review)\n        results.append({\"review\": review, \"classification\": classification})\n    return results\n\n\n# 테스트 리뷰\ntest_reviews = [\n    \"이 영화는 올해 본 영화 중 최고예요! 연출, 음악, 연기 모두 완벽했습니다.\",\n    \"음... 글쎄요. 나쁘지는 않았지만 특별히 좋지도 않았어요.\",\n    \"정말 별로였어요. 2시간이 정말 길게 느껴졌습니다.\",\n    \"기대보다는 좀 실망스러웠지만, 몇몇 장면은 정말 인상적이었어요.\",\n    \"독특한 스토리와 뛰어난 영상미가 인상적이었습니다. 다만 결말이 조금 아쉬웠어요.\",\n]\n\nresults = analyze_reviews(test_reviews)\n\nfor result in results:\n    print(f\"리뷰: {result['review']}\")\n    print(f\"분류: {result['classification']}\")\n    print()\n\n\n리뷰: 이 영화는 올해 본 영화 중 최고예요! 연출, 음악, 연기 모두 완벽했습니다.\n분류: 긍정\n\n리뷰: 음... 글쎄요. 나쁘지는 않았지만 특별히 좋지도 않았어요.\n분류: 중립\n\n리뷰: 정말 별로였어요. 2시간이 정말 길게 느껴졌습니다.\n분류: 부정\n\n리뷰: 기대보다는 좀 실망스러웠지만, 몇몇 장면은 정말 인상적이었어요.\n분류: 중립\n\n리뷰: 독특한 스토리와 뛰어난 영상미가 인상적이었습니다. 다만 결말이 조금 아쉬웠어요.\n분류: 긍정",
    "crumbs": [
      "AI 코딩",
      "프롬프트 코딩"
    ]
  },
  {
    "objectID": "prompt_coding.html#cot-프롬프트",
    "href": "prompt_coding.html#cot-프롬프트",
    "title": "프롬프트 코딩",
    "section": "CoT 프롬프트",
    "text": "CoT 프롬프트\n생각의 사슬(Chain-of-Thought, CoT) 프롬프팅은 복잡한 추론 작업에서 AI 모델의 성능을 크게 향상시킨다. 모델이 최종 답변을 제시하기 전에 중간 사고 과정을 명시적으로 보여주도록 유도하기 때문에, 모델은 복잡한 문제를 단계별로 접근하고, 각 단계에서의 논리적 흐름을 명확히 할 수 있다. 결과적으로 최종 답변의 정확성이 높아지고, 오류가 줄어드는 효과를 얻을 수 있다.\nCoT 프롬프팅은 AI 모델이 처리하는 작업의 복잡성이 증가함에 따라 더욱 부각된다. 수학 문제 해결, 논리적 추론, 다단계 의사결정 등의 작업에서 CoT는 모델의 사고 과정을 투명하게 만들어 사용자가 결과를 이해하고 검증하는 데 도움을 줄 뿐만 아니라, 모델 자체의 학습에도 긍정적인 영향을 미쳐, 보다 체계적이고 논리적인 사고 능력을 기를 수 있게 한다.\nCoT 방식이 제공하는 상세한 풀이 과정과 일반적인 방식의 간단한 답변을 비교할 수 있다. 한 가게에서 20% 할인하는 상품에 대한 할인가격을 구하는 코드를 다음과 같이 작성할 수 있다. CoT 방식은 특히 복잡한 추론이 필요한 문제에서 그 우수성이 두드러지며, 사용자가 AI의 사고 과정을 이해하고 검증하는 데 큰 도움을 준다.\n\n\n코드\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef get_response(messages):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0.2\n    )\n    return response.choices[0].message.content\n\n\ndef solve_problem_with_cot(problem):\n    prompt = f\"\"\"문제를 단계별로 해결해주세요. 각 단계에서의 사고 과정을 명확히 설명하고, 최종 답변을 제시하세요.\n\n문제: {problem}\n\n풀이 과정:\n1) \"\"\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"당신은 수학 문제를 논리적으로 해결하는 전문가입니다.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n\n    return get_response(messages)\n\n\ndef solve_problem_without_cot(problem):\n    prompt = f\"\"\"다음 문제의 답을 구하세요:\n\n문제: {problem}\n\n답변:\"\"\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"당신은 수학 문제를 해결하는 전문가입니다.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n\n    return get_response(messages)\n\n\n# 테스트 문제\nproblem = \"\"\"\n한 가게에서 20% 할인 행사를 하고 있습니다. 원래 가격이 80,000원인 제품을 구매하고 5,000원 쿠폰을 사용했다면, 최종 지불 금액은 얼마인가요?\n또한, 이 금액은 원래 가격의 몇 퍼센트에 해당하나요? (퍼센트는 소수점 첫째 자리까지 계산하세요.)\n\"\"\"\n\nprint(\"Chain-of-Thought 방식의 풀이:\")\ncot_solution = solve_problem_with_cot(problem)\nprint(cot_solution)\n\nprint(\"\\n일반적인 방식의 풀이:\")\nnormal_solution = solve_problem_without_cot(problem)\nprint(normal_solution)\n\n\nChain-of-Thought 방식의 풀이:\n원래 가격이 80,000원이고 20% 할인을 받는다면, 할인된 가격은 얼마인지 계산해보겠습니다.\n\n할인된 가격 = 원래 가격 - (원래 가격 * 할인율)\n할인된 가격 = 80,000 - (80,000 * 0.20)\n할인된 가격 = 80,000 - 16,000\n할인된 가격 = 64,000원\n\n따라서, 할인된 가격은 64,000원입니다.\n\n2) 5,000원 쿠폰을 사용했으므로, 최종 지불 금액은 할인된 가격에서 쿠폰 금액을 빼면 됩니다.\n\n최종 지불 금액 = 할인된 가격 - 쿠폰 금액\n최종 지불 금액 = 64,000 - 5,000\n최종 지불 금액 = 59,000원\n\n따라서, 최종 지불 금액은 59,000원입니다.\n\n3) 최종 지불 금액이 원래 가격의 몇 퍼센트에 해당하는지 계산해보겠습니다.\n\n할인된 가격과 최종 지불 금액의 비율을 구하면 됩니다.\n\n비율 = (최종 지불 금액 / 원래 가격) * 100\n비율 = (59,000 / 80,000) * 100\n비율 = 0.7375 * 100\n비율 = 73.75%\n\n따라서, 최종 지불 금액은 원래 가격의 73.75%에 해당합니다.\n\n일반적인 방식의 풀이:\n먼저 20% 할인을 적용한 가격은 80,000원에서 20% 할인을 적용하여 80,000원 * 0.8 = 64,000원 입니다. 이후 5,000원 쿠폰을 사용하면 최종 지불 금액은 64,000원 - 5,000원 = 59,000원 입니다.\n\n따라서 최종 지불 금액은 59,000원이며, 이는 원래 가격 80,000원의 몇 퍼센트인지 계산하면 (59,000원 / 80,000원) * 100 ≈ 73.8% 입니다. \n\n따라서 최종 지불 금액은 원래 가격의 약 73.8%에 해당합니다.",
    "crumbs": [
      "AI 코딩",
      "프롬프트 코딩"
    ]
  },
  {
    "objectID": "prompt_coding.html#자기검증-프롬프트",
    "href": "prompt_coding.html#자기검증-프롬프트",
    "title": "프롬프트 코딩",
    "section": "자기검증 프롬프트",
    "text": "자기검증 프롬프트\n자기 검증 프롬프팅(Self-consistency prompting)은 AI 모델의 일관성과 신뢰성을 향상시키는 중요한 기법으로 동일한 문제에 대해 여러 번의 추론 과정을 생성하고, 그 결과들 중 다수결로 최종 답변을 선택한다. 이를 통해 모델의 단일 추론 과정에서 발생할 수 있는 오류나 편향을 줄일 수 있으며, 특히 복잡하거나 모호한 문제에 직면했을 때, 다양한 관점에서의 접근을 통해 더 균형 잡힌 결론을 도출할 수 있다.\nSelf-consistency prompting의 우수성은 그 안정성과 신뢰성에 있다. 여러 번의 추론 과정을 통해 모델은 자체적으로 결과를 검증하고 보완할 수 있는 기회를 갖게 된다. 이는 단일 추론에 의존할 때보다 더 견고한 결과를 제공하며, 모델의 불확실성을 줄이는 데 도움을 준다. 또한, 이 방법은 모델이 다양한 사고 경로를 탐색하도록 유도하여, 더 창의적이고 포괄적인 해결책을 찾는 데 기여한다. 결과적으로 Self-consistency prompting은 AI 시스템의 신뢰성을 높이고, 더 정확하고 일관된 결과를 제공하는 데 중요한 역할을 한다.\n\n\n코드\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nfrom collections import Counter\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef get_response(messages):\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        temperature=0.7,  # 다양한 답변을 얻기 위해 temperature를 높입니다\n    )\n    return response.choices[0].message.content\n\n\ndef solve_problem_with_self_consistency(problem, num_attempts=5):\n    solutions = []\n\n    for _ in range(num_attempts):\n        prompt = f\"\"\"문제를 단계별로 해결해주세요. 각 단계에서의 사고 과정을 명확히 설명하고, 최종 답변을 제시하세요.\n        마지막에는 \"최종 답변: [당신의 답변]\" 형식으로 답을 명확히 적어주세요.\n\n        문제: {problem}\n\n        풀이 과정:\n        1) \"\"\"\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"당신은 수학 문제를 논리적으로 해결하는 전문가입니다.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        solution = get_response(messages)\n        solutions.append(solution)\n\n    return solutions\n\n\ndef extract_final_answer(solution):\n    lines = solution.split(\"\\n\")\n    for line in reversed(lines):\n        if line.startswith(\"최종 답변:\"):\n            return line.split(\":\")[1].strip()\n    return \"답변을 찾을 수 없습니다.\"\n\n\ndef get_majority_vote(answers):\n    return Counter(answers).most_common(1)[0][0]\n\n\n# 테스트 문제\nproblem = \"\"\"\n한 농장에서 닭, 소, 돼지를 키우고 있습니다. \n이 농장의 동물들의 다리 수의 합계는 180개이고, 머리 수의 합계는 60개입니다.\n닭의 수는 소의 수의 두 배입니다. \n각 동물의 수를 구하세요.\n\"\"\"\n\nsolutions = solve_problem_with_self_consistency(problem)\n\nprint(\"각 시도의 풀이 과정과 답변:\")\nfor i, solution in enumerate(solutions, 1):\n    print(f\"\\n시도 {i}:\")\n    print(solution)\n    print(\"-\" * 50)\n\nfinal_answers = [extract_final_answer(solution) for solution in solutions]\nmajority_answer = get_majority_vote(final_answers)\n\nprint(\"\\n최종 다수결 답변:\")\nprint(majority_answer)\n\n\n각 시도의 풀이 과정과 답변:\n\n시도 1:\n1) 문제를 이해하고, 주어진 정보를 식으로 표현합니다.\n   - 닭의 수를 C, 소의 수를 S, 돼지의 수를 P라고 하겠습니다.\n   - 닭, 소, 돼지의 다리 수를 각각 2, 4, 4로 가정합니다.\n   - 머리 수의 합계는 60개입니다. 즉, C + S + P = 60\n   - 다리 수의 합계는 180개입니다. 즉, 2C + 4S + 4P = 180\n   - 닭의 수는 소의 수의 두 배입니다. 즉, C = 2S\n\n2) 주어진 식을 정리합니다.\n   - 첫 번째 식: C + S + P = 60\n   - 두 번째 식: 2C + 4S + 4P = 180\n   - 세 번째 식: C = 2S\n\n3) 세 번째 식을 첫 번째 식에 대입합니다.\n   - C = 2S이므로, 첫 번째 식은 2S + S + P = 60이 됩니다.\n   - 따라서, 3S + P = 60\n\n4) 세 번째 식을 두 번째 식에 대입합니다.\n   - C = 2S이므로, 두 번째 식은 2(2S) + 4S + 4P = 180이 됩니다.\n   - 따라서, 4S + 4S + 4P = 180\n   - 즉, 8S + 4P = 180\n\n5) 네 번째 식을 정리합니다.\n   - 8S + 4P = 180을 양변을 4로 나누면, 2S + P = 45가 됩니다.\n\n6) 3S + P = 60과 2S + P = 45를 연립해서 풀어봅니다.\n   - 3S + P = 60\n   - 2S + P = 45\n\n7) 두 식을 빼서 S를 구합니다.\n   - (3S + P) - (2S + P) = 60 - 45\n   - S = 15\n\n8) S 값을 첫 번째 식에 대입해서 P를 구합니다.\n   - 2S + P = 45에서 2(15) + P = 45\n   - 30 + P = 45\n   - P = 15\n\n9) C 값을 세 번째 식에서 구합니다.\n   - C = 2S에서 C = 2(15)\n   - C = 30\n\n최종 답변: 닭의 수는 30마리, 소의 수는 15마리, 돼지의 수는 15마리입니다.\n--------------------------------------------------\n\n시도 2:\n풀이 과정:\n1) 문제를 이해하고, 각 동물의 다리 수와 머리 수의 관계를 설정합니다.\n    - 닭: 다리 2개, 머리 1개\n    - 소: 다리 4개, 머리 1개\n    - 돼지: 다리 4개, 머리 1개\n\n2) 변수를 설정합니다.\n    - 닭의 수를 \\( C \\) (Chicken)라 하겠습니다.\n    - 소의 수를 \\( C/2 \\) (Cow)라 하겠습니다. (문제에서 닭의 수가 소의 수의 두 배라고 주어졌으므로)\n    - 돼지의 수를 \\( P \\) (Pig)라 하겠습니다.\n\n3) 주어진 조건을 수식으로 표현합니다.\n    - 머리 수의 합계: \\( C + \\frac{C}{2} + P = 60 \\)\n    - 다리 수의 합계: \\( 2C + 4 \\left( \\frac{C}{2} \\right) + 4P = 180 \\)\n\n4) 각 수식을 단순화합니다.\n    - 머리 수의 합계: \\( C + \\frac{C}{2} + P = 60 \\)\n    - 다리 수의 합계: \\( 2C + 2C + 4P = 180 \\)\n\n5) 각 수식을 더 단순화하여 풀기 쉽게 합니다.\n    - 머리 수의 합계: \\( \\frac{3C}{2} + P = 60 \\)\n    - 다리 수의 합계: \\( 4C + 4P = 180 \\)\n\n6) 첫 번째 식을 \\(P\\)에 대해 정리합니다.\n    - \\( P = 60 - \\frac{3C}{2} \\)\n\n7) 두 번째 식에 \\( P \\)의 값을 대입하여 \\(C\\)에 대해 정리합니다.\n    - \\( 4C + 4 \\left( 60 - \\frac{3C}{2} \\right) = 180 \\)\n    - \\( 4C + 240 - 6C = 180 \\)\n    - \\( -2C + 240 = 180 \\)\n    - \\( -2C = -60 \\)\n    - \\( C = 30 \\)\n\n8) 닭의 수를 구했습니다. 이제 소와 돼지의 수를 구합니다.\n    - \\( 소의 수 = \\frac{C}{2} = \\frac{30}{2} = 15 \\)\n    - \\( 돼지의 수 = P = 60 - \\frac{3 \\times 30}{2} = 60 - 45 = 15 \\)\n\n각 동물의 수는:\n    - 닭: 30마리\n    - 소: 15마리\n    - 돼지: 15마리\n\n최종 답변: 닭 30마리, 소 15마리, 돼지 15마리\n--------------------------------------------------\n\n시도 3:\n풀이 과정:\n1) 문제를 이해하고 변수 정의:\n   - 닭의 수를 \\( C \\),\n   - 소의 수를 \\( S \\),\n   - 돼지의 수를 \\( P \\)로 정의합니다.\n\n2) 주어진 조건을 수식으로 표현:\n   - 닭의 수는 소의 수의 두 배: \\( C = 2S \\)\n   - 다리 수의 합계는 180개:\n     \\( 2C + 4S + 4P = 180 \\) (닭은 다리가 2개, 소와 돼지는 각각 다리가 4개)\n   - 머리 수의 합계는 60개:\n     \\( C + S + P = 60 \\)\n\n3) 앞서 정의한 변수와 조건을 바탕으로 식을 정리:\n   - \\( C = 2S \\)를 \\( C \\)의 위치에 대입:\n     \\( 2(2S) + 4S + 4P = 180 \\)\n     \\( 4S + 4S + 4P = 180 \\)\n     \\( 8S + 4P = 180 \\)\n\n4) 식을 간단히 정리:\n   - \\( 8S + 4P = 180 \\)을 4로 나눕니다:\n     \\( 2S + P = 45 \\)\n\n5) 두 번째 조건을 사용하여 두 번째 식을 정리:\n   - \\( C + S + P = 60 \\)에 \\( C = 2S \\)를 대입:\n     \\( 2S + S + P = 60 \\)\n     \\( 3S + P = 60 \\)\n\n6) 두 식을 연립하여 풉니다:\n   - \\( 2S + P = 45 \\)\n   - \\( 3S + P = 60 \\)\n\n   위의 두 식에서 \\( P \\)를 제거하기 위해, 두 번째 식에서 첫 번째 식을 뺍니다:\n   \\( (3S + P) - (2S + P) = 60 - 45 \\)\n   \\( 3S - 2S = 15 \\)\n   \\( S = 15 \\)\n\n7) \\( S \\)의 값을 첫 번째 식에 대입하여 \\( P \\)를 구합니다:\n   - \\( 2S + P = 45 \\)\n   - \\( 2(15) + P = 45 \\)\n   - \\( 30 + P = 45 \\)\n   - \\( P = 45 - 30 \\)\n   - \\( P = 15 \\)\n\n8) \\( C \\)의 값을 구합니다:\n   - \\( C = 2S \\)\n   - \\( C = 2(15) \\)\n   - \\( C = 30 \\)\n\n9) 최종 확인:\n   - 머리 수: \\( C + S + P = 30 + 15 + 15 = 60 \\) (조건 만족)\n   - 다리 수: \\( 2C + 4S + 4P = 2(30) + 4(15) + 4(15) = 60 + 60 + 60 = 180 \\) (조건 만족)\n\n최종 답변: 닭 30마리, 소 15마리, 돼지 15마리\n--------------------------------------------------\n\n시도 4:\n풀이 과정:\n1) 문제를 이해하고, 각 동물의 수를 변수로 정의합니다.\n    - 닭의 수를 C, 소의 수를 S, 돼지의 수를 P라고 하겠습니다.\n\n2) 문제에서 주어진 조건을 식으로 표현합니다.\n    - 닭의 수는 소의 수의 두 배입니다: \\( C = 2S \\)\n    - 다리 수의 합계는 180개입니다: \\( 2C + 4S + 4P = 180 \\) (닭은 2다리, 소와 돼지는 4다리)\n    - 머리 수의 합계는 60개입니다: \\( C + S + P = 60 \\)\n\n3) 위의 식들을 정리하여 해결해보겠습니다.\n    - \\( C = 2S \\) 이므로, 이를 다른 두 식에 대입합니다.\n    - \\( 2(2S) + 4S + 4P = 180 \\) (다리 수의 식에 대입)\n    - \\( 4S + 4S + 4P = 180 \\)\n    - \\( 8S + 4P = 180 \\)\n    - \\( 2S + P = 30 \\) (양변을 4로 나눔)\n\n    - \\( C = 2S \\) 이므로, 이를 머리 수의 식에 대입합니다.\n    - \\( 2S + S + P = 60 \\)\n    - \\( 3S + P = 60 \\)\n\n4) 이제 두 개의 연립방정식을 풀어봅니다.\n    - \\( 2S + P = 30 \\)\n    - \\( 3S + P = 60 \\)\n\n    두 식을 빼서 \\( P \\)를 소거합니다.\n    - \\( (3S + P) - (2S + P) = 60 - 30 \\)\n    - \\( S = 30 \\)\n\n    이제 \\( S \\)를 구했으므로, 이를 이용해 \\( P \\)를 구합니다.\n    - \\( 2S + P = 30 \\)\n    - \\( 2(30) + P = 30 \\)\n    - \\( 60 + P = 30 \\)\n    - \\( P = 30 - 60 \\)\n    - \\( P = -30 \\)\n\n    위 식에서 \\( P \\) 값이 음수가 되므로, \\( S = 30 \\)이 잘못된 값을 줍니다. 따라서 처음부터 다시 점검해봅니다.\n    - \\( 2S + P = 30 \\)\n    - \\( 3S + P = 60 \\)\n\n    첫 번째 식에서 \\( P \\)를 구합니다.\n    - \\( P = 30 - 2S \\)\n\n    두 번째 식에 대입합니다.\n    - \\( 3S + (30 - 2S) = 60 \\)\n    - \\( S + 30 = 60 \\)\n    - \\( S = 30 \\)\n\n    위에서 잘못된 값이므로 다른식을 사용해봅니다.\n    - \\( 2C + 4S + 4P = 180 \\)\n    - \\( C = 2S \\)\n    - \\( 3S + P = 60 \\)\n\n    새로운 방식으로\n    - \\( C = S + P = 180 \\)\n\n5) 두 개의 연립방정식을 다시 점검해봅니다.\n    - \\( 2S + P = 30 \\)\n    - \\( 3S + P = 60 \\)\n\n최종 답변: [문제 해결 필요]\n--------------------------------------------------\n\n시도 5:\n1) 문제의 조건을 수학적으로 정리합니다.\n- 닭의 수를 \\( C \\) (Chicken)라고 합시다.\n- 소의 수를 \\( S \\) (Cow)라고 합시다.\n- 돼지의 수를 \\( P \\) (Pig)라고 합시다.\n\n주어진 조건에 따라 다음과 같은 식을 세울 수 있습니다.\n\n2) 각 동물의 다리 수와 머리 수를 고려합니다.\n- 닭은 다리가 2개, 머리가 1개 있습니다.\n- 소는 다리가 4개, 머리가 1개 있습니다.\n- 돼지는 다리가 4개, 머리가 1개 있습니다.\n\n따라서 다리 수와 머리 수에 대한 식은 다음과 같습니다.\n- 다리 수의 합계: \\( 2C + 4S + 4P = 180 \\)\n- 머리 수의 합계: \\( C + S + P = 60 \\)\n\n3) 추가 조건을 반영합니다.\n- 닭의 수는 소의 수의 두 배입니다: \\( C = 2S \\)\n\n4) 세 개의 식을 정리합니다.\n- \\( 2C + 4S + 4P = 180 \\)\n- \\( C + S + P = 60 \\)\n- \\( C = 2S \\)\n\n5) \\( C = 2S \\)를 다른 두 식에 대입하여 \\( C \\)를 제거합니다.\n첫 번째 식:\n- \\( 2(2S) + 4S + 4P = 180 \\)\n- \\( 4S + 4S + 4P = 180 \\)\n- \\( 8S + 4P = 180 \\)\n- \\( 2S + P = 45 \\) (양변을 4로 나눔)\n\n두 번째 식:\n- \\( 2S + S + P = 60 \\)\n- \\( 3S + P = 60 \\)\n\n6) 두 식을 연립하여 풉니다.\n- \\( 2S + P = 45 \\)\n- \\( 3S + P = 60 \\)\n\n두 식을 빼서 \\( P \\)를 제거합니다.\n- \\( (3S + P) - (2S + P) = 60 - 45 \\)\n- \\( S = 15 \\)\n\n7) \\( S = 15 \\)를 이용해 \\( P \\)를 구합니다.\n- \\( 2S + P = 45 \\)\n- \\( 2(15) + P = 45 \\)\n- \\( 30 + P = 45 \\)\n- \\( P = 15 \\)\n\n8) \\( S = 15 \\)를 이용해 \\( C \\)를 구합니다.\n- \\( C = 2S \\)\n- \\( C = 2(15) \\)\n- \\( C = 30 \\)\n\n따라서 각 동물의 수는:\n- 닭: 30마리\n- 소: 15마리\n- 돼지: 15마리\n\n최종 답변: 닭 30마리, 소 15마리, 돼지 15마리\n--------------------------------------------------\n\n최종 다수결 답변:\n닭 30마리, 소 15마리, 돼지 15마리",
    "crumbs": [
      "AI 코딩",
      "프롬프트 코딩"
    ]
  },
  {
    "objectID": "function_calling.html",
    "href": "function_calling.html",
    "title": "함수 호출",
    "section": "",
    "text": "함수 호출(function calling)은 대규모 언어 모델(LLM)의 능력을 확장하고 특정 작업을 수행하는 강력한 메커니즘이다. LLM은 사용자의 입력을 분석하고, 적절한 함수를 선택하여 필요한 매개변수와 함께 호출한다. 이 과정을 통해 LLM은 실시간 데이터 검색, 계산 수행, 외부 시스템과의 상호작용 등 다양한 작업을 수행할 수 있다. 함수 호출은 LLM의 자연어 처리 능력과 구조화된 작업 수행 능력을 결합하여, 더 정확하고 맥락에 맞는 응답을 생성한다.\n함수 호출의 주요 장점은 유연성과 확장성에 있다. 개발자는 특정 도메인이나 사용 사례에 맞는 사용자 정의 함수를 정의할 수 있으며, 이를 통해 LLM의 기능을 크게 확장할 수 있다. 예를 들어, 날씨 정보 검색, 데이터베이스 쿼리, 예약 시스템 접근 등 다양한 기능을 LLM에 통합할 수 있다.",
    "crumbs": [
      "AI 코딩",
      "함수 호출"
    ]
  },
  {
    "objectID": "function_calling.html#json-출력",
    "href": "function_calling.html#json-출력",
    "title": "함수 호출",
    "section": "JSON 출력",
    "text": "JSON 출력\n함수 호출을 JSON 형식으로 구조화하는 것은 데이터의 일관성과 해석의 용이성을 크게 향상시킨다. JSON은 인간과 기계 모두가 쉽게 읽고 이해할 수 있는 형식으로, 복잡한 데이터 구조를 명확하게 표현할 수 있어, 특히 다양한 유형의 정보를 포함하는 함수 호출에서 매우 유용하며, 파라미터의 이름과 값을 명확히 연결하여 오류의 가능성을 줄인다.\n또한, JSON을 사용한 함수 호출은 시스템 간의 상호 운용성을 크게 향상시킨다. 많은 프로그래밍 언어와 플랫폼이 JSON을 기본적으로 지원하기 때문에, 다양한 환경에서 쉽게 통합될 수 있다. 웹 기반 애플리케이션이나 마이크로서비스 아키텍처에서 중요하며, API 통신의 표준으로 자리 잡았다. 결과적으로, JSON을 통한 함수 호출은 개발 과정을 간소화하고, 시스템의 유지보수성과 확장성을 향상시키는 데 크게 기여한다.",
    "crumbs": [
      "AI 코딩",
      "함수 호출"
    ]
  },
  {
    "objectID": "function_calling.html#헬로월드",
    "href": "function_calling.html#헬로월드",
    "title": "함수 호출",
    "section": "헬로월드",
    "text": "헬로월드\n함수 호출 헬로월드 프로그램은 OpenAI의 GPT 모델을 사용하여 간단한 인사 기능을 구현했다. 프로그램은 먼저 hello_world라는 함수를 정의하고, 이 함수의 설명을 OpenAI API에 제공한 다음, 사용자의 입력을 API에 전송하면, GPT 모델이 이를 해석하여 적절한 함수 호출을 결정한다. API가 hello_world 함수 호출을 요청하면, 프로그램은 이 함수를 실행하여 개인화된 인사말을 생성하고 출력합니다.\n\n\n코드\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nimport json\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\n# 간단한 함수 정의\ndef hello_world(name):\n    return f\"안녕하세요, {name}님!\"\n\n\n# OpenAI에 제공할 함수 설명\nfunctions = [\n    {\n        \"name\": \"hello_world\",\n        \"description\": \"이름을 받아 인사말을 반환하는 함수\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"name\": {\"type\": \"string\", \"description\": \"사용자의 이름\"}},\n            \"required\": [\"name\"],\n        },\n    }\n]\n\n# OpenAI API 호출\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"내 이름은 홍길동이야. 인사해줘.\"}],\n    functions=functions,\n    function_call=\"auto\",\n)\n\n# 응답 처리 및 함수 실행\nmessage = response.choices[0].message\nif message.function_call:\n    function_args = json.loads(message.function_call.arguments)\n    result = hello_world(function_args[\"name\"])\n    print(result)\n\n\n안녕하세요, 홍길동님!",
    "crumbs": [
      "AI 코딩",
      "함수 호출"
    ]
  },
  {
    "objectID": "function_calling.html#날씨-api",
    "href": "function_calling.html#날씨-api",
    "title": "함수 호출",
    "section": "날씨 API",
    "text": "날씨 API\n날씨 API 호출 코드는 OpenAI의 GPT 모델과 OpenWeatherMap API를 결합하여 사용자의 날씨 관련 질문에 답한다. 먼저 OpenWeatherMap API를 사용하여 특정 도시의 날씨 정보를 가져오는 get_weather 함수를 정의했다. 그 다음 이 함수의 설명을 OpenAI API에 제공하고, 사용자의 자연어 입력을 처리하도록 GPT 모델에 요청했다. GPT 모델은 사용자의 질문을 해석하여 적절한 도시 이름을 추출하고 get_weather 함수를 호출했다. 마지막으로 함수의 반환값을 출력하여 사용자에게 현재 날씨 정보를 제공했다. 이 과정을 통해 자연어 처리와 실시간 데이터 조회를 결합한 실용적인 AI 애플리케이션의 기본 구조를 구현했다.\n\n\n코드\nfrom openai import OpenAI\nimport requests\nimport os\nfrom dotenv import load_dotenv\n\n# 환경 변수 로드\nload_dotenv()\n\n# OpenAI 및 OpenWeatherMap API 키 설정\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\nweather_api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n\n# OpenAI 클라이언트 초기화\nclient = OpenAI(api_key=openai_api_key)\n\ndef get_weather(city):\n    \"\"\"OpenWeatherMap API를 사용하여 날씨 정보를 가져오는 함수\"\"\"\n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n    params = {\"q\": city, \"appid\": weather_api_key, \"units\": \"metric\"}  # 섭씨 온도 사용\n    response = requests.get(base_url, params=params)\n    if response.status_code == 200:\n        data = response.json()\n        weather_desc = data[\"weather\"][0][\"description\"]\n        temp = data[\"main\"][\"temp\"]\n        return f\"{city}의 현재 날씨는 {weather_desc}이며, 기온은 {temp}°C 입니다.\"\n    else:\n        return \"날씨 정보를 가져오는데 실패했습니다.\"\n\n\n# OpenAI에 제공할 함수 설명\nfunctions = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"특정 도시의 현재 날씨 정보를 가져오는 함수\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"city\": {\"type\": \"string\", \"description\": \"날씨를 알고싶은 도시 이름\"}\n            },\n            \"required\": [\"city\"],\n        },\n    }\n]\n\n# 사용자 입력 처리 및 함수 호출\nuser_input = \"속초시 오늘 날씨 어때?\"\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": user_input}],\n    functions=functions,\n    function_call=\"auto\",\n)\n\n# 응답 처리 및 함수 실행\nmessage = response.choices[0].message\nif message.function_call:\n    function_args = eval(message.function_call.arguments)\n    result = get_weather(function_args[\"city\"])\n    print(result)\nelse:\n    print(message.content)\n\n\n날씨 정보를 가져오는데 실패했습니다.",
    "crumbs": [
      "AI 코딩",
      "함수 호출"
    ]
  }
]