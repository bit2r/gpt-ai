[
  {
    "objectID": "langchain.html",
    "href": "langchain.html",
    "title": "랭체인",
    "section": "",
    "text": "랭체인(langcahin)은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 소프트웨어 개발 프레임워크로, LLM을 다양한 애플리케이션과 통합하는 것을 용이하기 쉽기 때문에 인기를 얻고 있다. 랭체인은 LLM과 인터페이스, 다양한 구성 요소 연결, 메모리 관리 등이 수월하기 때문에 특히 개발자 사이에서 인기가 높다. 랭체인의 주요 목적은 LLM 기반 애플리케이션 개발을 단순화하고 가속화하는 것으로 데이터 처리, 프롬프트 관리, 모델 통합 등 LLM 애플리케이션 개발의 여러 측면을 쉽게 다룰 수 있도록 도구와 추상화를 제공한다.",
    "crumbs": [
      "기본기",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#api",
    "href": "langchain.html#api",
    "title": "랭체인",
    "section": "API",
    "text": "API\n\n허깅페이스\n파이썬과 R을 사용해 Hugging Face Hub의 대형 언어 모델(Large Language Model, LLM)을 활용한다. 파이썬에서는 필요한 라이브러리를 설치하고, R에서는 reticulate 라이브러리를 통해 파이썬 환경을 사용한다. 파이썬 코드에서 Hugging Face Hub에 접근하기 위한 API 토큰을 로드하고, HuggingFaceHub 클래스를 사용하여 특정 모델(‘tiiuae/falcon-7b-instruct’)에 질문을 하고, 모델의 답변을 출력한다.\n\npip install langchain_community, pip install dotenv, pip install langchain-huggingface: 이 세 명령어는 파이썬 환경에서 필요한 패키지들을 설치한다. langchain_community는 언어 체인 커뮤니티 라이브러리, dotenv는 환경 변수를 관리하는 라이브러리, huggingface_hub는 Hugging Face Hub와 연동하는 데 사용되는 라이브러리다.\nR 코드 부분에서 library(reticulate)를 사용해 파이썬과 R 사이의 상호작용을 가능하게 하는 reticulate 라이브러리를 로드한다. use_condaenv(\"langchain\", required = TRUE)는 langchain이라는 이름의 Conda 환경을 사용하도록 지시한다. 이는 파이썬 코드를 R 환경에서 실행하기 위한 준비 단계다.\n파이썬 코드에서는 먼저 langchain_community.llms에서 HuggingFaceHub 클래스를, dotenv에서 load_dotenv 함수를 가져온다. 이후 os 모듈을 임포트한다. load_dotenv()를 호출하여 환경 변수를 로드한다. 이는 .env 파일에 저장된 환경 변수를 사용할 수 있게 한다.\nhuggingfacehub_api_token = os.getenv('HF_TOKEN')는 환경 변수에서 ’HF_TOKEN’을 찾아 해당 토큰을 변수에 저장한다. 이 토큰은 Hugging Face Hub에 접근할 때 인증을 위해 사용된다.\nHuggingFaceHub 클래스의 인스턴스를 생성한다. 이 때 repo_id에는 사용할 Hugging Face 모델의 저장소 ID를, huggingfacehub_api_token에는 위에서 얻은 API 토큰을 넣는다.\n대형 언어 모델에 질문을 하기 위해 question 변수에 질문을 저장하고, llm.invoke(question)을 호출하여 모델에 질문을 전달하고 결과를 받는다.\n마지막으로 print(output)을 통해 얻은 결과를 출력한다. 이 코드는 Hugging Face Hub의 특정 모델을 사용하여 질문에 대한 답변을 얻는 과정을 보여준다.\n\n\npip install langchain_community\npip install dotenv\npip install -U langchain-huggingface\n\n다양한 한국어가 지원되는 언어모형을 실험해봤지만 언어모형 크기가 큰 경우 실행이 불가능하다.\n\n\n코드\nfrom langchain_huggingface import HuggingFaceEndpoint\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nhuggingfacehub_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\nhuggingfacehub_api_token\n\nllm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', \n                     huggingfacehub_api_token = huggingfacehub_api_token)\n\nquestion = 'What is LLM in AI?'\noutput = llm.invoke(question)\n\nprint(output)\n\n\n\n\nOpenAI\npip install --upgrade langchain openai 명령어로 openai 패키지를 설치하고 pip install langchain-openai 명령어로 랭체인 인터페이스를 사용해서 LLM 을 활용한다.\n\n\n코드\nfrom langchain_openai import OpenAI\n\nopenai_api_key = os.getenv('OPENAI_API_KEY')\n\nllm = OpenAI(\n    model_name=\"gpt-3.5-turbo-instruct\",   \n    openai_api_key=openai_api_key\n)\nquestion = '인공지능 대규모 언어모형 LLM이 뭐야'\noutput = llm.invoke(question)\nprint(output)\n\n\nLLM은 'Large Language Model'의 약자로, 인공지능 기술 중 하나인 자연어 처리(Natural Language Processing) 분야에서 사용되는 대규모 언어모형을 말합니다. LLM은 수많은 문장과 단어를 학습하고 이를 바탕으로 텍스트를 생성하고 이해하는 기술을 갖춘 인공지능 모델을 의미합니다. 이를 통해 LLM은 인간과 유사한 수준의 언어 이해 및 생성 능력을 가지고 있습니다. LLM은 다양한 분야에서 활용되고 있으며, 텍스트 생성, 기계 번역, 자연어 이해 등 다양한 응용 분야에서 성능을 발휘하고 있습니다.\n\n\n클로드\npip install -U langchain-anthropic 앤트로픽을 설치한 후 동일하게 실행한다.\n\n\n코드\nfrom langchain_anthropic import ChatAnthropic\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nanthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n\nllm = ChatAnthropic(\n    model=\"claude-3-opus-20240229\",\n    anthropic_api_key=anthropic_api_key\n)\n\nquestion = '인공지능 대규모 언어모형 LLM이 뭐야?'\noutput = llm.invoke(question)\nprint(output.content)\n\n\n'인공지능 대규모 언어모형(Large Language Model, LLM)은 방대한 양의 텍스트 데이터를 학습하여 만들어진 거대한 신경망 모델입니다. LLM은 다음과 같은 특징을 가지고 있습니다:\\n\\n1. 데이터 크기: 수백 기가바이트에서 수 테라바이트에 이르는 방대한 텍스트 데이터로 학습합니다.\\n\\n2. 모델 크기: 수십억에서 수조 개의 매개변수를 가진 거대한 신경망 구조를 가집니다. \\n\\n3. 자연어 이해 및 생성: 문맥을 이해하고 자연스러운 언어를 생성할 수 있습니다.\\n\\n4. 다양한 태스크 수행: 질의응답, 요약, 번역, 창작 등 다양한 자연어 처리 태스크를 수행할 수 있습니다.\\n\\n5. 사전 학습과 전이 학습: 대량의 데이터로 사전 학습된 후, 특정 태스크를 위해 추가 학습(전이 학습)될 수 있습니다.\\n\\n대표적인 LLM으로는 GPT-3, BERT, XLNet, T5 등이 있습니다. 이러한 모델들은 자연어 처리 분야에서 혁신을 가져왔으며, 다양한 응용 분야에서 활용되고 있습니다. 그러나 막대한 컴퓨팅 자원이 필요하고, 편향성 문제 등 한계점도 존재합니다. LLM 기술은 계속 발전하고 있으며, 앞으로도 자연어 인공지능 분야를 선도할 것으로 예상됩니다.'",
    "crumbs": [
      "기본기",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#프롬프트-템플릿",
    "href": "langchain.html#프롬프트-템플릿",
    "title": "랭체인",
    "section": "프롬프트 템플릿",
    "text": "프롬프트 템플릿\n프롬프트 템플릿(Prompt Template)은 대규모 언어 모델(LLM)에 입력할 프롬프트의 구조를 정의하는 틀(template)로 일관된 형식의 프롬프트를 생성하고, 동적으로 내용을 채워 넣을 수 있게 해주는 도구다.\n\n재사용성: 동일한 구조의 프롬프트를 여러 번 사용할 수 있다.\n일관성: 프롬프트의 형식을 일정하게 유지할 수 있다.\n동적 내용: 변수를 사용하여 프롬프트의 특정 부분을 동적으로 변경할 수 있다.\n구조화: 컨텍스트, 질문, 응답 형식 등을 체계적으로 구성할 수 있다.\n\n프롬프트 템플릿은 주로 세 가지 요소(컨텍스트, 질문, 응답 형식)로 이루어진다.\n컨텍스트는 모델에게 배경 정보를 제공하고, 질문은 모델에게 요구하는 구체적인 작업을 명시한다. 응답 형식은 모델이 어떤 방식으로 답변해야 하는지 지시한다.\n프롬프트 템플릿에는 변수를 포함시킬 수 있다. 변수는 프롬프트를 생성할 때 동적으로 값이 채워지는 부분으로 동일한 구조의 프롬프트를 다양한 상황에 맞춰 재사용할 수 있다.\n프롬프트 템플릿은 이러한 요소들을 조합해 최종 프롬프트를 만들어내는 과정에서 변수에 실제 값이 할당되고, 전체 프롬프트의 구조가 완성된다. 완성된 최종 프롬프트는 대규모 언어 모델(LLM)에 입력된다. LLM은 이 프롬프트를 받아 처리하고, 요청된 작업에 따라 적절한 응답을 생성해 출력한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n코드\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# .env 파일에서 환경 변수 로드\nload_dotenv()\n\n# 프롬프트 템플릿 정의\ntemplate = \"\"\"\n컨텍스트: {context}\n\n질문: {question}\n\n응답 형식: {response_format}\n\n위의 컨텍스트를 바탕으로 질문에 답하세요. 응답은 제시된 형식을 따라주세요.\n\"\"\"\n\nprompt_template = PromptTemplate(\n    template=template,\n    input_variables=[\"context\", \"question\", \"response_format\"]\n)\n\n# OpenAI LLM 초기화 (API 키는 .env 파일에서 가져옴)\nllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\",   \n             temperature=0.7, \n             openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# 프롬프트 생성\nprompt = prompt_template.format(\n    context=\"랭체인은 대규모 언어 모델을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\",\n    question=\"랭체인의 주요 특징 세 가지는 무엇인가요?\",\n    response_format=\"1. 첫 번째 특징\\n2. 두 번째 특징\\n3. 세 번째 특징\"\n)\n\n# LLM에 프롬프트 전달 및 응답 생성\nresponse = llm.invoke(prompt)\n\nprint(\"생성된 프롬프트:\")\nprint(prompt)\nprint(\"\\nLLM 응답:\")\nprint(response)\n\n\n생성된 프롬프트:\n\n컨텍스트: 랭체인은 대규모 언어 모델을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n\n질문: 랭체인의 주요 특징 세 가지는 무엇인가요?\n\n응답 형식: 1. 첫 번째 특징\n2. 두 번째 특징\n3. 세 번째 특징\n\n위의 컨텍스트를 바탕으로 질문에 답하세요. 응답은 제시된 형식을 따라주세요.\n\n\nLLM 응답:\n\n1. 랭체인은 대규모 언어 모델을 활용하여 자연어 처리 작업을 수행할 수 있습니다.\n2. 랭체인은 다양한 애플리케이션 개발을 지원하기 위한 다양한 API를 제공합니다.\n3. 랭체인은 사용자가 직접 학습한 데이터를 추가하여 모델을 개선할 수 있는 기능을 제공합니다.",
    "crumbs": [
      "기본기",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#lcel",
    "href": "langchain.html#lcel",
    "title": "랭체인",
    "section": "LCEL",
    "text": "LCEL\n랭체인 표현언어(LangChain Expression Language, LCEL)는 LangChain 컴포넌트들을 연결하고 조합하기 위한 선언적 방식의 인터페이스로 복잡한 AI 애플리케이션 구축을 단순화하고 가독성을 높이는 데 도움을 준다. 체인(Chain)은 LCEL의 핵심 개념 중 하나로, 여러 컴포넌트들을 연결하여 하나의 작업 흐름을 만드는 것을 말한다.\n유닉스 파이프(|)를 이해하고 있다면 prompt | model와 같이 프롬프트 템플릿과 LLM 모델을 연결하는 간단한 체인을 생성한다. 프롬프트 | LLM | 출력파서와 같은 패턴이 일반적이다. 프롬프트 템플릿으로 주제를 받아 프롬프트를 완성하고 LLM 모형에 전달하고 출력파서를 통해 원하는 결과물을 출력시킨다.\n\n\n코드\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom dotenv import load_dotenv\nimport os\n\n# .env 파일에서 환경 변수 로드\nload_dotenv()\n\n# 출력 파서 정의\noutput_parser = CommaSeparatedListOutputParser()\n\n# 프롬프트 템플릿 정의\nprompt_template = ChatPromptTemplate.from_template(\"\"\"\n다음 주제에 관련된 키워드를 5개 나열해주세요: {topic}\n\n당신의 응답은 반드시 쉼표로 구분된 단일 단어 목록이어야 합니다.\n\"\"\")\n\n# 모델 정의 (API 키는 .env 파일에서 가져옴)\nmodel = OpenAI(\n    model_name=\"gpt-3.5-turbo-instruct\",   \n    temperature=0.7, \n    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n\n# 체인 구성\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt_template \n    | model \n    | output_parser\n)\n\n# 체인 실행\nresult = chain.invoke(\"인공지능\")\n\nprint(\"인공지능 관련 키워드:\")\nfor keyword in result:\n    print(f\"- {keyword}\")\n\n\n인공지능 관련 키워드:\n- 기계학습\n- 딥러닝\n- 자연어처리\n- 로봇\n- 빅데이터\nLCEL과 Chain을 사용하면 복잡한 AI 로직을 더 쉽게 구조화하고 관리할 수 있으며, 코드의 가독성과 유지보수성을 크게 향상시킬 수 있다.",
    "crumbs": [
      "기본기",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#랭그래프",
    "href": "langchain.html#랭그래프",
    "title": "랭체인",
    "section": "랭그래프",
    "text": "랭그래프\n랭그래프(LangGraph)는 랭체인(LangChain)의 일부로, 에이전트(Agent) 시스템을 설계하기 위한 도구로 복잡한 AI 워크플로우를 구축하는 데 사용된다. 랭그래프는 에이전트의 상태를 추적하고 관리할 수 있는 상태 관리 기능을 제공한다. 이를 통해 장기적인 대화나 복잡한 작업의 진행 상황을 효과적으로 추적할 수 있고, 조건부 로직과 반복을 포함한 복잡한 워크플로우를 정의할 수 있는 유연성을 제공한다. 또한, 다양한 외부 도구와 API를 쉽게 통합할 수 있는 기능도 랭그래프의 중요한 특징으로 AI 시스템의 능력을 확장하고 실제 세계의 다양한 작업을 수행할 수 있게 된다. 특히, 랭그래프는 재사용 가능한 컴포넌트를 만들어 복잡한 시스템을 구축할 수 있는 모듈성을 제공하여 개발 효율성을 높이고 일관성 있는 시스템 구축을 가능하게 한다.\npip install langgraph 명령어로 설치한 후 프롬프트 템플릿에 사칙연산 관련 문제풀이로 LLM을 활용한다. LLMMathChain은 numexpr 패키지도 필요하니 pip install numexpr 명령어로 설치한다.\n\n코드실행결과\n\n\n\n\n코드\nfrom langchain.agents import initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Get OpenAI API key\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\nllm = OpenAI(\n    model_name=\"gpt-3.5-turbo-instruct\",  \n    temperature=0.0, \n    openai_api_key=openai_api_key\n)\n\n# Load the necessary tools\ntools = load_tools([\"llm-math\"], llm=llm)\n\n# Define the prompt for the agent\nprompt = \"\"\"\nYou are a helpful assistant that can perform various mathematical calculations and provide accurate results.\n\"\"\"\n\n# Create the ReAct agent with the prompt\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent_type=\"react\",\n    verbose=True\n)\n\n# List of questions to ask the agent\nquestions = [\n    \"15 더하기 27은 얼마인가요?\",\n    \"144의 제곱근은 얼마인가요?\",\n    \"250의 30%는 얼마인가요?\",\n    \"5 곱하기 8에서 12를 뺀 값은 얼마인가요?\",\n]\n\n# Ask each question to the agent and print the response\nfor question in questions:\n    response = agent.invoke({\"input\": question})\n    print(f\"질문: {question}\")\n    print(f\"답변: {response['output']}\\n\")\n\n\n\n\n&gt; Entering new AgentExecutor chain...\n I should use a calculator to add 15 and 27\nAction: Calculator\n\nObservation: Answer: 42\nThought: I now know the final answer\nFinal Answer: 42\n\n&gt; Finished chain.\n질문: 15 더하기 27은 얼마인가요?\n답변: 42\n\n\n\n&gt; Entering new AgentExecutor chain...\n I should use a calculator to find the square root of 144\nAction: Calculator\nAction Input: 144\nObservation: Answer: 144\n I now know the final answer\nFinal Answer: 12\n\n&gt; Finished chain.\n질문: 144의 제곱근은 얼마인가요?\n답변: 12\n\n\n\n&gt; Entering new AgentExecutor chain...\n We need to find the percentage of 250.\nAction: Calculator\n\nObservation: Answer: 75.0\nThought: We have found the percentage.\nFinal Answer: 75.0\n\n&gt; Finished chain.\n질문: 250의 30%는 얼마인가요?\n답변: 75.0\n\n\n\n&gt; Entering new AgentExecutor chain...\n I should use a calculator to solve this problem.\nAction: Calculator\nAction Input: 5 * 8 - 12\nObservation: Answer: 28\n I now know the final answer.\nFinal Answer: 28\n\n&gt; Finished chain.\n질문: 5 곱하기 8에서 12를 뺀 값은 얼마인가요?\n답변: 28",
    "crumbs": [
      "기본기",
      "랭체인"
    ]
  },
  {
    "objectID": "langchain.html#rag",
    "href": "langchain.html#rag",
    "title": "랭체인",
    "section": "RAG",
    "text": "RAG\n검색 증강 생성(Retrieval Augmented Generation, RAG)는 대규모 언어 모델(LLM)의 성능을 향상시키기 위해 외부 지식을 활용하는 기술로 사용자의 질문이나 프롬프트에 대해 관련성 높은 정보를 검색하고, 이를 원래의 프롬프트와 결합하여 LLM에 제공한다. RAG의 핵심 아이디어는 LLM의 생성 능력과 외부 데이터베이스의 최신 정보를 결합하는 것으로 임베딩(Embedding)이 중요한 역할을 한다. 사용자의 질문과 데이터베이스 내 문서들은 벡터 형태로 변환되며, 이를 통해 의미적 유사성을 기반으로 관련 정보를 빠르게 검색할 수 있다.\nRAG의 작동 과정은 다음과 같다. 먼저, 사용자가 질문을 입력하면 이 질문은 벡터로 변환된다. 그 다음, 이 벡터를 사용해 미리 준비된 벡터 데이터베이스에서 가장 유사한 문서나 정보를 검색한다. 검색된 정보는 원래의 질문과 함께 새로운 프롬프트를 구성하는 데 사용된다. 이렇게 증강된 프롬프트가 LLM에 입력되어 최종 응답을 생성한다.\nLLM은 학습 시점의 데이터에 기반하므로 최신 정보를 반영하지 못할 수 있지만, RAG를 통해 외부 데이터베이스의 최신 정보를 활용할 수 있어 LLM의 환각(hallucination) 문제를 줄이고 더 정확하고 신뢰할 수 있는 응답을 생성하는 데 도움을 준다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy 버전 오류\n\n\n\nAttributeError: np.float_ was removed in the NumPy 2.0 release. Use np.float64 instead.\nCell In[53], line 26\n     24 # 임베딩 모델 및 벡터 저장소 설정\n     25 embeddings = OpenAIEmbeddings()\n---&gt; 26 vectorstore = Chroma.from_documents(texts, embeddings)\n     28 # RAG 체인 설정\n     29 llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nShow Traceback\n\n\n\n\n코드\nfrom langchain.document_loaders import WikipediaLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\nfrom dotenv import load_dotenv\nimport os\n\n# 환경 변수 로드\nload_dotenv()\n\n# OpenAI API 키 설정\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n# 위키백과에서 \"인공지능\" 문서 로드\nloader = WikipediaLoader(\"인공지능\", load_max_docs=1)\ndocuments = loader.load()\n\n# 문서 분할\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_documents(documents)\n\n# 임베딩 모델 및 벡터 저장소 설정\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(texts, embeddings)\n\n# RAG 체인 설정\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever()\n)\n\n# 질문-답변 루프\nwhile True:\n    query = input(\"\\n질문을 입력하세요 (종료하려면 'q' 입력): \")\n    if query.lower() == 'q':\n        break\n    result = qa_chain.invoke(query)\n    print(f\"\\n답변: {result['result']}\")",
    "crumbs": [
      "기본기",
      "랭체인"
    ]
  },
  {
    "objectID": "chat.html",
    "href": "chat.html",
    "title": "",
    "section": "",
    "text": "챗GPT챗GPT 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "챗GPT",
      "챗GPT"
    ]
  },
  {
    "objectID": "chat.html#키보드-단축키",
    "href": "chat.html#키보드-단축키",
    "title": "",
    "section": "키보드 단축키",
    "text": "키보드 단축키\n\n\n코드library(chattr)\nchattr_use(provider = \"LlamaGPT\", path_url = \"~/LlamaGPTJ-chat/build/bin/chat\", model = \"D:/llms/ggml-gpt4all-j.bin\")\n\n\n&gt; chattr_defaults(path = \"D:\\\\llms\\\\ggml-gpt4all-j.bin\", model = \"LlamaGPT\")\n\n── chattr ────────────────────────────────────────────────────────────────────────\n\n── Defaults for: Default ──\n\n── Prompt: \n• Use the R language, the tidyverse, and tidymodels\n\n── Model \n• Provider: LlamaGPT\n• Path/URL: D:\\llms\\ggml-gpt4all-j.bin\n• Model: LlamaGPT\n• Label: GPT4ALL 1.3 (LlamaGPT)\n\n── Model Arguments: \n• threads: 4\n• temp: 0.01\n• n_predict: 1000\n\n── Context: \nMax Data Files: 0\nMax Data Frames: 0\n✖ Chat History\n✖ Document contents",
    "crumbs": [
      "챗GPT",
      "챗GPT"
    ]
  },
  {
    "objectID": "target.html",
    "href": "target.html",
    "title": "",
    "section": "",
    "text": "타겟(target)타겟 코드",
    "crumbs": [
      "타겟(target)",
      "타겟"
    ]
  },
  {
    "objectID": "target.html#정의",
    "href": "target.html#정의",
    "title": "",
    "section": "정의",
    "text": "정의\ntargets는 Will Landau가 개발하고 유지 관리하는 R 프로그래밍 언어용 작업흐름(Workflow) 관리 패키지다.\ntargets의 주요 기능은 다음과 같다.\n\n작업흐름 자동화\n작업흐름 단계 캐싱\n작업흐름 단계 일괄 생성\n작업흐름 수준에서 병렬화\n\n이를 통해 다음과 같은 작업을 할 수 있다.\n\n다른 일을 하다가 프로젝트로 돌아왔을 때, 혼란 없이 그리고 무엇을 하고 있었는지 기억하려 노력하지 않고도 바로 중단했던 부분부터 다시 시작할 수 있다.\n작업흐름를 변경한 다음, 변경의 영향을 받는 부분만 다시 실행한다.\n\n개별 함수를 변경하지 않고도 작업흐름를 대규모로 확장한다.\n\n… 그리고 물론, 이는 다른 사람들이 여러분의 분석을 재현하는 데 도움을 줄 것이다.",
    "crumbs": [
      "타겟(target)",
      "타겟"
    ]
  },
  {
    "objectID": "target.html#누가-targets를-사용해야-하는가",
    "href": "target.html#누가-targets를-사용해야-하는가",
    "title": "",
    "section": "누가 targets를 사용해야 하는가?",
    "text": "누가 targets를 사용해야 하는가?\ntargets는 결코 유일한 작업흐름 관리 소프트웨어가 아니다. 다양한 기능과 사용 사례를 가진 유사한 도구들이 많이 있다. 예를 들어, snakemake는 파이썬에서 인기 있는 작업흐름 도구이고, make는 bash 스크립트를 자동화하기 위해 오랫동안 사용되어 온 도구이다. targets는 특별히 R과 함께 작동하도록 설계되었기 때문에, 주로 R을 사용하거나 사용하려는 경우에 가장 적합하다. 대부분 다른 도구로 코딩한다면, 다른 대안을 고려해 보는 것이 좋다.\n\n재현 가능성이란 무엇인가?\n재현 가능성은 다른 사람들(미래의 자신 포함)이 여러분의 분석을 재현할 수 있는 능력이다. 과학적 분석 결과에 대해 확신을 가질 수 있는 유일한 방법은 그 결과를 재현할 수 있을 때뿐이다. 그러나 재현 가능성은 이분법적 개념(재현 불가능 vs. 재현 가능)이 아니다. 오히려 덜 재현 가능한 것에서 더 재현 가능한 것까지 스펙트럼이 존재한다.\ntargets는 여러분의 분석을 더 재현 가능하게 만드는 데 큰 도움을 준다.\nDocker, conda, renv와 같은 도구로 컴퓨팅 환경을 제어하는 것도 재현 가능성을 더욱 높이는 데 사용할 수 있다.",
    "crumbs": [
      "타겟(target)",
      "타겟"
    ]
  },
  {
    "objectID": "target.html#footnotes",
    "href": "target.html#footnotes",
    "title": "",
    "section": "각주",
    "text": "각주\n\n\n런타임(runtime)은 프로그램이 실제로 실행되는 시간을 의미한다. 코드를 작성하고 컴파일하는 시간은 런타임에 포함되지 않고, 프로그램이 실행을 시작하는 순간부터 종료되는 순간까지의 시간이 런타임이다. 즉, 런타임 동안에는 프로그램이 메모리를 할당받고, 변수들이 값을 가지게 되며, 함수가 호출되고 실행된다.↩︎",
    "crumbs": [
      "타겟(target)",
      "타겟"
    ]
  },
  {
    "objectID": "gcs.html",
    "href": "gcs.html",
    "title": "",
    "section": "",
    "text": "GCP버킷 코드",
    "crumbs": [
      "GCP",
      "버킷"
    ]
  },
  {
    "objectID": "gcs.html#버킷-생성",
    "href": "gcs.html#버킷-생성",
    "title": "",
    "section": "버킷 생성",
    "text": "버킷 생성\n\nGCS\n\nCloud Console 열기: Google Cloud Console로 이동합니다.\n프로젝트 선택: 버킷을 만들 프로젝트를 선택합니다.\nCloud Storage로 이동: 네비게이션 메뉴에서 “Cloud Storage” &gt; “Browser”를 선택합니다.\n버킷 생성:\n\n“Create bucket” 버튼을 클릭합니다.\n버킷 이름을 입력합니다. 버킷 이름은 전 세계에서 유일해야 합니다.\n버킷 위치를 선택합니다. 필요에 따라 특정 지역, 다중 지역 또는 이중 지역을 선택할 수 있습니다.\n기본 스토리지 클래스를 선택합니다. 이는 저장된 데이터의 비용 및 가용성에 영향을 미칩니다.\n접근 제어를 설정합니다. “Uniform” 또는 “Fine-grained” 접근 제어를 선택할 수 있습니다.\n필요한 추가 설정을 구성합니다.\n“Create”를 클릭하여 버킷을 만듭니다.\n\n\n\n\ngsutil 도구\nGoogle Cloud SDK에 포함된 gsutil은 Cloud Storage와 상호작용할 수 있는 명령어 도구다.\n\nGoogle Cloud SDK 설치:\n\nGoogle Cloud SDK 설치 페이지의 지침에 따라 Google Cloud SDK를 설치합니다.\n\nSDK 초기화:\ngcloud init\nGCS 버킷 만들기:\ngsutil mb -p [PROJECT_ID] -l [LOCATION] gs://[BUCKET_NAME]/\n\n[PROJECT_ID]를 Google Cloud 프로젝트 ID로 바꿉니다.\n[LOCATION]을 버킷 위치로 바꿉니다 (예: us-central1, us, EU).\n[BUCKET_NAME]을 고유한 버킷 이름으로 바꿉니다.\n\n예제: sh  gsutil mb -p alookso-id -l asia-northeast3 gs://my-buckets-name/  gstutil ls\n\n\n\n파이썬과 GC 클라이언트\nPython을 사용하여 Google Cloud 클라이언트 라이브러리를 통해 프로그래밍 방식으로 GCS 버킷을 만들 수도 있습니다.\n\nGoogle Cloud Storage 클라이언트 라이브러리 설치:\npip install google-cloud-storage\nPython을 사용하여 버킷 생성:\nfrom google.cloud import storage\n\n# GCS 클라이언트 초기화\nclient = storage.Client()\n\n# 버킷 이름과 위치 정의\nbucket_name = 'your-unique-bucket-name'\nlocation = 'US'  # 또는 'us-central1'과 같은 지역 지정\n\n# 버킷 생성\nbucket = client.bucket(bucket_name)\nnew_bucket = client.create_bucket(bucket, location=location)\n\nprint(f'Bucket {new_bucket.name} created.')\n\n\n\n예제\n아래는 Python을 사용하여 GCS 버킷을 생성하는 전체 예제입니다:\nimport os\nfrom google.cloud import storage\n\n# 인증 설정\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your-service-account-file.json'\n\n# GCS 클라이언트 초기화\nclient = storage.Client()\n\n# 버킷 이름과 위치 정의\nbucket_name = 'your-unique-bucket-name'\nlocation = 'US'  # 또는 'us-central1'과 같은 지역 지정\n\n# 버킷 생성\nbucket = client.bucket(bucket_name)\nnew_bucket = client.create_bucket(bucket, location=location)\n\nprint(f'Bucket {new_bucket.name} created.')",
    "crumbs": [
      "GCP",
      "버킷"
    ]
  },
  {
    "objectID": "gcs.html#버킷-권한",
    "href": "gcs.html#버킷-권한",
    "title": "",
    "section": "버킷 권한",
    "text": "버킷 권한\n서비스 계정에 적절한 권한을 부여해야 합니다. 다음 단계를 따라 권한을 부여할 수 있습니다:\n\nGoogle Cloud Console에 접속합니다.\n프로젝트를 선택합니다.\n왼쪽 메뉴에서 “Storage” 메뉴를 선택합니다.\n버킷명 버킷을 찾아 클릭합니다.\n상단 메뉴에서 “권한” 탭을 클릭합니다.\n“구성원 추가” 버튼을 클릭합니다.\n“새 구성원” 입력란에 youtube-analytics-api@버킷명.iam.gserviceaccount.com 서비스 계정 이메일을 입력합니다.\n역할 선택란에서 “Storage 객체 관리자” 또는 “Storage 객체 생성자” 역할을 선택합니다. 이 역할은 버킷 내의 객체에 대한 읽기/쓰기 권한을 부여합니다.\n“저장” 버튼을 클릭하여 권한을 저장합니다.",
    "crumbs": [
      "GCP",
      "버킷"
    ]
  },
  {
    "objectID": "code_interpreter.html",
    "href": "code_interpreter.html",
    "title": "",
    "section": "",
    "text": "기본기데이터 사이언스 코드",
    "crumbs": [
      "기본기",
      "데이터 사이언스"
    ]
  },
  {
    "objectID": "code_interpreter.html#프롬프트",
    "href": "code_interpreter.html#프롬프트",
    "title": "",
    "section": "프롬프트",
    "text": "프롬프트\n\n데이터 전처리EDA통계모형기계학습성능 최적화시각화분석도구대쉬보드파이프라인모듈 개발\n\n\n\nPrompt: What are the best practices for preprocessing {topic} data using {programming_language_or_framework}?\n\n\nData Cleaning: Identify and address missing values, outliers, and duplicate records. Cleaning your data ensures that you’re working with accurate and reliable information.\nData Transformation: Normalize or standardize numerical features, encode categorical variables, and create new features if necessary. Data transformation enhances the quality of your dataset.\nFeature Engineering: Extract meaningful information from your data to improve the performance of machine learning models. Feature engineering involves creating new features or modifying existing ones to make them more informative.\nData Validation: Ensure data consistency and integrity by performing validation checks. Validate that your data adheres to predefined rules and constraints.\n\n\n\n\nPrompt: How can I perform exploratory data analysis on {topic} data using {programming_language_or_framework}?\n\n\nData Visualization: Create informative plots, charts, histograms, and scatterplots to visualize data distributions, relationships, and trends. Visualization helps in gaining initial insights into the data.\nStatistical Analysis: Compute summary statistics, such as mean, median, and standard deviation, to describe the central tendencies and variability of your data. Statistical tests can reveal relationships and dependencies.\nHypothesis Testing: Formulate hypotheses about your data and conduct statistical tests to validate or reject these hypotheses. Hypothesis testing is useful for making data-driven decisions.\nInteractive Exploration: Leverage libraries and tools available in {programming_language_or_framework} to perform interactive exploration. Interactive visualization and widgets allow for dynamic exploration of the data.\n\n\n\n\nPrompt: What are the most common statistical techniques to analyze {topic} data in {programming_language_or_framework}?\n\n\nRegression Analysis: Use regression techniques to model relationships between variables and make predictions. Linear regression, logistic regression, and polynomial regression are common types.\nClustering: Apply clustering algorithms, such as K-means or hierarchical clustering, to group similar data points together. Clustering helps in segmentation and pattern recognition.\nClassification: Perform classification tasks to categorize data into predefined classes or labels. Decision trees, support vector machines, and neural networks are frequently used for classification.\nTime Series Analysis: Analyze data with temporal components using time series analysis. This technique is essential for understanding trends and patterns over time.\n\n\n\n\nPrompt: Provide a step-by-step guide for implementing a machine learning model for {specific_task} using {programming_language_or_framework}.\n\n\nData Preparation: Begin by preprocessing and cleaning your data, ensuring that it’s in the right format for modeling.\nFeature Selection: Identify and select the most relevant features or variables for your model. Feature selection helps improve model performance and reduce complexity.\nModel Selection: Choose an appropriate machine learning algorithm or model for your task. Consider factors like data size, complexity, and interpretability.\nTraining and Evaluation: Train your chosen model on a portion of your data and evaluate its performance using metrics like accuracy, precision, recall, and F1-score.\nDeployment: If the model performs satisfactorily, deploy it in your application or workflow for making predictions.\n\n\n\n\nPrompt: Explain how to optimize {topic} data analysis performance in {programming_language_or_framework} using best coding practices.\n\n\nVectorization: Take advantage of vectorized operations to perform calculations on entire arrays or datasets, which can significantly speed up computations.\nMemory Management: Efficiently manage memory resources, such as by releasing unnecessary objects or using data structures that minimize memory usage.\nParallel Processing: Utilize parallel computing techniques to distribute tasks across multiple cores or processors, thereby accelerating data processing.\nProfiling and Testing: Regularly profile your code to identify performance bottlenecks and optimize critical sections. Thoroughly test your code to ensure correctness and reliability.\n\n\n\n\nPrompt: Discuss the pros and cons of different data visualization techniques for {topic} data analysis in {programming_language_or_framework}.\n\n\nBar Charts and Histograms: These are effective for showing data distributions and comparing categories. They are excellent for visualizing frequency and count data.\nScatterplots: Ideal for displaying relationships between two continuous variables. Scatterplots help identify correlations and trends in data.\nHeatmaps: Useful for visualizing large datasets and identifying patterns in multidimensional data. They are especially valuable for displaying correlation matrices.\nInteractive Dashboards: Create user-friendly interactive dashboards that allow users to explore and interact with data. Dashboards can provide real-time insights and support decision-making.\n\n\n\n\nPrompt: Describe the process of building a custom data analysis tool for {topic} using {programming_language_or_framework}, including the necessary features and functionalities.\n\n\nData Import: Allow users to import data from various sources, such as CSV files, databases, or APIs.\nData Processing: Include preprocessing and transformation capabilities, enabling users to clean, filter, and manipulate data easily.\nVisualization: Incorporate interactive visualization components that help users explore and understand the data visually.\nExport and Reporting: Provide options for exporting analysis results, generating reports, and sharing findings with stakeholders.\n\n\n\n\nPrompt: Explain how to develop a user-friendly dashboard for visualizing and interacting with {topic} data analysis results using {programming_language_or_framework}.\n\n\nIntuitive Design: Create a visually appealing and intuitive design that makes it easy for users to navigate and understand the dashboard.\nInteractive Elements: Incorporate interactive elements, such as filters, sliders, and dropdowns, that allow users to customize their data views.\nReal-Time Updates: Enable real-time updates of data and visualizations to provide users with the latest information.\nAccessibility: Ensure that the dashboard is accessible to all users, including those with disabilities, by following accessibility guidelines.\n\n\n\n\nPrompt: Provide a step-by-step guide for creating a reusable data analysis pipeline for {topic} using {programming_language_or_framework}, covering data preprocessing, analysis, and visualization.\n\n\nData Ingestion: Develop a module for loading data from various sources, including files, databases, and APIs.\nPreprocessing: Create a preprocessing module that encompasses data cleaning, transformation, and feature engineering steps.\nAnalysis: Develop analysis modules that encapsulate statistical analyses, machine learning models, and hypothesis tests.\nVisualization: Implement visualization modules that generate informative charts and graphs for insights.\n\n\n\n\nPrompt: Discuss the key considerations when designing a scalable and modular data analysis tool for {topic} in {programming_language_or_framework}, including performance optimization and extensibility.\n\n\nPerformance Optimization: Optimize your code and algorithms for scalability to ensure that the tool can handle large datasets efficiently.\nModular Architecture: Design the tool with a modular architecture, making it easier to add new features, update existing ones, and maintain the codebase.\nExtensibility: Allow for the easy integration of additional data sources, analysis methods, and visualization techniques to accommodate changing needs.\nUser Collaboration: Implement features that enable collaboration among multiple users, such as data sharing, version control, and user permissions.",
    "crumbs": [
      "기본기",
      "데이터 사이언스"
    ]
  },
  {
    "objectID": "code_interpreter.html#llm-데이터-분석",
    "href": "code_interpreter.html#llm-데이터-분석",
    "title": "",
    "section": "LLM 데이터 분석",
    "text": "LLM 데이터 분석\n대형 언어 모델(LLMs)과 이미지 생성 모델(IGMs)을 기반으로 파이프라인을 사용하여 데이터 시각화 산출물 생성을 제시한 연구(Dibia, 2023) 로 LIDA라는 새로운 도구를 통해 문법에 구애받지 않고 자연어를 통해 시각화 및 인포그래픽을 생성한다. LIDA는 데이터를 자연어 요약으로 변환하는 SUMMARIZER, 데이터를 기반으로 시각화 목표를 나열하는 GOAL EXPLORER, 시각화 코드를 생성하고 정제하며 실행하고 필터링하는 VISGENERATOR, IGM을 사용해 데이터 충실한 스타일화된 그래픽을 생성하는 INFOGRAPHER로 구성된다.",
    "crumbs": [
      "기본기",
      "데이터 사이언스"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "기본기인공지능 코드\n\n\n\n\n\n인공지능\n\n자동화\n프롬프트",
    "crumbs": [
      "기본기",
      "인공지능"
    ]
  },
  {
    "objectID": "gh_action.html",
    "href": "gh_action.html",
    "title": "",
    "section": "",
    "text": "자동화깃헙 액션 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "자동화",
      "깃헙 액션"
    ]
  },
  {
    "objectID": "gh_action.html#주식가격",
    "href": "gh_action.html#주식가격",
    "title": "",
    "section": "주식가격",
    "text": "주식가격\n네이버 금융 크롤링 파이썬 코드를 참고하여 챗GPT로 코드를 동작하는 코드를 생성한다.\n\n코드import requests\nfrom bs4 import BeautifulSoup\n\ndef get_stock_info(stock_code):\n    url = f\"https://finance.naver.com/item/main.nhn?code={stock_code}\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Attempt to extract the stock name with a more robust approach\n    company_info = soup.find(\"div\", {\"class\":\"h_company\"})\n    stock_name = company_info.find(\"a\").text\n\n    # Find the element containing the current price\n    current_price_container = soup.find(\"p\", {\"class\": \"no_today\"})\n    current_price = current_price_container.find(\"span\", {\"class\": \"blind\"}).get_text() if current_price_container else \"Price not found\"\n\n    return stock_name, current_price\n\n# Example usage\nstock_code = '005930'  # Samsung Electronics code\nstock_name, current_price = get_stock_info(stock_code)\nprint(f\"{stock_name} (code {stock_code}) 현재가격: {current_price} KRW.\")\n\n삼성전자 (code 005930) 현재가격: 74,900 KRW.\n\n\n\nR 코드로 변환\n프롬프트: 다음 코드를 R 코드로 변환해줘\n\n인코딩 이슈가 있어 read_html()에서 다음과 같이 변환한다.\n\n코드library(rvest)\n\nWarning: package 'rvest' was built under R version 4.3.3\n\n코드get_stock_info &lt;- function(stock_code) {\n  url &lt;- sprintf(\"https://finance.naver.com/item/main.nhn?code=%s\", stock_code)\n  webpage &lt;- read_html(url,  encoding = \"euc-kr\")  # Setting the encoding to EUC-KR\n  \n  # Attempt to extract the stock name with a more robust approach\n  company_info &lt;- html_node(webpage, \"div.h_company\")\n  stock_name &lt;- html_text(html_node(company_info, \"a\"))\n\n  # Find the element containing the current price\n  current_price_container &lt;- html_node(webpage, \"p.no_today\")\n  if (!is.null(current_price_container)) {\n    current_price &lt;- html_text(html_node(current_price_container, \"span.blind\"))\n  } else {\n    current_price &lt;- \"Price not found\"\n  }\n  \n  list(stock_name = stock_name, current_price = current_price)\n}\n\n# Example usage\nstock_code &lt;- '005930'  # Samsung Electronics code\ninfo &lt;- get_stock_info(stock_code)\ncat(sprintf(\"%s (code %s) 현재가격: %s KRW.\\n\", info$stock_name, stock_code, info$current_price))\n\n삼성전자 (code 005930) 현재가격: 74,900 KRW.",
    "crumbs": [
      "자동화",
      "깃헙 액션"
    ]
  },
  {
    "objectID": "positron.html",
    "href": "positron.html",
    "title": "",
    "section": "",
    "text": "Positron데이터 과학 편집기 코드전체 코드 표시전체 코드 숨기기소스 코드 표시",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#인텔리센스",
    "href": "positron.html#인텔리센스",
    "title": "",
    "section": "인텔리센스",
    "text": "인텔리센스\n지능형 코딩 지원 기능인 인텔리센스(IntelliSense)는 VS Code 기능을 기반으로 데이터 과학 작업에 특화된 코드 작성을 돕는 기능이 포함되어 있으며 포지트론에서 지원되는 중요한 기능은 다음과 같다.\n\n코드 자동 완성\n\n변수, 함수, 클래스 이름 등을 입력하기 시작하면 관련 제안을 표시한다.\n라이브러리나 패키지의 함수들도 자동으로 제안한다.\n\n\n문법 강조\n\n코드의 구문에 따라 다양한 색상으로 강조 표시하여 가독성을 높인다.\n\n\n실시간 오류 검출\n\n코드를 입력하는 동안 문법 오류나 잠재적인 문제를 실시간으로 표시한다.\n\n\n함수 시그니처 도움말\n\n함수를 입력할 때 해당 함수의 매개변수 정보를 보여준다.\n\n\n정의로 이동\n\n변수나 함수의 정의된 위치로 쉽게 이동할 수 있다.\n\n\n리팩토링 도구\n\n변수 이름 변경, 코드 구조 개선 등의 리팩토링 작업을 지원한다.\n\n\n다양한 언어 지원\n\nR, 파이썬, SQL 등 다양한 언어에 대해 인텔리센스 기능을 제공한다.\n\n\n\n\n\n\n\n\n그림 2: 포지트론 인텔리센스",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#단축키",
    "href": "positron.html#단축키",
    "title": "",
    "section": "단축키",
    "text": "단축키\nR 코드 개발을 진행할 때  %&gt;% ,  ←  두가지 기능이 가장 많이 사용되는 단축키로 RStudio에서는 기본으로 지원되었다. 포지트론에서 자주 사용되는 단축키를  CTRL  +  Shift  +  m ,  Alt  +  -  를 적용시키는 방법을 살펴보자.\nPositron의 키보드 단축키는 몇 가지 예외를 제외하고 Visual Studio Code에서 사용되는 키보드 단축키를 그대로 사용할 수 있다. 다음 표는 Positron에 특별히 추가된 단축키를 번역했고 Positron 위키 Keyboard Shortcuts에서 원문을 확인할 수 있다.\n전역 단축키\n\n\n\n\n\n\n단축키\n설명\n\n\n\n\nCmd/Ctrl+Enter\n\n편집기에서 선택한 코드를 실행한다. 선택된 코드가 없으면 현재 문장을 실행한다.\n\n\n\nCmd/Ctrl+Shift+0\n\n콘솔에서 현재 열려있는 인터프리터를 재시작한다.\n\n\n\nCmd/Ctrl+Shift+Enter\n\n편집기에서 열린 파일을 실행한다(예: source() 또는 %run 사용).\n\n\nF1\n커서 아래의 주제에 대한 상황별 도움말을 표시한다.\n\n\n\nCmd/Ctrl+K, Cmd/Ctrl+R\n\n커서 아래의 주제에 대한 상황별 도움말을 표시한다(대체 바인딩).\n\n\n\nCmd/Ctrl+K, F\n\n콘솔에 초점을 맞춘다.\n\n\n\nCtrl+L\n\n콘솔을 지운다.\n\n\nR 단축키\n\n\n\n\n\n\n단축키\n설명\n\n\n\n\nCmd/Ctrl+Shift+M\n\n파이프 연산자(|&gt; 또는 %&gt;%)를 삽입한다.\n\n\n\nAlt+-\n\n할당 연산자(&lt;-)를 삽입한다.\n\n\n\nCmd/Ctrl+Shift+L\n\n현재 R 패키지가 있다면 로드한다.\n\n\n\nCmd/Ctrl+Shift+B\n\n현재 R 패키지가 있다면 빌드하고 설치한다.\n\n\n\nCmd/Ctrl+Shift+T\n\n현재 R 패키지가 있다면 테스트한다.\n\n\n\nCmd/Ctrl+Shift+E\n\n현재 R 패키지가 있다면 검사한다.\n\n\n\nCmd/Ctrl+Shift+D\n\n현재 R 패키지가 있다면 문서화한다.\n\n\nRStudio 키맵\nRStudio 키 바인딩을 사용하려면 다음 단계를 수행하면 다음 RStudio 키 매핑이 활성화된다.\n\nPositron 설정을 연다(Cmd+, 또는 Ctrl+,).\n“keymap”을 검색하거나 Extensions &gt; RStudio Keymap으로 이동한다.\n“Enable RStudio key mappings for Positron” 체크박스를 선택한다.\n\n\n\n\n\n\n\n단축키\n설명\n\n\n\n\nCtrl+1\n\n소스에 초점을 맞춘다.\n\n\n\nCtrl+2\n\n콘솔에 초점을 맞춘다.\n\n\n\nCmd/Ctrl+.\n\n심벌로 이동한다.\n\n\n\nCmd/Ctrl+Shift+C\n\n한 줄을 주석 처리하거나 주석 해제한다.\n\n\n\nCmd/Ctrl+Shift+N\n\n새 R 파일을 생성한다.\n\n\nF2\n정의로 이동한다.\n\n\n\nCmd/Ctrl+I\n\n선택 영역을 다시 들여쓰기한다.\n\n\n\nCmd/Ctrl+Shift+A\n\n선택 영역을 재포맷한다.\n\n\n\nCmd/Ctrl+Shift+S\n\n현재 R 스크립트를 소스로 실행한다.\n\n\n\nCmd/Ctrl+Alt+Shift+M\n\n이름을 변경한다.\n\n\n\nCmd/Ctrl+Alt+I\n\n새 Quarto/R Markdown 셀을 삽입한다.\n\n\n\nCmd/Ctrl+Alt+M\n\n버전 관리 창을 연다.\n\n\n\nCmd/Ctrl+Alt+Left\n\n이전 탭으로 이동한다.\n\n\n\nCmd/Ctrl+Alt+Right\n\n다음 탭으로 이동한다.\n\n\n\nCmd/Ctrl+D\n\n현재 줄을 삭제한다.\n\n\n\nCmd/Ctrl+Shift+M\n\n파이프 연산자를 삽입한다.\n\n\n\nCmd/Ctrl+Shift+R\n\n섹션을 삽입한다.\n\n\n\nAlt+Shift+K\n\n전역 키 바인딩 목록을 연다.\n\n\n\nAlt+-\n\n왼쪽 할당 연산자 &lt;-를 삽입한다.",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#위지윅-편집기",
    "href": "positron.html#위지윅-편집기",
    "title": "",
    "section": "위지윅 편집기",
    "text": "위지윅 편집기\nRStudio에서 쿼토, R마크다운, 마크다운 문서를 편집할 때 위지윅(WYSIWYG) 기능을 비주얼 편집기(Visual Editor) 기능을 통해서 제공하였다. 동일한 기능은 쿼토(Quart) 확장 기능을 설치하게 되면 포함되어 있다. 수식이나 도형 등 기본 미리보기 기능은 내장되어 제공되지만 위지윅 편집 기능은 Cmd/Ctrl+Shift+F4 단축키를 눌러 위지윅 모드, 텍스트 편집 모드 사이를 자유로이 오갈 수 있다.\n위지윅 편집기능을 통해서 문서에 적정한 이미지 크기 조절 및 정렬을 비롯하여 참고문헌 등 다양한 용도로 활용하여 문서제작 생산성을 획기적으로 높일 수 있다.\n\n\n\n\n\n포지트론 위지윅 편집기능",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#코딩-글꼴",
    "href": "positron.html#코딩-글꼴",
    "title": "",
    "section": "코딩 글꼴",
    "text": "코딩 글꼴\n다른 언어와 마찬가지로 R 코드로 데이터 과학 제품을 개발할 경우 글꼴도 코딩에 적합한 한글 글꼴을 설정한다. 먼저 D2 Coding 글꼴을 다운로드하여 운영체제에 설치한다.\n포지트론/VS코드 좌측 하단 톱니바퀴  Settings  설정을 클릭 혹은 메뉴에서 “File” → “Preferences” → “Settings”를 통해 편집기 (Text Editor)로 들어가 운영체제에 설치한 코딩 폰트를 지정한다. Font Ligatures 도 true로 설정한다. 이를 통해 &lt; - 표시가 &larra; 로 화면에 표현된다.\n\n\n\n\n\nD2코딩 글꼴 장착\n\nsettings.json 설정파일에 Font Family, Font Size, Font Ligature를 설정하는 방식도 있다.\n{\n    \"workbench.colorTheme\": \"Default Dark Modern\",\n    \"editor.fontFamily\": \"'D2Coding ligature', D2Coding, monospace\",\n    \"editor.fontSize\": 15,\n    \"editor.fontLigatures\": true\n}",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#테마",
    "href": "positron.html#테마",
    "title": "",
    "section": "테마",
    "text": "테마\nVS Code Themes 웹사이트에서 Visual Studio Code의 테마를 검색하고 발견할 수 있는 웹사이트로 다양한 테마를 설치할 수 있고, 평점 등으로 정렬하여 쉽게 찾을 수 있도록 돕고 있으며, 어두운 테마와 밝은 테마 등 여러 카테고리로 나뉘어져 있다.\nCommand Palette (CTRL + SHIFT + P) → CTRL + K CTRL + T 단축키를 통해 테마를 변경할 수 있다.\nOpen VSX 레지스트리는 Visual Studio Code 확장을 위한 오픈 소스 마켓플레이스 플랫폼으로 커뮤니티 주도로 운영되며, 개발자들이 확장을 게시하고 발견하며 설치할 수 있게 한다. 독립적으로 운영되어 독점적인 확장 마켓플레이스에 대한 대안으로 각광받고 있다.\n\n\n\n\n\nPositron 테마 변경과정",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#프로그래밍-엔진",
    "href": "positron.html#프로그래밍-엔진",
    "title": "",
    "section": "프로그래밍 엔진",
    "text": "프로그래밍 엔진\nR도 동일하지만, 특히 파이썬 버전을 달리하여 설치하고 아나콘다, 가상환경 등 다양한 파이썬 버전을 설치할 경우 경우에 포지트론에서 잡히지 않는 경우가 있다. 이와 같은 문제가 있는 경우 Cmd/Ctrl+Shift+P “명령 팔레트” (Command Palette)를 실행하고 나서 Devloper: Reload Window 명령어를 실행하게 되면 일렉트론(electron)으로 제작된 윈도우가 재실행되어 설치된 파이썬 버전을 찾을 수 있다.\n\n\n\n\n\n그림 3: 파이썬 버전 활성화",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#맞춤법-검사",
    "href": "positron.html#맞춤법-검사",
    "title": "",
    "section": "맞춤법 검사",
    "text": "맞춤법 검사\n비주얼 스튜디오 코드 한스펠은 개인이나 학생에게는 무료인 맞춤법 VS 코드 맞춤법 기능을 제공하는 확장프로그램이다. 아마도 당분간 포지트론에서 확장기능을 제공하지 않을 것으로 보인다. 따라서, vscode-hanspell 확장기능을 포지트론에서 사용하기 위해서 .vsix 파일을 소스코드를 컴파일해서 Extenstions → ... → Install from VSIX...을 통해 로컬 파일형태로 설치한다. 소스코드를 컴파일하지 않고 확장프로그램만 사용하실 분은 다음 .vsix 파일을 설치해서 사용하면 된다.\n .vsix 확장프로그램 \n\n\n\n\n\n\n\n\n\n\n\n그림 4: vscode-hanspell 확장프로그램\n\n\n\n\n\n\n\n\n\n그림 5: 맞춤법 선택\n\n\n\n\n\n\n\n\n\n그림 6: VS 코드 맞춤법 검사\n\n\n\n\n\n\n.vsix 컴파일 과정\n윈도우 환경에서 GitHub 저장소에 소스 코드만 있는 경우, 확장 프로그램을 직접 빌드하고 설치해야 한다.\n\n먼저, Node.js가 컴퓨터에 설치되어 있어야 하고, 설치되어 있지 않다면 Node.js 웹사이트에서 다운로드하여 설치한다.\n터미널(명령 프롬프트)을 연다.\n\nGitHub 저장소를 클론한다.\ngit clone https://github.com/9beach/vscode-hanspell.git\n\n\n클론한 디렉토리로 이동한다.\ncd vscode-hanspell\n\n\n필요한 의존성을 설치한다.\nnpm install\n\nvsce 를 확인한다. npm config get prefix     /c/Users/YourUsername/AppData/Roaming/npm/vsce.cmd --version\n\n확장 프로그램을 빌드한다.\n/c/Users/YourUsername/AppData/Roaming/npm/vsce.cmd package  \nvsce.cmd package 명령어는 .vsix 파일을 생성한다.\n\n포짓트론을 실행한다.\n포짓트론에서 확장 마켓플레이스(Ctrl+Shift+X)를 열고, 상단의 “…” 메뉴를 클릭한다.\n“Install from VSIX…”를 선택하고 방금 생성한 .vsix 파일을 선택한다.\n설치가 완료되면 포지트론을 재시작한다.\n사용자 정의 설정\n~/.hanspell-bad-expressions.json, ~/.hanspell-ignore을 적용한다. 자세한 사항은 비주얼 스튜디오 코드 한스펠 설정을 참고한다.",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#문서-제작",
    "href": "positron.html#문서-제작",
    "title": "",
    "section": "문서 제작",
    "text": "문서 제작\n쿼토 문서\n쿼토(Quarto) 확장프로그램을 설치하게 되면 New File...에서 Quarto Document와 Quarto Project를 선택하여 쿼토 문서를 제작할 수 있다.\n\n\n\n\n\nPositron 쿼토 문서 작성\n\nPDF 보기\n포지트론에서 PDF 파일을 직접 열어보기 위해서는 vscode-pdf 확장프로그램을 설치하면 별도 프로그램(아도브 애크로뱃 등) 없이 PDF 파일을 바로 열 수 있다.\n\n\n\n\n\n그림 7: PDF 확장프로그램 - vscode-pdf\n\n\nR/파이썬\nquarto-webr, pyodide 쿼토 확장프로그램을 설치하면 R, 파이썬 콘솔을 쿼토 문서에서 직접 프로그래밍 할 수 있다.\nquarto add coatless/quarto-webr\nquarto add coatless-quarto/pyodide\n\n\nR\n파이썬\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n샤이니 앱\nshiny 확장프로그램을 설치하면 포지트론에서 웹앱을 제작할 수 있다.\nPosit(이전의 RStudio)에서 Shiny 앱을 실행하는 단축키는 다음과 같다.\nCtrl+Shift+Enter (Windows/Linux) 또는 Cmd+Shift+Enter (Mac)\n추가로 알아두면 좋은 관련 단축키들은 다음과 같다.\n\n앱 중지: Esc 키\n앱 새로고침 (코드 변경 후): Ctrl+Enter (Windows/Linux) 또는 Cmd+Enter (Mac)\nRun App 버튼 클릭: Ctrl+Shift+Enter (Windows/Linux) 또는 Cmd+Shift+Enter (Mac)\n\n\n\n\n\n\n\nshiny app을 포지트론에서 실행할 때 오류\n\n\n\n\nCould not find R. Is R installed on your system?If R is installed, please make sure your PATH environment variable is configured correctly.\n\n환경설정에서 설치된 R 경로를 추가한다.\n\n\n\n\n\n\n\n\n\n\n\nshiny 확장 프로그램\n\n\n\n\n\nshiny 앱 실행\n\n\n\n\n\n그림 8\n\n\nshinylive\nshinylive 패키지를 r-shinylive에서 설치하고 쿼토 확장프로그램도 설치하면 개발한 shiny 앱을 문서를 포함한 다양한 곳에 삽입할 수 있다.\nquarto add quarto-ext/shinylive\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Load the Old Faithful dataset\ndata(faithful)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bins\",\n                  \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    ggplot(faithful, aes(x = waiting)) +\n      geom_histogram(bins = input$bins, fill = \"steelblue\", color = \"white\") +\n      labs(title = \"Histogram of waiting times\",\n           x = \"Waiting time to next eruption (in mins)\",\n           y = \"Frequency\") +\n      theme_minimal() +\n      theme(plot.title = element_text(hjust = 0.5))\n  })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#r-패키지-저장소",
    "href": "positron.html#r-패키지-저장소",
    "title": "",
    "section": "R 패키지 저장소",
    "text": "R 패키지 저장소\n‘C:/Program Files/R/R-4.4.1/library’ 디렉터리에 패키지를 설치할 수 있도록 환경을 설정하는 방법은 다음과 같다. 시스템 관리자 권한이 필요하며, 윈도즈 시스템 설정을 변경해야 한다.\n\n먼저 R을 관리자 권한으로 실행한다.\n\nR 아이콘에 우클릭하고 “관리자 권한으로 실행”을 선택한다.\n\n\nR 콘솔에서 현재 라이브러리 경로를 확인한다.\n\n\n코드.libPaths()\n\n[1] \"C:/Users/statkclee/AppData/Local/R/win-library/4.4\"\n[2] \"C:/Program Files/R/R-4.4.1/library\"                \n\n\n\n\n시스템 환경 변수를 설정한다.\n\nWindows 검색창에 “시스템 환경 변수 편집”을 입력하고 실행한다.\n“환경 변수” 버튼을 클릭한다.\n“시스템 변수” 섹션에서 “새로 만들기”를 클릭한다.\n변수 이름에 “R_LIBS”를 입력한다.\n변수 값에 “C:/Program Files/R/R-4.4.1/library”를 입력한다.\n“확인”을 눌러 저장한다.\n\n\nR을 재시작한다.\n다시 R을 관리자 권한으로 실행하고, 라이브러리 경로를 확인한다.\n\n\n코드.libPaths()\n\n[1] \"C:/Users/statkclee/AppData/Local/R/win-library/4.4\"\n[2] \"C:/Program Files/R/R-4.4.1/library\"                \n\n\n\n이제 패키지 설치를 시도한다.\n\ninstall.packages(\"remotes\")\n그럼에도 불구하고 패키지를 시스템 환경에 저장할 수 없는 경우, R_LIBS_USER을 사용해서 패키지 저장소를 설정한다.\n\n\n\n\n\n\n사용자 R 패키지 저장소\n\n\n\nR_LIBS_USER 사용자 R 패키지 저장소를 .libPaths() 설정하고 패키지 저장소로 활용한다.\n\n코드# 사용자 라이브러리 디렉토리 생성\ndir.create(path = Sys.getenv(\"R_LIBS_USER\"), showWarnings = FALSE, recursive = TRUE)\n\n# .libPaths()에 사용자 라이브러리 추가\n.libPaths(Sys.getenv(\"R_LIBS_USER\"))\n\n# 패키지 설치 시도\ninstall.packages(\"remotes\")",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#프로젝트-관리자",
    "href": "positron.html#프로젝트-관리자",
    "title": "",
    "section": "프로젝트 관리자",
    "text": "프로젝트 관리자\n프로젝트를 넘나들며 작업하기 위해서 Project Manager 확장 프로그램을 설치한다.",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  },
  {
    "objectID": "positron.html#데이터-탐색기",
    "href": "positron.html#데이터-탐색기",
    "title": "",
    "section": "데이터 탐색기",
    "text": "데이터 탐색기\n데이터 탐색기(Data Explorer)는 코드 우선 데이터 탐색을 보완하기 위한 도구다. 스프레드시트와 유사한 그리드에 데이터를 표시하고, 데이터 필터링 및 정렬을 지원하여 Positron 내에서 직접 요약 통계를 제공한다. 데이터 탐색기 목표는 코드 기반 워크플로우를 대체하는 것이 아니라, 코드를 통해 데이터를 추가로 탐색하거나 수정할 때 한번 쓰고 버리는 데이터 뷰나 요약 통계를 제공하여 보완하는 것이다. Positron 위키 Data Explorer을 번역한 것으로 Data Explorer에서 원문을 확인할 수 있다.\n데이터 탐색기는 다음과 같은 세 가지 주요 구성 요소를 가지고 있다.\n\n데이터 그리드: 개별 셀과 열의 스프레드시트 형태 표시 및 정렬 기능\n요약 패널: 각 열의 열 이름, 유형 및 결측 데이터 비율\n필터 바: 특정 열에 대한 일시적 필터\n\n\n데이터프레임 열기\n각 데이터 탐색기 인스턴스는 언어 런타임에 의해 구동되며 Python(pandas) 또는 R(data.frame, tibble, data.table)의 데이터프레임을 표시할 수 있다. 또한 polars에 대한 실험적 지원도 제공하며, 향후 추가적인 Python 데이터프레임 라이브러리가 추가될 예정이다.\n데이터 탐색기의 각 인스턴스는 기본 데이터의 변경 사항에 따라 새로 고쳐진다. 이를 통해 UI 중심의 데이터 탐색기와 코드 우선 접근 방식이 결합된 워크플로우가 가능하다.\n특정 데이터프레임에 대한 새 데이터 탐색기 인스턴스를 열려면 다음 방법 중 하나를 사용한다:\n\n언어 런타임을 직접 사용:\n\nPython을 통해: %view dataframe label\n\nR을 통해: View(dataframe, \"label\")\n\n\n\n변수 창으로 이동하여 특정 데이터프레임 객체에 대한 데이터 탐색기 아이콘을 클릭\n\n\n데이터 그리드\n데이터 그리드는 주요 표시 영역으로, 스프레드시트와 유사한 셀별 뷰를 제공한다. 수백만 행이나 열까지의 비교적 큰 인메모리 데이터셋을 효율적으로 처리하도록 설계되었다. 각 열 헤더에는 열 이름과 함께 언어 런타임에서 사용되는 데이터 유형이 표시된다. 각 열의 오른쪽 상단에 있는 문맥 메뉴를 통해 정렬을 제어하거나 선택한 열에 대한 필터를 빠르게 추가할 수 있다. 열 경계를 클릭하고 드래그하여 열의 크기를 조정할 수 있다.\n\n행 레이블은 기본적으로 관찰된 행 인덱스를 사용하며, Python에서는 0부터 시작하는 인덱스를, R에서는 1부터 시작하는 인덱스를 사용한다. 또는 pandas와 R 사용자는 수정된 인덱스나 문자열 기반 레이블이 있는 행을 가질 수 있다.\n요약 패널\n요약 패널은 모든 열 이름과 해당 유형을 나타내는 아이콘을 세로로 스크롤되는 목록으로 표시한다. 또한 결측 데이터의 양을 증가하는 백분율과 인라인 막대 그래프로 표시한다.\n\n열 이름을 더블 클릭하면 데이터 그리드에서 해당 열에 초점을 맞추어 더 넓은 데이터를 빠르게 탐색할 수 있다. 요약 패널은 레이아웃 컨트롤을 통해 데이터 탐색기의 왼쪽이나 오른쪽에 배치하거나 일시적으로 숨길 수 있다.\n필터 바\n필터 바에는 기존 필터를 표시, 숨기거나 제거하는 컨트롤과 새 필터를 추가하는 + 버튼이 있다. 데이터 탐색기 하단의 상태 표시줄에는 필터 적용 후 남은 행의 백분율과 수가 표시된다. 새 필터를 만들 때는 전체 목록을 스크롤하거나 특정 문자열로 열을 검색하여 열을 선택해야 한다. 열을 선택하면 해당 열 유형에 사용할 수 있는 필터가 표시된다. 또는 데이터 그리드의 각 열 레이블에 있는 콘텍스트 메뉴를 통해 열 이름이 미리 채워진 필터를 만들 수 있다.\n\n사용할 수 있는 필터는 열 유형에 따라 다르다. 예를 들어, 문자열 열에는 다음과 같은 필터 옵션이 있다: 포함, 시작 또는 끝남, 비어 있음, 정확히 일치. 반면 숫자 열에는 다음과 같은 논리 연산이 있다: 미만 또는 초과, 같음, 또는 두 값 사이(포함).\n\n코드# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nView(penguins)",
    "crumbs": [
      "Positron",
      "데이터 과학 편집기"
    ]
  }
]